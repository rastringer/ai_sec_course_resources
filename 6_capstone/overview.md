
# Capstone Project

Hopefully there have been sections of this course you found interesting or applicable to something you are working on, or both!

Now begins the capstone phase where we put into practice some of the techniques we have learned and build a project of your choice. This will help to solidify concepts, build skills, understanding, and can be a helpful portfolio item for your next job, studies or career advancement.

# How to begin

The key thing to start with is a project that interests you. In this section, we outline some ideas which may work well as a research-style notebook or web app:

### Adversarial ML


**Transferability**: Create and evaluate universarial adversarial patches that can fool image classifiers across different models. Write a  notebook with advice on how vulnerable some common models may be to this attack.


**Audio**: explore adversarial attacks for audio systems

### LLM Security


**Frameworks**: Create a framework to either discover and execute prompt injection techniques, or to defend against them. 


**Backdoors**: See if fine-tuning a model can insert a backdoor attack to be triggered by certain phrases. Have a go at developing detection methods to identify compromised models.


**Rogue models**: How easy and cheap can it be to strip a model of its safety conditioning through fine-tuning?

### Differential Privacy

**Real world datasets**: Try the various privacy attacks and defences with a real world dataset. Write a conclusion about how effective and realistic DP may be at scale.


**DP-SGD Study**: Compare DP training methods, measuring cost, privacy guarantees and model performance. Write conclusions about how realistic the implementation of such measures are at scale.

### Fusion projects

**Adversarial training with privacy constraints**: What are the tradeoffs of combining adversarial robustness with privacy protections?


**Red team automation**: Remember the `deckofmanyprompts` tool in the Red Teaming LLMs module? Create a toolkit to helkp automate different types of attacks, such as adversarial examples, prompt injections or privacy attacks.


**Benchmarking platform**: Create a standardized evaluation suite for evaluating models' robustness to provide security scorecards.


**AI Monitoring**: Build a simple system and show how you could develop AI-centric monitoring to detect potential attacks on the data pipeline, API, model etc. 

### Policy / Governance projects

If you aren't inclinded to write code and instead explore these topics in prose, think of an interesting paper featuring research, explainers or recommendations that would be useful to tech leads, governments, academics or others trying to learn about AI security.

### Timeline

If you are doing the course at your own pace, no need to keep any specific timeline in mind. If you are on a taught course, this activity will fill the second half of day five. We will have some follow-ups over the next two weeks to check on progress and help us get to complete a finished project. 


