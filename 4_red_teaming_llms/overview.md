
# Red teaming LLMs

In this module, we will explore some of the security considerations of working with LLMs by attacking them. 

LLM vulnerability and security is a vast and fastly-moving topic, here are some research papers to get an appraisal of the field. 

# LLM Security

[Universal and Transferable Adversarial Attacks
on Aligned Language Models](https://arxiv.org/pdf/2307.15043) by Zou et al.

[Red Teaming Language Models to Reduce Harms](https://arxiv.org/pdf/2209.07858) by Ganguli et al.

[Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527) by Perez and Ribeiro.

[Extracting Training Data from Large Language Models
](https://arxiv.org/abs/2012.07805) by Carlini et al.

[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566) by Hubinger et al.

# RAG and RAG security

[*Retrieval-Augmented Generation for Large
Language Models: A Survey*](https://arxiv.org/pdf/2312.10997) by Gao et al.

[PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/pdf/2402.07867) by Zou et al. 

[DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/html/2507.15042v1) by Wang and Yu.

