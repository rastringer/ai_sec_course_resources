{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a24aca-8e97-40d0-9776-8ab8433c05b8",
   "metadata": {},
   "source": [
    "# Weights exfiltration\n",
    "\n",
    "In our adversarial images notebook, we explored a permissioned, or white-box attack, where we have access to the model. \n",
    "\n",
    "Here, we will try various black-box, or unpermissioned attacks based on scenarios where we cannot access the underlying model (eg via an API service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7ccb9-7d1c-48f5-8c5f-14eab496c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9810e-1130-412f-883b-ba84ba65daf2",
   "metadata": {},
   "source": [
    "To simulate an API, we will have a wrapped ResNet18 model that can only receive inputs and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c39715-a167-424b-92ba-9fc0b3db979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VictimModel:\n",
    "    \"\"\"Simulates a black-box API - we can only send inputs and get predictions back.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"resnet18\"):\n",
    "        print(f\"Loading victim model: {model_name}\")\n",
    "        self.model = models.resnet18(weights=True)\n",
    "        self.model.eval()\n",
    "        self.queries = 0\n",
    "        self.max_queries = 10000\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"The only interface we have - like calling an API endpoint.\"\"\"\n",
    "        if self.queries >= self.max_queries:\n",
    "            raise Exception(\"API quota exceeded! (This is why attacks are expensive)\")\n",
    "        \n",
    "        self.queries += 1\n",
    "        with torch.no_grad():\n",
    "            if x.dim() == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            logits = self.model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            return probs.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e63f4-f02e-48cd-8fa4-e0b19f9b961e",
   "metadata": {},
   "source": [
    "We will try three attacks in our `BlacBoxAttacker` class\n",
    "\n",
    "* Decision boundary probing -- consider this the reconnaissance phase\n",
    "* Adversarial example crafting -- finding exploitations\n",
    "* Model extraction -- knowledge / IP theft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd491c-345f-4df8-9a75-d85e31e5e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackBoxAttacker:\n",
    "    \"\"\"The attacker - trying to steal secrets from the black box.\"\"\"\n",
    "    \n",
    "    def __init__(self, victim_model):\n",
    "        self.victim = victim_model\n",
    "\n",
    "# Initialize our attack scenario\n",
    "victim = VictimModel()\n",
    "attacker = BlackBoxAttacker(victim)\n",
    "\n",
    "# Generate test images for our attacks\n",
    "print(\"\\nGenerating test images...\")\n",
    "# img1 = torch.randn(3, 224, 224) * 0.8\n",
    "# img2 = torch.randn(3, 224, 224) * 0.8\n",
    "img1_path = \"images/siamese_cat.jpeg\"\n",
    "img2_path = \"images/saint_bernard.jpeg\"\n",
    "\n",
    "# Load and preprocess the actual images\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Standard ImageNet preprocessing for ResNet18\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load images\n",
    "img1 = preprocess(Image.open(img1_path).convert('RGB')).unsqueeze(0)\n",
    "img2 = preprocess(Image.open(img2_path).convert('RGB')).unsqueeze(0)\n",
    "\n",
    "print(\"Setup complete! Ready to begin attacks...\")\n",
    "print(f\"Starting query count: {victim.queries}/{victim.max_queries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b1a4a-ac63-4a42-9915-a30d43b71512",
   "metadata": {},
   "source": [
    "### Probing classification boundaries\n",
    "\n",
    "Finding the line at which classifications are split can be valuable information about the model's behaviour. In this cell, we take the following steps:\n",
    "\n",
    "* Create a straight line between two input images in the pixel space\n",
    "* Sample points along the line, querying the target model to elicit its class prediction, and its confidence in that prediction\n",
    "* Detect the exact locations where the model shifts its classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50a597-4fdc-4b25-9983-cf7186e4dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_decision_boundary(self, img1, img2, steps=20):\n",
    "    \"\"\"Find where the model changes its mind by interpolating between images.\"\"\"\n",
    "    print(\"\\nATTACK 1: Decision Boundary Reconnaissance\")\n",
    "    print(\"Interpolating between two inputs to find decision boundaries...\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Get predictions for two starting points\n",
    "    # This tells us which classes the model assigns to each endpoint\n",
    "    pred1 = torch.argmax(self.victim.predict(img1)).item()\n",
    "    pred2 = torch.argmax(self.victim.predict(img2)).item()\n",
    "    \n",
    "    # Initialize storage for our reconnaissance data\n",
    "    boundary_data = []\n",
    "    decision_changes = [] # this will track when the model changes its mind\n",
    "    \n",
    "    # Walk along the straight line between img1 and img2 in pixel space\n",
    "    # Think of this as drawing a line through high-dimensional space (3×224×224 = ~150k dimensions!)\n",
    "    for i in range(steps + 1):\n",
    "        # alpha controls our position along the line\n",
    "        # eg α=0 → pure img2, α=1 → pure img1, α=0.5 → halfway blend\n",
    "        alpha = i / steps\n",
    "    \n",
    "        # Linear interpolation: create a weighted blend of the two images\n",
    "        # This is like morphing between two photos, pixel by pixel\n",
    "        # interpolated[pixel] = α × img1[pixel] + (1-α) × img2[pixel]\n",
    "        interpolated = alpha * img1 + (1 - alpha) * img2\n",
    "    \n",
    "        # Query the mock API with interpolated image\n",
    "        probs = self.victim.predict(interpolated)\n",
    "        pred_class = torch.argmax(probs).item() # Which class has highest probability?\n",
    "        confidence = torch.max(probs).item() # How confident is the model?\n",
    "        \n",
    "        boundary_data.append({\n",
    "            'position': alpha,\n",
    "            'prediction': pred_class,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "        \n",
    "        # Detect decision boundary crossings\n",
    "        # This should reveal the point in the input space where the model's\n",
    "        # decision changes. This is how we gain x-ray vision into the\n",
    "        # internal structure of the model without access to weights or gradients\n",
    "        if i > 0 and boundary_data[i-1]['prediction'] != pred_class:\n",
    "            decision_changes.append(alpha)\n",
    "\n",
    "        print(boundary_data)\n",
    "    \n",
    "    print(f\"   Found {len(decision_changes)} decision boundary crossings!\")\n",
    "    print(f\"   Queries used: {len(boundary_data)} | Total: {self.victim.queries}\")\n",
    "    \n",
    "    return boundary_data, decision_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbbdad-d762-4794-9c3b-31b29d3054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_data, crossings = probe_decision_boundary(attacker, img1, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db9b2a-e0bd-4962-9416-af18dcc6aef8",
   "metadata": {},
   "source": [
    "We can see from these results:\n",
    "\n",
    "* Class 247 (Siamese cat) at α=0 → α=0.3 (high confidence ~98% → 79%)\n",
    "* Decision boundary crossings at α=0.35 and α=0.4:\n",
    "\n",
    "    α=0.35: Brief switch to Class 195 (low confidence 37%)\n",
    "\n",
    "    α=0.4: Switches to Class 284 (Saint Bernard) and stays there\n",
    "\n",
    "\n",
    "The model is most vulnerable around α=0.35 (confidence drops to 37%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226a0bd-b8c2-4217-9df9-017646682d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_results(boundary_data):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    positions = [d['position'] for d in boundary_data]\n",
    "    predictions = [d['prediction'] for d in boundary_data]\n",
    "    confidences = [d['confidence'] for d in boundary_data]\n",
    "    \n",
    "    # Plot 1: Decision changes\n",
    "    ax1.plot(positions, predictions, 'bo-', markersize=8, linewidth=2)\n",
    "    ax1.set_title('Decision Boundary Crossings', fontweight='bold')\n",
    "    ax1.set_xlabel('Interpolation Position (α)')\n",
    "    ax1.set_ylabel('Predicted Class')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Confidence changes\n",
    "    ax2.plot(positions, confidences, 'ro-', markersize=6, linewidth=2)\n",
    "    ax2.set_title('Model Confidence Along Path', fontweight='bold')\n",
    "    ax2.set_xlabel('Interpolation Position (α)')\n",
    "    ax2.set_ylabel('Confidence')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_boundary_results(boundary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d609bb8-e6e6-468a-ba7e-36b8928b6f43",
   "metadata": {},
   "source": [
    "We can see from the visualisations that the model has simple decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a36ba-4a54-449f-bb04-7b75190f1174",
   "metadata": {},
   "source": [
    "### Crafting adversarial examples\n",
    "\n",
    "Now we can use the reconnaissance data to craft adversarial examples! In the following cell, we\n",
    "\n",
    "* take the boundary crossing points found (α=0.35, α=0.4 or similar)\n",
    "* calculate the direction vectors pointing toward those vulnerable regions\n",
    "* create 'smart' perturbation directions\n",
    "\n",
    "This is a more targeted approach to the fast gradient sign method, which creates random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1925d15-a0ea-4d48-ad77-400d46d44a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_adversarial_example(self, base_img, target_class, boundary_data, img1, img2, max_iters=200):\n",
    "    \"\"\"Create an adversarial example using boundary-guided optimization.\"\"\"\n",
    "    print(f\"\\nATTACK 2: Boundary-Guided Adversarial Example Generation\")\n",
    "    print(f\"   Trying to fool model into predicting class {target_class}...\")\n",
    "    \n",
    "    current_img = base_img.clone()\n",
    "    original_pred = torch.argmax(self.victim.predict(base_img)).item()\n",
    "    \n",
    "    print(f\"   Original prediction: Class {original_pred}\")\n",
    "    \n",
    "    # Extract boundary intelligence from reconnaissance\n",
    "    decision_changes = []\n",
    "    vulnerable_regions = []\n",
    "    \n",
    "    for i in range(1, len(boundary_data)):\n",
    "        if boundary_data[i-1]['prediction'] != boundary_data[i]['prediction']:\n",
    "            change_point = boundary_data[i]['position']\n",
    "            decision_changes.append(change_point)\n",
    "            print(f\"   Using boundary crossing at α={change_point:.2f}\")\n",
    "            \n",
    "        # Find regions with low confidence (vulnerable spots)\n",
    "        if boundary_data[i]['confidence'] < 0.5:\n",
    "            vulnerable_regions.append(boundary_data[i]['position'])\n",
    "            print(f\"   Found vulnerable region at α={boundary_data[i]['position']:.2f} (confidence: {boundary_data[i]['confidence']:.3f})\")\n",
    "    \n",
    "    # Calculate direction vectors toward boundary crossings\n",
    "    boundary_directions = []\n",
    "    for alpha in decision_changes + vulnerable_regions:\n",
    "        # Direction from base_img toward the boundary crossing point\n",
    "        boundary_point = alpha * img1 + (1 - alpha) * img2\n",
    "        direction = boundary_point - base_img\n",
    "        direction = direction / torch.norm(direction)  # Normalize\n",
    "        boundary_directions.append(direction)\n",
    "    \n",
    "    print(f\"   Computed {len(boundary_directions)} boundary-guided directions\")\n",
    "    \n",
    "    step_size = 0.01  # Smaller steps for more precision\n",
    "    attack_history = []\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        best_perturbation = None\n",
    "        best_target_prob = -1\n",
    "        \n",
    "        # Phase 1: Try boundary-guided perturbations (70% of attempts)\n",
    "        for direction in boundary_directions:\n",
    "            for scale in [0.5, 1.0, 1.5]:  # Different magnitudes\n",
    "                perturbation = direction * step_size * scale\n",
    "                perturbed = torch.clamp(current_img + perturbation, -2, 2)\n",
    "                \n",
    "                probs = self.victim.predict(perturbed)\n",
    "                target_prob = probs[target_class].item()\n",
    "                \n",
    "                if target_prob > best_target_prob:\n",
    "                    best_target_prob = target_prob\n",
    "                    best_perturbation = perturbation\n",
    "        \n",
    "        # Phase 2: Random exploration (30% of attempts) for diversity\n",
    "        for _ in range(3):\n",
    "            noise = torch.randn_like(current_img) * step_size\n",
    "            perturbed = torch.clamp(current_img + noise, -2, 2)\n",
    "            \n",
    "            probs = self.victim.predict(perturbed)\n",
    "            target_prob = probs[target_class].item()\n",
    "            \n",
    "            if target_prob > best_target_prob:\n",
    "                best_target_prob = target_prob\n",
    "                best_perturbation = noise\n",
    "        \n",
    "        # Apply best perturbation\n",
    "        if best_perturbation is not None:\n",
    "            current_img = torch.clamp(current_img + best_perturbation, -2, 2)\n",
    "            current_pred = torch.argmax(self.victim.predict(current_img)).item()\n",
    "            \n",
    "            attack_history.append({\n",
    "                'iteration': iteration,\n",
    "                'target_probability': best_target_prob,\n",
    "                'prediction': current_pred,\n",
    "                'perturbation_norm': torch.norm(current_img - base_img).item()\n",
    "            })\n",
    "            \n",
    "            # Success condition\n",
    "            if current_pred == target_class:\n",
    "                print(f\"   Success! Adversarial example found at iteration {iteration}\")\n",
    "                print(f\"   Leveraged boundary intelligence to reduce attack queries!\")\n",
    "                print(f\"   Perturbation magnitude: {attack_history[-1]['perturbation_norm']:.4f}\")\n",
    "                break\n",
    "            \n",
    "            # Progress update\n",
    "            if iteration % 20 == 0:\n",
    "                print(f\"   Iteration {iteration}: Target prob = {best_target_prob:.3f}, Current pred = {current_pred}\")\n",
    "    \n",
    "    final_pred = torch.argmax(self.victim.predict(current_img)).item()\n",
    "    success = final_pred == target_class\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"   Attack failed. Final prediction: Class {final_pred}\")\n",
    "        print(f\"   Best target probability achieved: {max([h['target_probability'] for h in attack_history]):.3f}\")\n",
    "    \n",
    "    # Calculate total queries (boundary reconnaissance + attack phases)\n",
    "    estimated_queries = len(boundary_directions) * 3 + len(attack_history) * 3\n",
    "    print(f\"   Attack queries used: ~{estimated_queries} | Total: {self.victim.queries}\")\n",
    "    \n",
    "    return {\n",
    "        'adversarial_image': current_img,\n",
    "        'original_image': base_img,\n",
    "        'success': success,\n",
    "        'history': attack_history,\n",
    "        'boundary_directions_used': len(boundary_directions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7614037-ae9d-4e34-8a13-ec2c00835c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_data, crossings = probe_decision_boundary(\n",
    "    attacker,\n",
    "    img1, \n",
    "    img2\n",
    ")\n",
    "\n",
    "result = craft_adversarial_example(\n",
    "    attacker,\n",
    "    base_img=img1,  # Start with the cat image\n",
    "    target_class=284,  # Try to make it predict \"Saint Bernard\" \n",
    "    boundary_data=boundary_data,\n",
    "    img1=img1, \n",
    "    img2=img2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9c2ce-59eb-4240-b45a-dfada86ef763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def visualize_adversarial_attack(result, boundary_data, img1, img2, victim_model):\n",
    "    \"\"\"\n",
    "    Visualize the adversarial attack results with original images, \n",
    "    adversarial example, and classification results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ImageNet class names (you can expand this or use a proper mapping)\n",
    "    imagenet_classes = {\n",
    "        247: \"Siamese Cat\",\n",
    "        284: \"Saint Bernard\", \n",
    "        195: \"Border Terrier\"  # The intermediate class we saw\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Boundary-Guided Adversarial Attack Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Helper function to denormalize images for display\n",
    "    def denormalize_image(tensor_img):\n",
    "        # Reverse ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "        denorm = tensor_img * std + mean\n",
    "        denorm = torch.clamp(denorm, 0, 1)\n",
    "        return denorm.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    def get_prediction_info(img, model):\n",
    "        with torch.no_grad():\n",
    "            probs = model.predict(img)\n",
    "            pred_class = torch.argmax(probs).item()\n",
    "            confidence = torch.max(probs).item()\n",
    "            return pred_class, confidence\n",
    "    \n",
    "    # Top row: Original images and adversarial result\n",
    "    \n",
    "    # Original Image 1 (Cat)\n",
    "    pred1, conf1 = get_prediction_info(img1, victim_model)\n",
    "    axes[0, 0].imshow(denormalize_image(img1))\n",
    "    axes[0, 0].set_title(f'Original: {imagenet_classes.get(pred1, f\"Class {pred1}\")}\\nConfidence: {conf1:.3f}', \n",
    "                        fontsize=12, color='green', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Original Image 2 (Dog)  \n",
    "    pred2, conf2 = get_prediction_info(img2, victim_model)\n",
    "    axes[0, 1].imshow(denormalize_image(img2))\n",
    "    axes[0, 1].set_title(f'Original: {imagenet_classes.get(pred2, f\"Class {pred2}\")}\\nConfidence: {conf2:.3f}', \n",
    "                        fontsize=12, color='green', fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Adversarial Example\n",
    "    adv_pred, adv_conf = get_prediction_info(result['adversarial_image'], victim_model)\n",
    "    orig_pred, orig_conf = get_prediction_info(result['original_image'], victim_model)\n",
    "    \n",
    "    axes[0, 2].imshow(denormalize_image(result['adversarial_image']))\n",
    "    \n",
    "    if result['success']:\n",
    "        title_color = 'red'\n",
    "        title = f'\\nNow predicts: {imagenet_classes.get(adv_pred, f\"Class {adv_pred}\")}\\nConfidence: {adv_conf:.3f}'\n",
    "    else:\n",
    "        title_color = 'orange'  \n",
    "        title = f'Attack Failed\\nStill predicts: {imagenet_classes.get(adv_pred, f\"Class {adv_pred}\")}\\nConfidence: {adv_conf:.3f}'\n",
    "    \n",
    "    axes[0, 2].set_title(title, fontsize=12, color=title_color, fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Bottom row: Analysis plots\n",
    "    \n",
    "    # Perturbation visualization (difference between original and adversarial)\n",
    "    perturbation = result['adversarial_image'] - result['original_image'] \n",
    "    pert_magnitude = torch.norm(perturbation).item()\n",
    "    \n",
    "    # Scale perturbation for visualization\n",
    "    pert_vis = perturbation.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    pert_vis = (pert_vis - pert_vis.min()) / (pert_vis.max() - pert_vis.min())\n",
    "    \n",
    "    axes[1, 0].imshow(pert_vis)\n",
    "    axes[1, 0].set_title(f'Perturbation Applied\\nMagnitude: {pert_magnitude:.4f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Decision boundary crossing visualization\n",
    "    positions = [bd['position'] for bd in boundary_data]\n",
    "    predictions = [bd['prediction'] for bd in boundary_data] \n",
    "    confidences = [bd['confidence'] for bd in boundary_data]\n",
    "    \n",
    "    axes[1, 1].plot(positions, predictions, 'o-', linewidth=2, markersize=6)\n",
    "    axes[1, 1].set_xlabel('Interpolation Position (α)')\n",
    "    axes[1, 1].set_ylabel('Predicted Class')\n",
    "    axes[1, 1].set_title('Decision Boundary Crossings', fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add boundary crossing markers\n",
    "    for i in range(1, len(boundary_data)):\n",
    "        if boundary_data[i-1]['prediction'] != boundary_data[i]['prediction']:\n",
    "            axes[1, 1].axvline(x=boundary_data[i]['position'], color='red', \n",
    "                              linestyle='--', alpha=0.7, linewidth=2)\n",
    "    \n",
    "    # Attack progress visualization\n",
    "    if result['history']:\n",
    "        iterations = [h['iteration'] for h in result['history']]\n",
    "        target_probs = [h['target_probability'] for h in result['history']]\n",
    "        \n",
    "        axes[1, 2].plot(iterations, target_probs, 'b-', linewidth=2)\n",
    "        axes[1, 2].set_xlabel('Attack Iteration')\n",
    "        axes[1, 2].set_ylabel('Target Class Probability')\n",
    "        axes[1, 2].set_title('Attack Progress\\n(Boundary-Guided)', fontweight='bold')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        if result['success']:\n",
    "            success_iter = next(i for i, h in enumerate(result['history']) if h['prediction'] == adv_pred)\n",
    "            axes[1, 2].axvline(x=success_iter, color='red', linestyle='--', \n",
    "                              label=f'Success at iter {success_iter}')\n",
    "            axes[1, 2].legend()\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5, 'No attack progress\\ndata available', \n",
    "                       ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "        axes[1, 2].set_title('Attack Progress', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Boundary Directions Used: {result.get('boundary_directions_used', 'N/A')}\")\n",
    "    print(f\"Total Queries: {victim_model.queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ab777-c870-436f-af81-0c07f035c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_adversarial_attack(result, boundary_data, img1, img2, attacker.victim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd431c-6bc4-4959-a2b1-651708435210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adversarial_results(adv_result):\n",
    "    if not adv_result['history']:\n",
    "        print(\"No attack history to plot.\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    iterations = [h['iteration'] for h in adv_result['history']]\n",
    "    target_probs = [h['target_probability'] for h in adv_result['history']]\n",
    "    perturbation_norms = [h['perturbation_norm'] for h in adv_result['history']]\n",
    "    \n",
    "    # Plot 1: Attack progress\n",
    "    ax1.plot(iterations, target_probs, 'go-', markersize=6, linewidth=2)\n",
    "    ax1.set_title('😈 Adversarial Attack Progress', fontweight='bold')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Target Class Probability')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    if adv_result['success']:\n",
    "        ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Success Threshold')\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Plot 2: Perturbation magnitude\n",
    "    ax2.plot(iterations, perturbation_norms, 'mo-', markersize=6, linewidth=2)\n",
    "    ax2.set_title('🔧 Perturbation Magnitude', fontweight='bold')\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('L2 Norm of Perturbation')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_adversarial_results(adv_result)\n",
    "\n",
    "print(f\"\\n{'Success!' if adv_result['success'] else 'Failed.'} This shows the model's vulnerability (if any) to small perturbations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f67da-f8d4-4335-8be5-a6ab2eff5968",
   "metadata": {},
   "source": [
    "### Stealing model knowledge\n",
    "\n",
    "In the next example of an extractive, black box attack, we attempt to reverse engineer a model's decision making progress.\n",
    "\n",
    "We generate synthetic inputs and get results from the model to collect input-output pairs. We then use these pairs as surrogate data to train a model which mimics the original model's behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4179b-24aa-4bc5-8fa8-5e3fd6d64de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steal_model_knowledge(self, n_samples=400):\n",
    "    \"\"\"Model extraction attack - steal the victim's knowledge!\"\"\"\n",
    "    print(f\"\\nATTACK 3: Model Extraction (Knowledge Theft)\")\n",
    "    print(f\"  Generating {n_samples} strategic queries to steal model knowledge...\")\n",
    "    \n",
    "    # Generate diverse synthetic inputs to query the model\n",
    "    synthetic_inputs = []\n",
    "    stolen_labels = []\n",
    "    \n",
    "    # Strategy: Use different types of inputs to maximize information gain\n",
    "    for i in range(n_samples):\n",
    "        if i % 4 == 0:\n",
    "            # Random noise\n",
    "            sample = torch.randn(3, 224, 224) * 0.5\n",
    "        elif i % 4 == 1:\n",
    "            # Structured patterns\n",
    "            sample = torch.zeros(3, 224, 224)\n",
    "            sample[:, :, ::8] = 2.0  # Vertical stripes\n",
    "        elif i % 4 == 2:\n",
    "            # Gradients\n",
    "            x = torch.linspace(-2, 2, 224)\n",
    "            y = torch.linspace(-2, 2, 224)\n",
    "            X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "            sample = torch.stack([X, Y, X*Y])\n",
    "        else:\n",
    "            # High frequency patterns\n",
    "            sample = torch.randn(3, 224, 224) * 2.0\n",
    "            sample[:, ::2, ::2] *= 3\n",
    "        \n",
    "        # Query the victim model\n",
    "        probs = self.victim.predict(sample)\n",
    "        predicted_class = torch.argmax(probs).item()\n",
    "        \n",
    "        synthetic_inputs.append(sample.flatten().numpy())\n",
    "        stolen_labels.append(predicted_class)\n",
    "    \n",
    "    # Train our surrogate model on stolen data\n",
    "    X_stolen = np.array(synthetic_inputs)\n",
    "    y_stolen = np.array(stolen_labels)\n",
    "    \n",
    "    # Check if we got diverse enough data\n",
    "    unique_classes = len(set(stolen_labels))\n",
    "    print(f\"   Discovered {unique_classes} different classes\")\n",
    "    \n",
    "    if unique_classes < 2:\n",
    "        print(\"   Warning: Low class diversity - model might be too specialized\")\n",
    "        return None\n",
    "    \n",
    "    # Train surrogate model\n",
    "    print(\"   Training surrogate model on stolen data...\")\n",
    "    surrogate = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    surrogate.fit(X_stolen, y_stolen)\n",
    "    \n",
    "    # Evaluate how well we've stolen the knowledge\n",
    "    train_accuracy = accuracy_score(y_stolen, surrogate.predict(X_stolen))\n",
    "    \n",
    "    print(f\"   Surrogate model trained!\")\n",
    "    print(f\"   Training accuracy: {train_accuracy:.3f}\")\n",
    "    print(f\"   Queries used: {n_samples} | Total: {self.victim.queries}\")\n",
    "    \n",
    "    return {\n",
    "        'surrogate_model': surrogate,\n",
    "        'training_data': (X_stolen, y_stolen),\n",
    "        'accuracy': train_accuracy,\n",
    "        'classes_discovered': unique_classes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de1255-c360-4a64-9d94-e42bfcaaae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_queries = victim.max_queries - victim.queries\n",
    "extraction_samples = min(300, remaining_queries - 50)  # Leave buffer\n",
    "extraction_result = steal_model_knowledge(attacker, extraction_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c13aad-4973-47d0-b11f-7be5236d0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_extraction_results(extraction_result):\n",
    "    if extraction_result is None:\n",
    "        print(\"No extraction results to plot.\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot 1: Class distribution\n",
    "    _, y_stolen = extraction_result['training_data']\n",
    "    unique_classes, counts = np.unique(y_stolen, return_counts=True)\n",
    "    \n",
    "    ax1.bar(range(len(unique_classes)), counts, color='lightblue', edgecolor='darkblue')\n",
    "    ax1.set_title('Stolen Class Distribution', fontweight='bold')\n",
    "    ax1.set_xlabel('Class ID')\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    ax1.set_xticks(range(len(unique_classes)))\n",
    "    ax1.set_xticklabels([f'{c}' for c in unique_classes])\n",
    "    \n",
    "    # Plot 2: Model performance comparison\n",
    "    methods = ['Stolen\\nKnowledge', 'Random\\nGuessing']\n",
    "    accuracies = [extraction_result['accuracy'], 1.0/extraction_result['classes_discovered']]\n",
    "    \n",
    "    bars = ax2.bar(methods, accuracies, color=['lightcoral', 'lightgray'], edgecolor='black')\n",
    "    ax2.set_title('Surrogate vs Random Performance', fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if extraction_result:\n",
    "    plot_extraction_results(extraction_result)\n",
    "    print(f\"\\nINSIGHT: We successfully stole knowledge from {extraction_result['classes_discovered']} classes!\")\n",
    "    print(\"    Our surrogate model can now mimic the victim's behavior on new inputs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ad35a-8141-404e-a9ed-082ec71fa779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ATTACK SUMMARY\")\n",
    "\n",
    "print(f\"Total API queries used: {victim.queries}/{victim.max_queries}\")\n",
    "print(f\"Decision boundaries found: {len(crossings)}\")\n",
    "print(f\"Adversarial attack success: {'✅' if adv_result['success'] else '❌'}\")\n",
    "\n",
    "if extraction_result:\n",
    "    print(f\"Model extraction accuracy: {extraction_result['accuracy']:.3f}\")\n",
    "    print(f\"Classes discovered: {extraction_result['classes_discovered']}\")\n",
    "\n",
    "cost_estimate = victim.queries * 0.001  # Assume $0.001 per query\n",
    "print(f\"Estimated attack cost: ${cost_estimate:.2f}\")\n",
    "\n",
    "# Final comprehensive visualization\n",
    "def plot_complete_attack_summary(boundary_data, adv_result, extraction_result):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Complete Black-Box Attack Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Decision boundary\n",
    "    positions = [d['position'] for d in boundary_data]\n",
    "    predictions = [d['prediction'] for d in boundary_data]\n",
    "    \n",
    "    axes[0,0].plot(positions, predictions, 'bo-', markersize=8)\n",
    "    axes[0,0].set_title('Decision Boundary Probing')\n",
    "    axes[0,0].set_xlabel('Interpolation Position (α)')\n",
    "    axes[0,0].set_ylabel('Predicted Class')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Adversarial progress\n",
    "    if adv_result['history']:\n",
    "        iterations = [h['iteration'] for h in adv_result['history']]\n",
    "        target_probs = [h['target_probability'] for h in adv_result['history']]\n",
    "        \n",
    "        axes[0,1].plot(iterations, target_probs, 'go-', markersize=6)\n",
    "        axes[0,1].set_title('Adversarial Attack Progress')\n",
    "        axes[0,1].set_xlabel('Iteration')\n",
    "        axes[0,1].set_ylabel('Target Class Probability')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Model extraction results\n",
    "    if extraction_result:\n",
    "        _, y_stolen = extraction_result['training_data']\n",
    "        unique_classes, counts = np.unique(y_stolen, return_counts=True)\n",
    "        \n",
    "        axes[1,0].bar(range(len(unique_classes)), counts, color='lightcoral')\n",
    "        axes[1,0].set_title('Extracted Class Distribution')\n",
    "        axes[1,0].set_xlabel('Class ID')\n",
    "        axes[1,0].set_ylabel('Samples Stolen')\n",
    "    \n",
    "    # 4. Query usage summary\n",
    "    attack_names = ['Boundary\\nProbing', 'Adversarial\\nGeneration', 'Model\\nExtraction']\n",
    "    query_counts = [21, len(adv_result['history']) * 8 if adv_result['history'] else 0, \n",
    "                   extraction_samples if extraction_result else 0]\n",
    "    \n",
    "    bars = axes[1,1].bar(attack_names, query_counts, color=['skyblue', 'lightgreen', 'lightyellow'])\n",
    "    axes[1,1].set_title('📊 Query Usage by Attack')\n",
    "    axes[1,1].set_ylabel('Number of Queries')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, query_counts):\n",
    "        height = bar.get_height()\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                      f'{count}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_complete_attack_summary(boundary_data, adv_result, extraction_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b05e8-ac72-4c5f-b0f1-b2d3e07bb010",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We can elicit behavioural patterns that reveal model decision tendencies, map decision boundaries and find adversarial examples iteratively. Model extraction is possible however can require thousands or more queries so is a resource-intensive process.\n",
    "\n",
    "Black-box, or unpermissioned probing also has other limitations: since the process cannot access internal representations of the models, it's harder to understand specific model layer behaviours.\n",
    "\n",
    "### Defences\n",
    "Some precautions against model, or weights exfiltration, include:\n",
    "\n",
    "* Rate limiting and query monitoring\n",
    "* Output perturbation and noise injection\n",
    "* Differential privacy mechanisms\n",
    "* Anomalous query pattern detection\n",
    "* API access logging and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
