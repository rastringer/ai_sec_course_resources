{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a66cde-1acc-4d39-81f0-4603144615d7",
   "metadata": {},
   "source": [
    "# Adversarial Images Overview\n",
    "\n",
    "In this notebook, we will explore crafting adversarial images and how to make more robust models that are harder to confuse. This is a toy example comprising a simple convolutional neural network which we train ourselves using the CIFAR-10 dataset, one of the less resource-intensive image datasets. Everyone should be able to run this notebook on CPU or on a free Colab T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea23e6b-b549-4784-b147-7e8f6599ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and random seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d438d-95c5-4a16-94a1-c8d87d863853",
   "metadata": {},
   "source": [
    "Here's our simple CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1fec2-87e4-47c3-9f5c-511356477c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ea039-cda3-442c-a420-85b0cf6abb54",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The normalisation here is more extensive than in previous notebooks. This process standardises the pixel values of the images based on the mean values for color channels in the CIFAR-10 dataset. See the comments in the code to explain \n",
    "\n",
    "As in previous notebooks, we get the data from TorchVision and use the DataLoader. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cee45-7f68-48a2-95d5-fb50e5e75933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), # the mean values for red, green, blue channels\n",
    "                         (0.2023, 0.1994, 0.2010)) # standard deviation values for red, green, blue channels\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(f\"Dataset loaded: {len(trainset)} training samples, {len(testset)} test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f9994-d3a3-4f6f-b211-dc6e978a5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training data\n",
    "def show_sample_data(dataloader, num_samples=8):\n",
    "    \"\"\"Show sample images from the dataset\"\"\"\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1)\n",
    "    images_viz = images * std + mean\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))\n",
    "    fig.suptitle('Sample CIFAR-10 Training Data', fontsize=14)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        axes[i].imshow(images_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[i].set_title(f'{classes[labels[i]]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n📊 Sample training data:\")\n",
    "show_sample_data(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918b588-2ab9-41b3-be5d-4a7f7f1bc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, epochs=10, lr=0.001, model_name=\"Model\"):\n",
    "    \"\"\"Train the model and show progress\"\"\"\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/len(trainloader):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(range(1, epochs+1), train_losses, 'b-', linewidth=2)\n",
    "    ax1.set_title(f'{model_name} Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(range(1, epochs+1), train_accuracies, 'g-', linewidth=2)\n",
    "    ax2.set_title(f'{model_name} Training Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ {model_name} training complete! Final accuracy: {epoch_acc:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a390dd-cabc-41d1-9b4a-ab71537dbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, testloader):\n",
    "    \"\"\"Test model accuracy and show results\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Show per-class accuracy\n",
    "    print(f\"\\n📊 Test Results:\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            class_acc = 100 * class_correct[i] / class_total[i]\n",
    "            print(f\"  {classes[i]:>8}: {class_acc:.1f}%\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd99d4-af08-4e5c-b4b6-e7082ff84b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the clean model\n",
    "print(\"\\nTraining our baseline (clean) model...\")\n",
    "clean_model = SimpleNet()\n",
    "clean_model = train_model(clean_model, trainloader, epochs=8, model_name=\"Clean Model\")\n",
    "\n",
    "print(\"\\nTesting clean model performance:\")\n",
    "clean_accuracy = test_model(clean_model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864ee94-a201-4a79-9388-e21f40b4bc90",
   "metadata": {},
   "source": [
    "### Adversarial attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95229-9b8d-4ca7-ae9f-363dc0d466d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def denormalize_images(images):\n",
    "    \"\"\"Denormalise CIFAR-10 images for visualization\"\"\"\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1).to(images.device)\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1).to(images.device)\n",
    "    return images * std + mean\n",
    "\n",
    "# Get sample data for demonstrations\n",
    "dataiter = iter(testloader)\n",
    "sample_images, sample_labels = next(dataiter)\n",
    "sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)\n",
    "\n",
    "print(\"Let's start with some clean examples:\")\n",
    "\n",
    "# Show clean predictions\n",
    "def show_clean_predictions(model, images, labels, num_examples=8):\n",
    "    \"\"\"Show clean model predictions\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images[:num_examples])\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        confidences = torch.max(probs, dim=1)[0]\n",
    "    \n",
    "    images_viz = denormalize_images(images[:num_examples])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(16, 2))\n",
    "    fig.suptitle('Clean Model Predictions', fontsize=14)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        axes[i].imshow(images_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        correct = '✅' if preds[i] == labels[i] else '❌'\n",
    "        axes[i].set_title(f'{correct}\\nTrue: {classes[labels[i]]}\\nPred: {classes[preds[i]]}\\nConf: {confidences[i]:.2f}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    accuracy = (preds == labels[:num_examples]).sum().item() / num_examples\n",
    "    print(f\"Clean accuracy on sample: {accuracy*100:.1f}%\")\n",
    "\n",
    "show_clean_predictions(clean_model, sample_images, sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8a62e-f47e-4341-8815-d50617db7e50",
   "metadata": {},
   "source": [
    "### FGSM\n",
    "\n",
    "The Fast Gradient Sign Attack is one of the key adversarial attacks to cause untargeted or targeted misclassifications.\n",
    "\n",
    "Rather than minimizing the loss by adjusting the weights based on backpropagated gradients, the FGSM seeks to adjust the input data to maximize the loss based on the same backpropagated gradients. [This](https://docs.pytorch.org/tutorials/beginner/fgsm_tutorial.html) PyTorch tutorial provides a good introduction to the algorithm. \n",
    "\n",
    "Here is the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff4d6a-463c-4a2b-bd86-af4909edd80e",
   "metadata": {},
   "source": [
    "### \n",
    "Core FGSM Formula\n",
    "\n",
    "$$xadv=x+ϵ⋅sign(∇xJ(θ,x,y))$$\n",
    "\n",
    "x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y))xadv​=x+ϵ⋅sign(∇x​J(θ,x,y))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$x$ = original input (clean example)\n",
    "\n",
    "$x_{adv}$ = adversarial example\n",
    "\n",
    "$ϵ$ = (epsilon) perturbation magnitude (attack strength)\n",
    "\n",
    "$J(θ,x,y)$ = loss function with parameters \n",
    "$θ$, input $x$, and true label $y$\n",
    "\n",
    "$∇xJ(θ,x,y)$ = gradient of loss with respect to input $x$\n",
    "\n",
    "$\\text{sign}(\\cdot)$ sign function (returns -1, 0, or +1)\n",
    "\n",
    "This means we take the original input $x$, compute the gradient of the loss function with respect to that input $∇xJ(θ,x,y)$, take the sign of that gradient, multiply by the perturbation magnitude $ϵ$ ('epsilon'), and add this to the original input to create the adversarial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4f732-5a10-4b8f-b89b-619d433c20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, images, labels, epsilon):\n",
    "    \"\"\"\n",
    "    Fast Gradient Sign Method (FGSM) Attack\n",
    "    \n",
    "    This function creates adversarial examples by adding small perturbations\n",
    "    to input images in the direction that maximizes the model's loss.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to attack\n",
    "        images: Input images tensor (batch_size, channels, height, width)\n",
    "        labels: True labels for the images\n",
    "        epsilon: Maximum perturbation magnitude (controls attack strength)\n",
    "    \"\"\"\n",
    "    # Prepare the images for computing gradients\n",
    "    # We need to clone the images to avoid manipulating the \n",
    "    # original data. We detatch from any existing computational\n",
    "    # graph, then re-enable gradient tracking.\n",
    "    images = images.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Forward pass through the model: get the predictions\n",
    "    # for the original images\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Calculate the loss using cross entropy\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "    # Prepare for the backward pass by clearing\n",
    "    # any existing gradients in model parameters\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Clear any existing gradients on the input images\n",
    "    # This means we start with a clean slate for gradient computation\n",
    "    if images.grad is not None:\n",
    "        images.grad.zero_()\n",
    "\n",
    "    # Backward pass: calculate gradients of the loss with respect to the input\n",
    "    # image. This tells us how to change each pixel to increase the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Verify gradients were computed\n",
    "    if images.grad is None:\n",
    "        print(\"Warning: No gradients computed. Returning original images.\")\n",
    "        return images.detach()\n",
    "\n",
    "    # Extract and use gradients of the loss with respect to input pixels\n",
    "    # to craft adversarial pertubations\n",
    "    data_grad = images.grad\n",
    "\n",
    "    # Create adversarial examples using the FGSM: \n",
    "    # x_adversarial = x_original + epsilon * sign(gradient)\n",
    "    # sign gives the direction that increases loss\n",
    "    perturbed_data = images + epsilon * data_grad.sign()\n",
    "\n",
    "    # We have to clamp values to keep them within the expected \n",
    "    # normalized input range\n",
    "    perturbed_data = torch.clamp(perturbed_data, -2.5, 2.5)\n",
    "\n",
    "    # Detach from computation graph since we have no need for \n",
    "    # gradients for the output\n",
    "    return perturbed_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16c7c4-7f8e-41f0-9927-8229c89d77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_fgsm_attack(model, images, labels, epsilon, num_examples=6):\n",
    "    \"\"\"Demonstrate FGSM attack with visualization\"\"\"\n",
    "    print(f\"\\nFGSM Attack Demonstration (ε = {epsilon})\")\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    adv_images = fgsm_attack(model, images, labels, epsilon)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        clean_outputs = model(images[:num_examples])\n",
    "        adv_outputs = model(adv_images[:num_examples])\n",
    "        \n",
    "        clean_preds = torch.argmax(clean_outputs, dim=1)\n",
    "        adv_preds = torch.argmax(adv_outputs, dim=1)\n",
    "        \n",
    "        clean_probs = F.softmax(clean_outputs, dim=1)\n",
    "        adv_probs = F.softmax(adv_outputs, dim=1)\n",
    "        \n",
    "        clean_conf = torch.max(clean_probs, dim=1)[0]\n",
    "        adv_conf = torch.max(adv_probs, dim=1)[0]\n",
    "    \n",
    "    # Visualize\n",
    "    clean_viz = denormalize_images(images[:num_examples])\n",
    "    adv_viz = denormalize_images(adv_images[:num_examples])\n",
    "    perturbations = (adv_images[:num_examples] - images[:num_examples]) * 10 + 0.5\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_examples, figsize=(2*num_examples, 6))\n",
    "    fig.suptitle(f'FGSM Attack Results (ε = {epsilon})', fontsize=16)\n",
    "    \n",
    "    successful_attacks = 0\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Clean image\n",
    "        axes[0, i].imshow(clean_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[0, i].set_title(f'Original\\n{classes[labels[i]]}\\nConf: {clean_conf[i]:.2f}', fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Perturbation (amplified)\n",
    "        axes[1, i].imshow(perturbations[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        l2_norm = torch.norm(adv_images[i] - images[i]).item()\n",
    "        axes[1, i].set_title(f'Perturbation ×10\\nL2: {l2_norm:.3f}', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Adversarial image\n",
    "        axes[2, i].imshow(adv_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        attack_success = adv_preds[i] != labels[i]\n",
    "        if attack_success:\n",
    "            successful_attacks += 1\n",
    "        symbol = '🎯' if attack_success else '✅'\n",
    "        color = 'red' if attack_success else 'green'\n",
    "        axes[2, i].set_title(f'Adversarial {symbol}\\n{classes[adv_preds[i]]}\\nConf: {adv_conf[i]:.2f}', \n",
    "                           fontsize=9, color=color)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    success_rate = (successful_attacks / num_examples) * 100\n",
    "    print(f\"🎯 Attack Success Rate: {successful_attacks}/{num_examples} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    return successful_attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c680b58-29da-4b67-b72f-b00e2efbf776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate FGSM with different epsilon values\n",
    "epsilons = [0.01, 0.03, 0.07]\n",
    "for eps in epsilons:\n",
    "    demonstrate_fgsm_attack(clean_model, sample_images, sample_labels, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449468c1-43d9-4329-8ce9-9c896859c489",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent (PGD) Attack\n",
    "\n",
    "PGD is an iterative attack that refines adversarial examples over multiple steps.\n",
    "It's more powerful than FGSM because it can find better adversarial examples\n",
    "by taking multiple smaller steps and projecting back to the allowed perturbation region.\n",
    "\n",
    "#### Core PGD Formula (Iterative)\n",
    "\n",
    "$x(t+1)=Πx+S(x(t)+α⋅sign(∇xJ(θ,x(t),y)))$\n",
    "\n",
    "Where:\n",
    "\n",
    "$x^{(t)}$ = adversarial example at iteration $t$\n",
    "\n",
    "$x^{(t+1)}$ = adversarial example at iteration $t+1$\n",
    "\n",
    "$x(0)$ = $x+uniform(−ϵ,ϵ) = random initialization within ϵ-ball\n",
    "\n",
    "$Πx+S$ = projection operator onto feasible set $S$\n",
    "\n",
    "$S={x′:∣∣x′−x∣∣∞≤ϵ}$ = $L\\infty$ ball of radius ϵ \n",
    "\n",
    "$α\\alpha$ = step size (typically $α=ϵ/k\\alpha$ where $k$\n",
    "is number of iterations)\n",
    "\n",
    "$J(θ,x,y)J(\\theta, x, y)$ = loss function\n",
    "\n",
    "$∇xJ(θ,x(t),y)$ = gradient of loss with respect to current adversarial example\n",
    "\n",
    "PDG therefore applies multiple gradient steps vs PGD's single step. The projection operator $Πx+S$ ensures pertubations stay within the ϵ-ball, the allowed perturbation space, after each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8207a1-78d6-4b90-9e53-66d911a19faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, epsilon, alpha=0.01, num_iter=10):\n",
    "    \"\"\"\n",
    "    Projected Gradient Descent (PGD) Attack\n",
    "    Args:\n",
    "    model: The neural network model to attack\n",
    "    images: Input images tensor (batch_size, channels, height, width)\n",
    "    labels: True labels for the images\n",
    "    epsilon: Maximum allowed perturbation magnitude (L∞ constraint)\n",
    "    alpha: Step size for each iteration (should be smaller than epsilon)\n",
    "    num_iter: Number of attack iterations to perform\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Just as with FGSM, we clone the data\n",
    "    images = images.clone().detach()\n",
    "    \n",
    "    # Unlike FGSM, where we start with clean images, PGD begins with random noise\n",
    "    # uniform(-epsilon, epsilon) creates random values between -epsilon and epsilon\n",
    "    perturbed_images = images + torch.empty_like(images).uniform_(-epsilon, epsilon)\n",
    "\n",
    "    # Ensure initial perturbed images stay within valid pixel range\n",
    "    perturbed_images = torch.clamp(perturbed_images, -2.5, 2.5)\n",
    "\n",
    "    # Iterative refinement loop:\n",
    "    # PGD takes multiple gradient steps to iteratively improve \n",
    "    # an adversarial example\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Detach from previous iteration's computation graph, enable gradients\n",
    "        perturbed_images = perturbed_images.detach().requires_grad_(True)\n",
    "\n",
    "        # Forward pass \n",
    "        outputs = model(perturbed_images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Clear gradients from model parameters and input images\n",
    "        model.zero_grad()\n",
    "        if perturbed_images.grad is not None:\n",
    "            perturbed_images.grad.zero_()\n",
    "        \n",
    "        # Calculate how to change pixels to increase the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # If no gradients computed, exit\n",
    "        if perturbed_images.grad is None:\n",
    "            break\n",
    "\n",
    "        # Move in the direction that increases loss\n",
    "        # alpha is the step size - smaller than epsilon for gradual improvement\n",
    "        data_grad = perturbed_images.grad\n",
    "        perturbed_images = perturbed_images + alpha * data_grad.sign()\n",
    "        \n",
    "        # Project back to epsilon ball - this is the 'projected' in PGD\n",
    "        # Ensure pixels aren't perturbed by more than epsilon\n",
    "        delta = perturbed_images - images\n",
    "\n",
    "        # Constrain perturbation and ensure pixels within valid range\n",
    "        delta = torch.clamp(delta, -epsilon, epsilon)\n",
    "        perturbed_images = torch.clamp(images + delta, -2.5, 2.5)\n",
    "\n",
    "    # Just as FGSM, we don't need gradients for output \n",
    "    # so we detach from compuation graph\n",
    "    return perturbed_images.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162842f2-c006-48f6-b824-bcf1e58e221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attacks(model, images, labels, epsilon, num_examples=6):\n",
    "    \"\"\"Compare FGSM vs PGD attacks\"\"\"\n",
    "    print(f\"\\nAttack Comparison (ε = {epsilon})\")\n",
    "    \n",
    "    fgsm_adv = fgsm_attack(model, images, labels, epsilon)\n",
    "    pgd_adv = pgd_attack(model, images, labels, epsilon)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        clean_outputs = model(images[:num_examples])\n",
    "        fgsm_outputs = model(fgsm_adv[:num_examples])\n",
    "        pgd_outputs = model(pgd_adv[:num_examples])\n",
    "        \n",
    "        clean_preds = torch.argmax(clean_outputs, dim=1)\n",
    "        fgsm_preds = torch.argmax(fgsm_outputs, dim=1)\n",
    "        pgd_preds = torch.argmax(pgd_outputs, dim=1)\n",
    "    \n",
    "    clean_viz = denormalize_images(images[:num_examples])\n",
    "    fgsm_viz = denormalize_images(fgsm_adv[:num_examples])\n",
    "    pgd_viz = denormalize_images(pgd_adv[:num_examples])\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_examples, figsize=(2*num_examples, 6))\n",
    "    fig.suptitle(f'Attack Comparison (ε = {epsilon})', fontsize=16)\n",
    "    \n",
    "    fgsm_success = 0\n",
    "    pgd_success = 0\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Clean\n",
    "        axes[0, i].imshow(clean_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[0, i].set_title(f'Clean\\n{classes[labels[i]]}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # FGSM\n",
    "        axes[1, i].imshow(fgsm_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        fgsm_attack_success = fgsm_preds[i] != labels[i]\n",
    "        if fgsm_attack_success:\n",
    "            fgsm_success += 1\n",
    "        symbol = '🎯' if fgsm_attack_success else '✅'\n",
    "        axes[1, i].set_title(f'FGSM {symbol}\\n{classes[fgsm_preds[i]]}', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # PGD\n",
    "        axes[2, i].imshow(pgd_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        pgd_attack_success = pgd_preds[i] != labels[i]\n",
    "        if pgd_attack_success:\n",
    "            pgd_success += 1\n",
    "        symbol = '🎯' if pgd_attack_success else '✅'\n",
    "        axes[2, i].set_title(f'PGD {symbol}\\n{classes[pgd_preds[i]]}', fontsize=10)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"FGSM Success Rate: {fgsm_success}/{num_examples} ({100*fgsm_success/num_examples:.1f}%)\")\n",
    "    print(f\"PGD Success Rate:  {pgd_success}/{num_examples} ({100*pgd_success/num_examples:.1f}%)\")\n",
    "\n",
    "compare_attacks(clean_model, sample_images, sample_labels, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcba0b-4b66-486f-a247-b456ca962728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_robustness_curve(model, testloader, attack_fn, epsilons, max_batches=5):\n",
    "    \"\"\"Plot model robustness vs epsilon\"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    print(\"Testing robustness across epsilon values...\")\n",
    "    for eps in epsilons:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            adv_inputs = attack_fn(model, inputs, labels, eps)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(adv_inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"ε = {eps:.3f}: {accuracy:.2f}%\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot([0] + epsilons, [clean_accuracy] + accuracies, 'ro-', linewidth=3, markersize=8)\n",
    "    plt.xlabel('Attack Strength (ε)', fontsize=12)\n",
    "    plt.ylabel('Model Accuracy (%)', fontsize=12)\n",
    "    plt.title('Model Robustness vs Attack Strength', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, max(epsilons))\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.annotate(f'{clean_accuracy:.1f}%', (0, clean_accuracy), \n",
    "                textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    for i, (eps, acc) in enumerate(zip(epsilons, accuracies)):\n",
    "        plt.annotate(f'{acc:.1f}%', (eps, acc), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "eps_range = [0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1]\n",
    "clean_robustness = plot_robustness_curve(clean_model, testloader, fgsm_attack, eps_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d4010-9ffb-4db2-b3dd-2b4d2ffa5ac3",
   "metadata": {},
   "source": [
    "This graph shows small perturbations (ε < 0.01) already reduce accuracy significantly. The model also looks more vulnerable as ε increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb99b9-fe1e-41a7-a4f1-603d975c383b",
   "metadata": {},
   "source": [
    "### Defences\n",
    "\n",
    "Adversarial training is a common precaution against perturbed samples. We add adversarial images to the training data in the hope that the model becomes harder to deceive. Since adversarial training introduces more overhead to what may already be a costly training cycle, organizations will have to calculate the level of risk, time and expenditure to inform budgeting for such defensive measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd53e4-68f4-4ce7-906e-6ba8e2bf2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, trainloader, epochs=8, epsilon=0.03):\n",
    "    \"\"\"\n",
    "    Adversarial Training - train on mix of clean and adversarial examples\n",
    "    \"\"\"\n",
    "    print(f\"\\nAdversarial Training (ε = {epsilon})...\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    adv_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_clean = 0\n",
    "        correct_adv = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(trainloader, desc=f'Robust Training Epoch {epoch+1}/{epochs}')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 50% clean, 50% adversarial examples\n",
    "            if torch.rand(1) < 0.5:\n",
    "                # Use adversarial examples\n",
    "                adv_inputs = fgsm_attack(model, inputs.clone(), labels, epsilon)\n",
    "                train_inputs = adv_inputs\n",
    "                is_adv = True\n",
    "            else:\n",
    "                # Use clean examples\n",
    "                train_inputs = inputs\n",
    "                is_adv = False\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            if is_adv:\n",
    "                correct_adv += predicted.eq(labels).sum().item()\n",
    "            else:\n",
    "                correct_clean += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Show both clean and adversarial accuracy during training\n",
    "            clean_acc = 100. * correct_clean / (total/2) if total > 0 else 0\n",
    "            adv_acc = 100. * correct_adv / (total/2) if total > 0 else 0\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/len(trainloader):.3f}',\n",
    "                'Clean': f'{clean_acc:.1f}%',\n",
    "                'Adv': f'{adv_acc:.1f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(clean_acc)\n",
    "        adv_accuracies.append(adv_acc)\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(range(1, epochs+1), train_losses, 'b-', linewidth=2)\n",
    "    ax1.set_title('Adversarial Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(range(1, epochs+1), train_accuracies, 'g-', linewidth=2, label='Clean')\n",
    "    ax2.plot(range(1, epochs+1), adv_accuracies, 'r-', linewidth=2, label='Adversarial')\n",
    "    ax2.set_title('Training Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Adversarial training complete!\")\n",
    "    return model\n",
    "\n",
    "# Train robust model\n",
    "robust_model = SimpleNet()\n",
    "robust_model = adversarial_training(robust_model, trainloader, epochs=8)\n",
    "\n",
    "print(\"\\nTesting robust model performance:\")\n",
    "robust_clean_acc = test_model(robust_model, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025edf3-e953-4e33-a2a7-32234a7ea249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_predictions(clean_model, robust_model, images, labels, num_examples=8):\n",
    "    \"\"\"Compare predictions from clean and robust models\"\"\"\n",
    "    \n",
    "    clean_model.eval()\n",
    "    robust_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        clean_outputs = clean_model(images[:num_examples])\n",
    "        robust_outputs = robust_model(images[:num_examples])\n",
    "        \n",
    "        clean_preds = torch.argmax(clean_outputs, dim=1)\n",
    "        robust_preds = torch.argmax(robust_outputs, dim=1)\n",
    "        \n",
    "        clean_probs = F.softmax(clean_outputs, dim=1)\n",
    "        robust_probs = F.softmax(robust_outputs, dim=1)\n",
    "        \n",
    "        clean_conf = torch.max(clean_probs, dim=1)[0]\n",
    "        robust_conf = torch.max(robust_probs, dim=1)[0]\n",
    "    \n",
    "    images_viz = denormalize_images(images[:num_examples])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_examples, figsize=(16, 4))\n",
    "    fig.suptitle('Clean vs Robust Model Predictions', fontsize=16)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Clean model predictions\n",
    "        axes[0, i].imshow(images_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        clean_correct = '✅' if clean_preds[i] == labels[i] else '❌'\n",
    "        axes[0, i].set_title(f'Clean Model {clean_correct}\\nPred: {classes[clean_preds[i]]}\\nConf: {clean_conf[i]:.2f}', \n",
    "                           fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Robust model predictions\n",
    "        axes[1, i].imshow(images_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        robust_correct = '✅' if robust_preds[i] == labels[i] else '❌'\n",
    "        axes[1, i].set_title(f'Robust Model {robust_correct}\\nPred: {classes[robust_preds[i]]}\\nConf: {robust_conf[i]:.2f}', \n",
    "                           fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_model_predictions(clean_model, robust_model, sample_images, sample_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c012ab-698b-4caf-85f3-61b78c03dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_robustness(clean_model, robust_model, images, labels, attack_fn, epsilon, num_examples=6):\n",
    "    \"\"\"Compare how clean and robust models handle attacks\"\"\"\n",
    "    \n",
    "    print(f\"\\nModel Robustness Comparison ({attack_fn.__name__.upper()}, ε = {epsilon})\")\n",
    "    \n",
    "    # Generate adversarial examples for both models\n",
    "    clean_adv = attack_fn(clean_model, images, labels, epsilon)\n",
    "    robust_adv = attack_fn(robust_model, images, labels, epsilon)\n",
    "    \n",
    "    # Get predictions\n",
    "    clean_model.eval()\n",
    "    robust_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Clean images\n",
    "        clean_clean_outputs = clean_model(images[:num_examples])\n",
    "        robust_clean_outputs = robust_model(images[:num_examples])\n",
    "        \n",
    "        # Adversarial images\n",
    "        clean_adv_outputs = clean_model(clean_adv[:num_examples])\n",
    "        robust_adv_outputs = robust_model(robust_adv[:num_examples])\n",
    "        \n",
    "        clean_clean_preds = torch.argmax(clean_clean_outputs, dim=1)\n",
    "        robust_clean_preds = torch.argmax(robust_clean_outputs, dim=1)\n",
    "        clean_adv_preds = torch.argmax(clean_adv_outputs, dim=1)\n",
    "        robust_adv_preds = torch.argmax(robust_adv_outputs, dim=1)\n",
    "    \n",
    "    # Visualize\n",
    "    clean_viz = denormalize_images(images[:num_examples])\n",
    "    clean_adv_viz = denormalize_images(clean_adv[:num_examples])\n",
    "    robust_adv_viz = denormalize_images(robust_adv[:num_examples])\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_examples, figsize=(2*num_examples, 6))\n",
    "    fig.suptitle(f'Model Robustness Comparison ({attack_fn.__name__.upper()}, ε = {epsilon})', fontsize=16)\n",
    "    \n",
    "    clean_success = 0\n",
    "    robust_success = 0\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Original images\n",
    "        axes[0, i].imshow(clean_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[0, i].set_title(f'Original\\n{classes[labels[i]]}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Clean model under attack\n",
    "        axes[1, i].imshow(clean_adv_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        clean_attack_success = clean_adv_preds[i] != labels[i]\n",
    "        if clean_attack_success:\n",
    "            clean_success += 1\n",
    "        symbol = '🎯' if clean_attack_success else '✅'\n",
    "        color = 'red' if clean_attack_success else 'green'\n",
    "        axes[1, i].set_title(f'Clean Model {symbol}\\n{classes[clean_adv_preds[i]]}', \n",
    "                           fontsize=10, color=color)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Robust model under attack\n",
    "        axes[2, i].imshow(robust_adv_viz[i].permute(1, 2, 0).clamp(0, 1))\n",
    "        robust_attack_success = robust_adv_preds[i] != labels[i]\n",
    "        if robust_attack_success:\n",
    "            robust_success += 1\n",
    "        symbol = '🎯' if robust_attack_success else '✅'\n",
    "        color = 'red' if robust_attack_success else 'green'\n",
    "        axes[2, i].set_title(f'Robust Model {symbol}\\n{classes[robust_adv_preds[i]]}', \n",
    "                           fontsize=10, color=color)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    clean_success_rate = (clean_success / num_examples) * 100\n",
    "    robust_success_rate = (robust_success / num_examples) * 100\n",
    "    \n",
    "    print(f\"Clean Model Attack Success:  {clean_success}/{num_examples} ({clean_success_rate:.1f}%)\")\n",
    "    print(f\"Robust Model Attack Success: {robust_success}/{num_examples} ({robust_success_rate:.1f}%)\")\n",
    "    print(f\"🛡️ Defense Improvement: {clean_success_rate - robust_success_rate:.1f} percentage points\")\n",
    "\n",
    "# Test both models against FGSM\n",
    "compare_model_robustness(clean_model, robust_model, sample_images, sample_labels, fgsm_attack, 0.03)\n",
    "\n",
    "# Test both models against PGD\n",
    "compare_model_robustness(clean_model, robust_model, sample_images, sample_labels, pgd_attack, 0.03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188ed67-245b-4f4a-ad43-a5131e575ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(clean_model, robust_model, testloader, attack_fn, epsilons, max_batches=5):\n",
    "    \"\"\"Compare robustness of both models across epsilon values\"\"\"\n",
    "    \n",
    "    clean_accs = []\n",
    "    robust_accs = []\n",
    "    \n",
    "    print(\"Comparing model robustness across epsilon values...\")\n",
    "    for eps in epsilons:\n",
    "        print(f\"Testing ε = {eps:.3f}...\")\n",
    "        \n",
    "        # Test clean model\n",
    "        clean_correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            adv_inputs = attack_fn(clean_model, inputs, labels, eps)\n",
    "            with torch.no_grad():\n",
    "                outputs = clean_model(adv_inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                clean_correct += (predicted == labels).sum().item()\n",
    "        clean_acc = 100 * clean_correct / total\n",
    "        clean_accs.append(clean_acc)\n",
    "        \n",
    "        # Test robust model\n",
    "        robust_correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            adv_inputs = attack_fn(robust_model, inputs, labels, eps)\n",
    "            with torch.no_grad():\n",
    "                outputs = robust_model(adv_inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                robust_correct += (predicted == labels).sum().item()\n",
    "        robust_acc = 100 * robust_correct / total\n",
    "        robust_accs.append(robust_acc)\n",
    "        \n",
    "        print(f\"  Clean Model: {clean_acc:.1f}%  |  Robust Model: {robust_acc:.1f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_vals = [0] + epsilons\n",
    "    clean_vals = [clean_accuracy] + clean_accs\n",
    "    robust_vals = [robust_clean_acc] + robust_accs\n",
    "    \n",
    "    plt.plot(x_vals, clean_vals, 'ro-', linewidth=3, markersize=8, label='Clean Model')\n",
    "    plt.plot(x_vals, robust_vals, 'bo-', linewidth=3, markersize=8, label='Robust Model')\n",
    "    \n",
    "    plt.xlabel('Attack Strength (ε)', fontsize=12)\n",
    "    plt.ylabel('Model Accuracy (%)', fontsize=12)\n",
    "    plt.title(f'Robustness Comparison: {attack_fn.__name__.upper()} Attack', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, max(epsilons))\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add annotations showing the improvement\n",
    "    for i, eps in enumerate(x_vals):\n",
    "        improvement = robust_vals[i] - clean_vals[i]\n",
    "        if improvement > 5:  # Only annotate significant improvements\n",
    "            plt.annotate(f'+{improvement:.0f}%', \n",
    "                        xy=(eps, (clean_vals[i] + robust_vals[i])/2),\n",
    "                        xytext=(10, 0), textcoords='offset points',\n",
    "                        fontsize=10, color='green', weight='bold',\n",
    "                        arrowprops=dict(arrowstyle='->', color='green'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return clean_accs, robust_accs\n",
    "\n",
    "eps_comparison = [0.01, 0.02, 0.03, 0.05, 0.07]\n",
    "clean_robustness_comp, robust_robustness_comp = plot_model_comparison(\n",
    "    clean_model, robust_model, testloader, fgsm_attack, eps_comparison, max_batches=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ccdef-b493-44c9-9e49-87e2d1dcf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tradeoffs(clean_model, robust_model, testloader, attack_fn, epsilon):\n",
    "    \"\"\"Analyze the trade-offs between clean and robust accuracy\"\"\"\n",
    "    \n",
    "    print(f\"\\nPerformance Analysis:\")\n",
    "    print(f\"{'Metric':<25} {'Clean Model':<15} {'Robust Model':<15} {'Difference':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Clean accuracy\n",
    "    diff_clean = robust_clean_acc - clean_accuracy\n",
    "    print(f\"{'Clean Accuracy':<25} {clean_accuracy:<14.1f}% {robust_clean_acc:<14.1f}% {diff_clean:<+11.1f}%\")\n",
    "    \n",
    "    # Adversarial accuracy\n",
    "    def test_adv_accuracy(model, testloader, attack_fn, epsilon, max_batches=5):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            adv_inputs = attack_fn(model, inputs, labels, epsilon)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(adv_inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return 100 * correct / total\n",
    "    \n",
    "    clean_adv_acc = test_adv_accuracy(clean_model, testloader, attack_fn, epsilon)\n",
    "    robust_adv_acc = test_adv_accuracy(robust_model, testloader, attack_fn, epsilon)\n",
    "    diff_adv = robust_adv_acc - clean_adv_acc\n",
    "    \n",
    "    print(f\"{'Adversarial Accuracy':<25} {clean_adv_acc:<14.1f}% {robust_adv_acc:<14.1f}% {diff_adv:<+11.1f}%\")\n",
    "    \n",
    "    # Attack success rates\n",
    "    clean_attack_success = 100 - clean_adv_acc\n",
    "    robust_attack_success = 100 - robust_adv_acc\n",
    "    diff_attack = clean_attack_success - robust_attack_success\n",
    "    \n",
    "    print(f\"{'Attack Success Rate':<25} {clean_attack_success:<14.1f}% {robust_attack_success:<14.1f}% {diff_attack:<+11.1f}%\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nKey Insights:\")\n",
    "    if diff_clean < 0:\n",
    "        print(f\"   • Robust model sacrifices {abs(diff_clean):.1f}% clean accuracy for robustness\")\n",
    "    else:\n",
    "        print(f\"   • Robust model maintains clean accuracy (+{diff_clean:.1f}%)\")\n",
    "    \n",
    "    print(f\"   • Robust model reduces attack success by {diff_attack:.1f} percentage points\")\n",
    "    print(f\"   • Trade-off ratio: {abs(diff_clean/diff_attack):.2f}% clean accuracy per 1% robustness gain\")\n",
    "    \n",
    "    return {\n",
    "        'clean_acc_diff': diff_clean,\n",
    "        'adv_acc_diff': diff_adv,\n",
    "        'attack_success_reduction': diff_attack\n",
    "    }\n",
    "\n",
    "tradeoffs = analyze_tradeoffs(clean_model, robust_model, testloader, fgsm_attack, 0.03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4eeb93-9c32-4a90-a9ea-9b0d8de3c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_differences(clean_model, robust_model, images, labels, num_examples=8):\n",
    "    \"\"\"Visualize differences in how models process the same images\"\"\"\n",
    "    \n",
    "    clean_model.eval()\n",
    "    robust_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        clean_outputs = clean_model(images[:num_examples])\n",
    "        robust_outputs = robust_model(images[:num_examples])\n",
    "        \n",
    "        clean_probs = F.softmax(clean_outputs, dim=1)\n",
    "        robust_probs = F.softmax(robust_outputs, dim=1)\n",
    "        \n",
    "        clean_preds = torch.argmax(clean_outputs, dim=1)\n",
    "        robust_preds = torch.argmax(robust_outputs, dim=1)\n",
    "    \n",
    "    # Find examples where models disagree\n",
    "    disagreements = clean_preds != robust_preds\n",
    "    agreement_rate = (1 - disagreements.float().mean()) * 100\n",
    "    \n",
    "    print(f\"\\n🤔 Model Agreement Analysis:\")\n",
    "    print(f\"Models agree on {agreement_rate:.1f}% of examples\")\n",
    "    \n",
    "    if disagreements.sum() > 0:\n",
    "        print(\"Examples where models disagree:\")\n",
    "        disagree_indices = torch.where(disagreements)[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, min(4, len(disagree_indices)), figsize=(12, 4))\n",
    "        if len(disagree_indices) == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        images_viz = denormalize_images(images[:num_examples])\n",
    "        \n",
    "        for i, idx in enumerate(disagree_indices[:4]):\n",
    "            if i >= 4:\n",
    "                break\n",
    "                \n",
    "            # Clean model view\n",
    "            axes[0, i].imshow(images_viz[idx].permute(1, 2, 0).clamp(0, 1))\n",
    "            clean_conf = clean_probs[idx, clean_preds[idx]].item()\n",
    "            axes[0, i].set_title(f'Clean Model\\n{classes[clean_preds[idx]]}\\n({clean_conf:.2f})', fontsize=10)\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Robust model view\n",
    "            axes[1, i].imshow(images_viz[idx].permute(1, 2, 0).clamp(0, 1))\n",
    "            robust_conf = robust_probs[idx, robust_preds[idx]].item()\n",
    "            axes[1, i].set_title(f'Robust Model\\n{classes[robust_preds[idx]]}\\n({robust_conf:.2f})', fontsize=10)\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Model Disagreements (True: {classes[labels[disagree_indices[0]]]})', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_model_differences(clean_model, robust_model, sample_images, sample_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceff83d-45b6-4fd2-a9d3-157571523775",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "There are obvious findings in this toy example. \n",
    "First, we have a simple CNN with 3 convolutional layers and 2 fully-connected layers, trained on the CIFAR10 dataset of 10 classes of 32 x 32 pixel images.\n",
    "Secondly, we implemented the FGSM and PGD attacks using epsilons from 0.005 -> 0.1. These examples were met with a significant drop in accuracy in our model's classification abilities of around 30 percent.\n",
    "Thirdly, our robust model trained on both clean and adversarial examples had a minor drop of 3 percent in classification accuracy on clean samples, and proved more resistant to misclassifying adversarial examples, with accuracy falling around 18 percent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
