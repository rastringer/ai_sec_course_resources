{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9827d-9fa8-4d6e-8d80-33438d6faf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c91fb5-6ff3-440a-8a7b-0e5894b3599c",
   "metadata": {},
   "source": [
    "# Activations and more on networks\n",
    "\n",
    "In the last notebook, we:\n",
    "* Built a quadratic function\n",
    "* Added MSE loss to calculate how wrong we were\n",
    "* Used gradient descent to learn how to change our weights\n",
    "* Learned how backpropagation can update weights through complex functions\n",
    "\n",
    "Now we can go from a singular node doing all of the above, to a neural network of several.\n",
    "\n",
    "### Understanding neural network architectures\n",
    "\n",
    "Neural nets generalize to data and a task by having any number (sometimes millions) of computation nodes that make predictions and update weights that inform those predictions. \n",
    "\n",
    "### Deep learning vs Machine Learning\n",
    "\n",
    "'Deep' learning emerged as a term to indicate additional hidden layers in a neural network. The majority of network architectures for any problem (vision, text, forecasting etc) fall into this category now, so it's become unneccesary to specify between the two. Throughout this course, I use machine learning and deep learning interchangeably, since we are generally concerned with networks of several hidden layers. \n",
    "\n",
    "![](https://github.com/rastringer/code_first_ml/blob/main/images/flowers_example.png?raw=true)\n",
    "\n",
    "Here we see a typical diagram of a neural network. This net classifies flowers based on input features such as sepal and petal measurements. From left to right, we see:\n",
    "* The input (feature) layer\n",
    "* Layers of calculation nodes that perform a linear calculation, `weight * input + bias`\n",
    "* A final output layer determining the probabilities the flower is of a particular kind.\n",
    "\n",
    "\n",
    "Today, we will build our own simple neural network in vanilla Python with some Numpy. This will help us solidify our understanding of the concepts before we start using frameworks (PyTorch) that make the process far easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec7946-d511-477b-a0a6-fa0a066c72fe",
   "metadata": {},
   "source": [
    "### Activations\n",
    "\n",
    "There's one last component we haven't yet covered which we'll now explore: the activation function. If we use our network in its current form, nodes will simply perform linear calculations. This linearity will lead to results which won't generalize well to large datasets and complex tasks. Without activations, all we have is linear regression.\n",
    "\n",
    "If our nodes perform the linear calculation, $Wx + b$, then three layers would give us:\n",
    "\n",
    "$W_3(W_2(W_1 + b_1) + b_2) + b_3$\n",
    "\n",
    "This would simplify to $Ax + b$\n",
    "\n",
    "This means our network would only learn straight lines and flat planes. Activations add bends, curves and sharp turns that allow a model to consider complex, wiggly patterns in input data. Activations turn linear networks into a universal function approximator.\n",
    "\n",
    "#### What's a universal function approximator?\n",
    "The universal approximation theorem states that with enough neurons and non-linear activation functions, we can approximate any continuous function to arbitrary precision. In similar fashion, with enough toy blocks, you could build any tower. This is how neural networks can perform varied tasks such as recognizing geographical images, play games and write poetry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6d6a3-6b3d-41a2-ba53-55df8538bceb",
   "metadata": {},
   "source": [
    "Let's explore some common activations. \n",
    "\n",
    "One point we skipped over above is that rather than doing calculations on one weight and input, our nodes are doing a weighted sum calculation of multiple (potentially thousands or millions) of inputs. So we do something along these lines:\n",
    "\n",
    "`weighted_sum = (weight1 * input1) + (weight2 * input2) + ... + bias`\n",
    "\n",
    "and our output will be calcuated by the activation function:\n",
    "\n",
    "`output = activation(weighted_sum)`\n",
    "\n",
    "The activation roughly mimics our biological neurons, where if an input stimulus reaches a certain threshold, we send further information to act (eg if the pan feels too hot, put it back down). \n",
    "\n",
    "### Introducing the Sigmoid\n",
    "\n",
    "![Sigmoid](./images/sigmoid.png)\n",
    "\n",
    "*Image from Wikipedia* \n",
    "\n",
    "A good activation to start with is the Sigmoid. It's a smooth decision maker, which takes any number and 'squashes' it into a probability-like value of between 0 and 1. Going back to the biological neuron analogy, it will 'fire' for values that pass a threshold to be close to 1, and will be quiet for values close to 0. \n",
    "\n",
    "This activation has several benefits:\n",
    "* Smooth and differentiable, so useful for backpropagation\n",
    "* Non-linear, so helps for learning complex patterns\n",
    "* Squashes large inputs to prevent exploding values\n",
    "* Prevents vanishing gradients (where derivatives can approach zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d936a-1ead-4f44-8514-a151c1853590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(self, x):\n",
    "    \"\"\"Sigmoid activation function - squashes values between 0 and 1\"\"\"\n",
    "    # Clip x to prevent overflow\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5fb60-0a89-48b8-be84-174394db31a3",
   "metadata": {},
   "source": [
    "The 'clipping' [-500, 500] prevents the exponential from causing numerical overflow - a practical detail that keeps the mathematics stable in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb0422-f506-4dfd-8a38-8a60087292cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "fig.suptitle('Sigmoid Function in Neural Networks', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_derivative = sigmoid_derivative(x)\n",
    "\n",
    "# Plot 1: Basic sigmoid function\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y_sigmoid, 'b-', linewidth=3, label='σ(x) = 1/(1+e^(-x))')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='y = 0.5')\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.7, label='x = 0')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output σ(x)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.legend()\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Approaches 1', xy=(5, 0.99), xytext=(6, 0.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "plt.annotate('Approaches 0', xy=(-5, 0.01), xytext=(-6, 0.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "# Plot 2: Sigmoid vs other activation functions\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y_sigmoid, 'b-', linewidth=3, label='Sigmoid')\n",
    "plt.plot(x, np.tanh(x), 'g-', linewidth=3, label='Tanh')\n",
    "plt.plot(x, np.maximum(0, x), 'r-', linewidth=3, label='ReLU')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Sigmoid vs Other Activation Functions')\n",
    "plt.legend()\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "# Plot 3: Sigmoid derivative (for understanding gradients)\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "plt.plot(x, y_derivative, 'purple', linewidth=3, label=\"σ'(x) = σ(x)(1-σ(x))\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Derivative')\n",
    "plt.title('Sigmoid Derivative (Gradient)')\n",
    "plt.legend()\n",
    "\n",
    "# Add annotation for vanishing gradient\n",
    "plt.annotate('Vanishing Gradient\\n(small derivatives)', \n",
    "            xy=(-8, sigmoid_derivative(-8)), xytext=(-6, 0.15),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# Plot 4: Sigmoid in action - simple classification example\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "x_sample = np.random.randn(100)\n",
    "y_sample = (x_sample > 0).astype(int) + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Plot data points\n",
    "colors = ['red' if y < 0.5 else 'blue' for y in y_sample]\n",
    "plt.scatter(x_sample, y_sample, c=colors, alpha=0.6, s=30)\n",
    "\n",
    "# Plot sigmoid decision boundary\n",
    "x_boundary = np.linspace(-3, 3, 100)\n",
    "y_boundary = sigmoid(x_boundary)\n",
    "plt.plot(x_boundary, y_boundary, 'k-', linewidth=3, label='Sigmoid Decision Boundary')\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Classification Threshold')\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Sigmoid for Binary Classification')\n",
    "plt.legend()\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957b458-fbb1-4af8-8fb3-cfe1f33d8975",
   "metadata": {},
   "source": [
    "### The ReLU\n",
    "\n",
    "ReLU (Rectified Linear Unit) is probably the most widely used activation function in modern neural networks. It's deceptively simple - if the input is positive (eg > 0), it outputs the input. If the input is negative, it outputs zero. Consider the ReLU a gate that allows positive signals to pass through the network, discarding negative ones. \n",
    "\n",
    "Here's the mathematical notation:\n",
    "\n",
    "$f(x) = max(0,x)$\n",
    "\n",
    "ReLU has become fairly ubiquitos in modern machine learning since sigmoid and other activation functions have gradients that can become very small for large input values, making it hard to train deep networks. This is known as the *vanishing gradient problem*. \n",
    "Since ReLU has a constant gradient of 1 for positive inputs, gradients flow back through the network efficiently.\n",
    "Other benefits include:\n",
    "* Fast to compute\n",
    "* Naturally creates sparsity by zeroing out negative activations. This can lead to more efficient models.\n",
    "\n",
    "Here we are in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5d5cb-c188-437e-8a2b-cad94cf06f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU activation function - sets negative values to 0\"\"\"\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982339d-2576-498f-ad04-870243f3d192",
   "metadata": {},
   "source": [
    "Again, we will apply this function to our random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f824e8c-10b6-4a18-be04-2f1ef1eb67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input values from -5 to 5\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = relu(x)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='ReLU(x)')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output ReLU(x)')\n",
    "plt.title('ReLU Activation Function: f(x) = max(0, x)')\n",
    "plt.legend()\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('f(x) = 0 for x < 0', xy=(-3, 0), xytext=(-3, 1),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=10, color='red')\n",
    "plt.annotate('f(x) = x for x ≥ 0', xy=(3, 3), xytext=(2, 4),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=10, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6b4c6-d8cd-4847-bf9b-9f01239d920f",
   "metadata": {},
   "source": [
    "### The Complete QuadraticModel Class\n",
    "\n",
    "Now we have our inputs, weight assignment, ReLU, loss function, gradient descent and backpropagation, we can apply our model class to the toy problem again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed419a4a-9acb-4e15-bd2d-c5d7ed0a9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error loss function\"\"\"\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU function\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class QuadraticModel:\n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.a = np.random.randn()\n",
    "        self.b = np.random.randn()\n",
    "        self.c = np.random.randn()\n",
    "        self.predictions_history = []\n",
    "        \n",
    "        # Store intermediate values for backpropagation\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass - compute predictions and cache intermediate values\"\"\"\n",
    "        # Cache inputs for backpropagation\n",
    "        self.cache['x'] = x\n",
    "        self.cache['x_squared'] = x**2\n",
    "        \n",
    "        # Compute prediction\n",
    "        # y_pred = self.a * self.cache['x_squared'] + self.b * x + self.c\n",
    "        # Compute quadratic output (before activation)\n",
    "        z = self.a * self.cache['x_squared'] + self.b * x + self.c\n",
    "        self.cache['z'] = z  # Cache pre-activation values\n",
    "        y_pred = relu(z)\n",
    "        self.cache['relu_mask'] = (z > 0).astype(float)  # Cache ReLU derivative\n",
    "        self.cache['y_pred'] = y_pred\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Alias for forward pass (for compatibility)\"\"\"\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backpropagation - compute gradients using cached values\"\"\"\n",
    "        if 'y_pred' not in self.cache:\n",
    "            raise ValueError(\"Must call forward() before backward()\")\n",
    "        \n",
    "        # Get cached values\n",
    "        x = self.cache['x']\n",
    "        x_squared = self.cache['x_squared']\n",
    "        y_pred = self.cache['y_pred']\n",
    "        \n",
    "        # Compute error (derivative of loss w.r.t. predictions)\n",
    "        error = y_pred - y_true\n",
    "        N = len(x)\n",
    "\n",
    "        # Backpropagate through the model\n",
    "        # dL/dy_pred = 2 * (y_pred - y_true) / N  (from MSE loss)\n",
    "        # But we can work directly with error = y_pred - y_true\n",
    "\n",
    "        relu_mask = self.cache['relu_mask']\n",
    "        dL_dz = (2/N) * error * relu_mask\n",
    "        \n",
    "        # Gradients w.r.t. parameters\n",
    "        dL_da = (2/N) * np.sum(error * x_squared)  # ∂L/∂a = ∂L/∂y_pred * ∂y_pred/∂a\n",
    "        dL_db = (2/N) * np.sum(error * x)          # ∂L/∂b = ∂L/∂y_pred * ∂y_pred/∂b  \n",
    "        dL_dc = (2/N) * np.sum(error)              # ∂L/∂c = ∂L/∂y_pred * ∂y_pred/∂c\n",
    "        \n",
    "        # Store gradients\n",
    "        self.gradients = {\n",
    "            'dL_da': dL_da,\n",
    "            'dL_db': dL_db,\n",
    "            'dL_dc': dL_dc\n",
    "        }\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = mse_loss(y_pred, y_true)\n",
    "        \n",
    "        return loss, dL_da, dL_db, dL_dc\n",
    "    \n",
    "    def compute_gradients(self, x, y_true):\n",
    "        \"\"\"Combined forward and backward pass (for compatibility)\"\"\"\n",
    "        self.forward(x)\n",
    "        return self.backward(y_true)\n",
    "    \n",
    "    def update_parameters(self, grads, lr):\n",
    "        \"\"\"Update parameters using gradients\"\"\"\n",
    "        if isinstance(grads, tuple):\n",
    "            # Handle old format: (loss, dL_da, dL_db, dL_dc)\n",
    "            _, dL_da, dL_db, dL_dc = grads\n",
    "        else:\n",
    "            # Handle new format: dict\n",
    "            dL_da = grads['dL_da']\n",
    "            dL_db = grads['dL_db'] \n",
    "            dL_dc = grads['dL_dc']\n",
    "        \n",
    "        # Gradient descent update\n",
    "        self.a -= lr * dL_da\n",
    "        self.b -= lr * dL_db\n",
    "        self.c -= lr * dL_dc\n",
    "    \n",
    "    def train(self, x, y, epochs=100, lr=0.01, verbose=True):\n",
    "        \"\"\"Training loop using explicit forward/backward passes\"\"\"\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(x)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss, dL_da, dL_db, dL_dc = self.backward(y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(self.gradients, lr)\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Logging and history tracking\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                self.predictions_history.append((epoch, y_pred.copy()))\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:3}: a={self.a:.3f}, b={self.b:.3f}, c={self.c:.3f}, Loss={loss:.3f}\")\n",
    "        \n",
    "        return loss_history\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        \"\"\"Reset gradients (useful for more complex training loops)\"\"\"\n",
    "        self.gradients = {\n",
    "            'dL_da': 0.0,\n",
    "            'dL_db': 0.0,\n",
    "            'dL_dc': 0.0\n",
    "        }\n",
    "        self.cache = {}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic quadratic data\n",
    "    np.random.seed(123)\n",
    "    x_train = np.linspace(-2, 2, 100)\n",
    "    true_a, true_b, true_c = 1.5, -0.5, 2.0\n",
    "    y_train = true_a * x_train**2 + true_b * x_train + true_c + 0.1 * np.random.randn(100)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = QuadraticModel(seed=42)\n",
    "    print(\"Training with explicit backpropagation:\")\n",
    "    print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}\")\n",
    "    print(f\"Initial parameters: a={model.a:.3f}, b={model.b:.3f}, c={model.c:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    loss_history = model.train(x_train, y_train, epochs=50, lr=0.01)\n",
    "    \n",
    "    print(f\"\\nFinal parameters: a={model.a:.3f}, b={model.b:.3f}, c={model.c:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
