{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa303516-68b2-4732-b4d5-c93d513bebfe",
   "metadata": {},
   "source": [
    "# A Simple Neural Network\n",
    "\n",
    "Now we have an understanding of how models learn, let's make a simple neural network. Again, we will do so in vanilla Python to get a stronger awareness of the key concepts before we learn the relative ease of using PyTorch.\n",
    "\n",
    "We can also start looking at some real data! The canonical starter for many ML adventures is the MNIST dataset, which comprises black and white images of hand-drawn digits between 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1a753-afe5-49e6-af5a-6e76e5870219",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn fetch_openml --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667488ac-8df4-4c61-b2f1-6f4f90d453cd",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "Firstly, here is a simple function to import the dataset. We will use scikit-learn, a popular scientific programming library for Python which also comprises the MNIST and other datasets.\n",
    "\n",
    "Notice we split the dataset of images into `train` and `test` sets. The majority of images are used to train the model, some are held back to test it. The `X` refers to the training and testing images, the `y` to the respective labels (0, 1, 2 etc). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a177f-5e35-4dd9-b78e-b3525529eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def load_mnist_sample(n_samples=5000):\n",
    "    \"\"\"Load a sample of MNIST data for quick experimentation\"\"\"\n",
    "    print(\"Loading MNIST data...\")\n",
    "    \n",
    "    # Load MNIST from sklearn\n",
    "    mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Take a random sample for faster training\n",
    "    indices = random.sample(range(len(X)), min(n_samples, len(X)))\n",
    "    X_sample = X.iloc[indices].values if hasattr(X, 'iloc') else X[indices]\n",
    "    y_sample = y[indices]\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_sample = X_sample / 255.0\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_onehot = np.zeros((len(y_sample), 10))\n",
    "    y_onehot[np.arange(len(y_sample)), y_sample] = 1\n",
    "    \n",
    "    # Split into train and test\n",
    "    split_idx = int(0.8 * len(X_sample))\n",
    "    X_train, X_test = X_sample[:split_idx], X_sample[split_idx:]\n",
    "    y_train, y_test = y_onehot[:split_idx], y_onehot[split_idx:]\n",
    "\n",
    "    print(\"MNIST data downloaded\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_mnist_sample(n_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8a266-0af3-461c-bcf2-d1c4b837f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# Display a few random MNIST digits from your training set\n",
    "fig, axs = plt.subplots(1, 5, figsize=(7, 4), sharex=True, sharey=True)   \n",
    "# Set parameters for subplot (num_rows, num_cols, figure size etc.)\n",
    "\n",
    "for i in range(5):    # Iterate over a few images to display\n",
    "    axs[i].imshow(X_train[i].reshape(28, 28), cmap='binary')   # Reshape the image from (784,) array format into an 28x28 grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c625162-3fec-47ac-8070-60ed3bdda8c5",
   "metadata": {},
   "source": [
    "### What exactly is our dataset?\n",
    "\n",
    "Our images of hand-drawn numbers are just arrays of pixel values, showing various floats depending on whether the pixel is pure white, black, or in between. \n",
    "\n",
    "If we check the type, we see we are working with a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9eff0-f667-48ea-886f-9cbba1a8153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af71be2-c60b-4494-8eba-2873224d96b5",
   "metadata": {},
   "source": [
    "There are 784 values for each image. Notice when we print the array that many values are 0., this indicates white / blank space around the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46961871-30d1-4959-b8f9-d8aa14f3e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6aa64-ef40-4c95-8e67-c0ed160b2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6ce24-c139-4660-90e2-c0799e772b03",
   "metadata": {},
   "source": [
    "### Making a Neural Network\n",
    "\n",
    "Our code will make some changes to the QuadraticModel class.\n",
    "We will also use a new loss function, \n",
    "**Cross entropy**, which is a good choice for classification tasks (whereas MSE \n",
    "can be best for regression). Cross entropy loss measures how far our predicted probability distribution is from the true distribution. \n",
    "\n",
    "Another new feature is **one hot encoding**. This is a method of representing categorical data (digits 0-9 in this case) as binary vectors. \n",
    "0 is represented as [1,0,0,0,0,0,0,0,0,0]\n",
    "1 is [0,1,0,0,0,0,0,0,0,0]\n",
    "2 is [0,0,1,0,0,0,0,0,0,0] etc.\n",
    "\n",
    "We also use a **softmax** function in the output layer to produce a probability distribution over all classes, outputting values that sum to 1. \n",
    "So if we test the model on a handwritten '2', its output would be:\n",
    "\n",
    "**0**: 0.1\n",
    "\n",
    "**1**: 0.3\n",
    "\n",
    "**2**: 0.9\n",
    "\n",
    "**3**: 0.3\n",
    "\n",
    "etc, with 2 as the obvious favourite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b382ae2-2d60-4060-a7c7-c803e11ef93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple neural network implementation to emphasize core concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of integers representing the size of each layer\n",
    "                        e.g., [784, 128, 64, 10] for MNIST\n",
    "            learning_rate: Step size for gradient descent\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        \n",
    "        # Initialize weights and biases using small random values\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier initialization for better convergence\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function - squashes values between 0 and 1\"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function - sets negative values to 0\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation for output layer - converts to probabilities\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Making predictions!\n",
    "        Forward propagation through the network, where we feed data through \n",
    "        layer by layer.\n",
    "        Each layer transforms the input using weights and biases, then applies an \n",
    "        activation function, so output = activation(input @ weights + bias)\n",
    "        The final layer gives us probabilities for each class.\n",
    "        \"\"\"\n",
    "        self.activations = [X]  # Store activations for backprop\n",
    "        self.z_values = []      # Store pre-activation values\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        # Go through each layer\n",
    "        for i in range(len(self.weights)):\n",
    "            # Linear transformation: z = input @ weights + bias\n",
    "            z = current_input @ self.weights[i] + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            # Apply activation function\n",
    "            if i == len(self.weights) - 1:  # Output layer\n",
    "                activation = self.softmax(z)\n",
    "            else:  # Hidden layers\n",
    "                activation = self.relu(z)\n",
    "            \n",
    "            self.activations.append(activation)\n",
    "            current_input = activation\n",
    "        \n",
    "        return self.activations[-1]  # Return final output\n",
    "    \n",
    "    def compute_loss(self, predictions, true_labels):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        L = -[y*log(p) + (1-y)*log(1-p)]\n",
    "        This measures how wrong our predictions are.\n",
    "        Lower loss = better predictions!\n",
    "        \"\"\"\n",
    "        # Avoid log(0) by adding small epsilon\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(np.sum(true_labels * np.log(predictions), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, true_labels):\n",
    "        \"\"\"\n",
    "        Backpropagation - calculate how much each weight contributed\n",
    "        to the error, adjust weights to reduce that error.\n",
    "        \"\"\"\n",
    "        batch_size = true_labels.shape[0]\n",
    "        \n",
    "        # Start with the output layer error\n",
    "        # This is the derivative of loss with respect to output\n",
    "        output_error = self.activations[-1] - true_labels\n",
    "        \n",
    "        # Store gradients for weights and biases\n",
    "        weight_gradients = []\n",
    "        bias_gradients = []\n",
    "        \n",
    "        # Work backwards through the layers\n",
    "        current_error = output_error\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Gradient for weights: previous_activation^T @ current_error\n",
    "            weight_grad = self.activations[i].T @ current_error / batch_size\n",
    "            weight_gradients.insert(0, weight_grad)\n",
    "            \n",
    "            # Gradient for biases: mean of current_error\n",
    "            bias_grad = np.mean(current_error, axis=0, keepdims=True)\n",
    "            bias_gradients.insert(0, bias_grad)\n",
    "            \n",
    "            # If not the first layer, compute error for previous layer\n",
    "            if i > 0:\n",
    "                # Error propagates backwards: current_error @ weights^T\n",
    "                error_from_next = current_error @ self.weights[i].T\n",
    "                \n",
    "                # Apply derivative of activation function\n",
    "                activation_derivative = self.relu_derivative(self.z_values[i-1])\n",
    "                current_error = error_from_next * activation_derivative\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def update_parameters(self, weight_gradients, bias_gradients):\n",
    "        \"\"\"\n",
    "        Gradient descent update step.\n",
    "        \n",
    "        We adjust weights in the opposite direction of the gradient\n",
    "        to minimize the loss function.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * weight_gradients[i]\n",
    "            self.biases[i] -= self.learning_rate * bias_gradients[i]\n",
    "    \n",
    "    def train_batch(self, X, y):\n",
    "        \"\"\"Train on a single batch of data\"\"\"\n",
    "        # Forward pass: make predictions\n",
    "        predictions = self.forward_pass(X)\n",
    "        \n",
    "        # Compute loss: how wrong are we?\n",
    "        loss = self.compute_loss(predictions, y)\n",
    "        \n",
    "        # Backward pass: calculate gradients\n",
    "        weight_grads, bias_grads = self.backward_pass(y)\n",
    "        \n",
    "        # Update parameters: improve the weights\n",
    "        self.update_parameters(weight_grads, bias_grads)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"Calculate accuracy on a dataset\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y, axis=1)\n",
    "        return np.mean(predicted_classes == true_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658561dc-a09a-4d55-83e9-74f344129caf",
   "metadata": {},
   "source": [
    "Our model looks like this:\n",
    "```\n",
    " Input     Hidden1    Hidden2    Output\n",
    "  784  →    128   →    64   →    10\n",
    "   ●         ●          ●         ● 0\n",
    "   ●    ╲    ●     ╲    ●    ╲    ● 1  \n",
    "   ●     ╲   ●      ╲   ●     ╲   ● 2\n",
    "   ●      ╲  ●       ╲  ●      ╲  ● 3\n",
    "   ●       ╲ ●        ╲ ●       ╲ ● 4\n",
    "   ●        ╲●         ╲●        ╲● 5\n",
    "   ●        ╱●         ╱●        ╱● 6\n",
    "   ●       ╱ ●        ╱ ●       ╱ ● 7\n",
    "   ●      ╱  ●       ╱  ●      ╱  ● 8\n",
    "   ●     ╱   ●      ╱   ●     ╱   ● 9\n",
    "   ●    ╱    ●     ╱    ●    ╱\n",
    "   ●         ●          ●\n",
    "   ⋮         ⋮          ⋮\n",
    "\n",
    "```\n",
    "\n",
    "We have a model of 4 layers: \n",
    "\n",
    "* Layer 1: takes in 784 pixel values from the 28x28 flattened images\n",
    "\n",
    "* Layers 2 and 3 process the inputs through hidden layers of 128 and 64 neurons with ReLU activation\n",
    "\n",
    "* The output layer represents the 10 probabilitiy scores that the input is a 1, or 3, or 5 etc using softmax activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65012e8a-b6d6-406a-8e75-268f2281e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_sample(n_samples=5000):\n",
    "    \"\"\"Load a sample of MNIST data for quick experimentation\"\"\"\n",
    "    print(\"Loading MNIST data...\")\n",
    "    \n",
    "    # Load MNIST from sklearn\n",
    "    mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Take a random sample for faster training\n",
    "    indices = random.sample(range(len(X)), min(n_samples, len(X)))\n",
    "    X_sample = X.iloc[indices].values if hasattr(X, 'iloc') else X[indices]\n",
    "    y_sample = y[indices]\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_sample = X_sample / 255.0\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_onehot = np.zeros((len(y_sample), 10))\n",
    "    y_onehot[np.arange(len(y_sample)), y_sample] = 1\n",
    "    \n",
    "    # Split into train and test\n",
    "    split_idx = int(0.8 * len(X_sample))\n",
    "    X_train, X_test = X_sample[:split_idx], X_sample[split_idx:]\n",
    "    y_train, y_test = y_onehot[:split_idx], y_onehot[split_idx:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ca1d-60c1-4c36-85ff-eea4586a2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1b902-48df-4607-b3d5-ab859f8b6b00",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Now that we have the model, training and test data, here's our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c6794-00d7-4243-8574-617cda6e36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"The neural network training process\"\"\"\n",
    "    # Load data\n",
    "    X_train, X_test, y_train, y_test = load_mnist_sample(n_samples=2000)\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\\n\")\n",
    "    \n",
    "    # Create network\n",
    "    # Architecture: 784 input -> 128 hidden -> 64 hidden -> 10 output\n",
    "    nn = SimpleNeuralNetwork([784, 128, 64, 10], learning_rate=0.1)\n",
    "    print(\"Network architecture:\", nn.layer_sizes)\n",
    "    print(f\"Total parameters: {sum(w.size + b.size for w, b in zip(nn.weights, nn.biases))}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"Epoch | Train Loss | Train Acc | Test Acc\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        # Train in batches\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train_shuffled[i:i+batch_size]\n",
    "            batch_y = y_train_shuffled[i:i+batch_size]\n",
    "            \n",
    "            loss = nn.train_batch(batch_X, batch_y)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Calculate metrics every 5 epochs\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            train_acc = nn.accuracy(X_train, y_train)\n",
    "            test_acc = nn.accuracy(X_test, y_test)\n",
    "            \n",
    "            train_losses.append(avg_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "            \n",
    "            print(f\"{epoch:5d} | {avg_loss:10.4f} | {train_acc:9.3f} | {test_acc:8.3f}\")\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Training Accuracy: {train_accuracies[-1]:.3f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracies[-1]:.3f}\")\n",
    "    \n",
    "    # Show some predictions\n",
    "    print(f\"\\nSample Predictions:\")\n",
    "    sample_indices = np.random.choice(len(X_test), 5)\n",
    "    predictions = nn.predict(X_test[sample_indices])\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        predicted_class = np.argmax(predictions[i])\n",
    "        true_class = np.argmax(y_test[idx])\n",
    "        confidence = predictions[i][predicted_class]\n",
    "        print(f\"Sample {i+1}: Predicted={predicted_class}, True={true_class}, Confidence={confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c883d-94e2-4317-bc20-5fa8e6555c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ec613-208f-4772-ae73-4631721a31a8",
   "metadata": {},
   "source": [
    "### \n",
    "Summary\n",
    "\n",
    "We trained a simple neural network by:\n",
    "* Feeding the data through the network, layer by layer\n",
    "* Each layer transforms the input using weights and biases, then\n",
    "  applies an activation function\n",
    "* The final layer gives us probabilities for each class\n",
    "\n",
    "Other key concepts:\n",
    "* The loss function measures how wrong our predictions are. We use cross-entropy loss\n",
    "* Backprogagation calculates how much each weight contributed to the error by\n",
    "  working backwards through the network. We learn to adjust each weight to reduce the error\n",
    "* Gradient descent is the process of updating the weights in the opposite direction of the gradient\n",
    "* Activations add non-linearity to the network. ReLU sets negative values to 0\n",
    "* The learning rate controls the size of steps we make when updating weights. Too large, we\n",
    "  may overshoot the minimum; too small will mean slow training\n",
    "\n",
    "Now we have a solid understanding of ML fundemantals, we will learn to use the PyTorch framework in the next notebook to make our experiments easier.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
