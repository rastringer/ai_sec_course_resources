{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a04c82-1be1-4255-954f-5652604b1e7c",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd594f5b-e106-4bb2-8780-27d298e45081",
   "metadata": {},
   "source": [
    "Machine learning has become the dominant approach in the field of Artificial Intelligence (AI), a broad term encompassing efforts to make computational models that can perform complex tasks without the need for human guidance.\n",
    "\n",
    "In this module, we will develop an intuitive understanding of of how computers learn from data. This will give us a good foundation for the rest of the course, and help us to understand some of the more cutting-edge model architectures used in text understanding and generation.\n",
    "\n",
    "## A Brief History\n",
    "\n",
    "The concept of machines learning how to do things has been discussed by scientists for decades. \n",
    "\n",
    "In 1943, Warren McCulloch and Walter Pitts [explored](https://home.csulb.edu/~cwallis/382/readings/482/mccolloch.logical.calculus.ideas.1943.pdf) a mathematical model of our brains and nervous systems as a net of 'neurons' that respond to stimuli when 'excitation' exceeds a threshold. \n",
    "\n",
    "Later, Frank Rosenblatt worked on the 'percepton' at Cornell University, writing that, *\"We are about to witness the birth of...a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.\"*\n",
    "\n",
    "<img src=\"https://news.cornell.edu/sites/default/files/styles/full_size/public/2019-09/0925_rosenblatt5.jpg?itok=7UpHtbRj\" width=450>\n",
    "Rosenblatt with the Mark I Perceptron, which performed image classification of some shapes successfully.\n",
    "\n",
    "A decade later, in 1959, Arthur Samuel, a researcher at IBM, [wrote](https://people.csail.mit.edu/brooks/idocs/Samuel.pdf) that:\n",
    "\n",
    "\"A computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program\".\n",
    "\n",
    "The idea was emerging that computers could learn functions whereby they applied weights to input which then determined an output, such as whether the image is a square, or a circle.\n",
    "\n",
    "Machine learning has of course soared in complexity and achievement since the 1940s and 50s, with a particularly intensive period of evolution from the 2010s onwards. There are several reasons for this, including:\n",
    "* Advances in algorithms\n",
    "* Availability of computational resources such as accelerators, or GPUs\n",
    "* Varied and substantial datasets made available by the internet and by smartphones (representing the world and our behaviour)\n",
    "\n",
    "Arthur Samuel, a researcher at IBM succinctly described a process for machine learning in 1962:\n",
    "\n",
    "*\"Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximise the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.\"*\n",
    "\n",
    "What I hope emerges here is the idea that ML is an approach different to traditional computer programming, where we typically give a computer a task and the exact instructions how to accomplish that task, as in this diagram:\n",
    "\n",
    "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/input_program_output.png?raw=true\" width=650>\n",
    "\n",
    "Instead, we want to create functions which learn from data such that they can receive inputs, apply weights to the features of those inputs (more on this soon), then generate an output. The saved weights from this process if what we call a 'model'.\n",
    "\n",
    "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/input_model_backprop.png?raw=true\" width=650>\n",
    "\n",
    "\n",
    "In this lesson, we will explore the basics of a neural network, which still closely resemble the vision of McCulloch, Pitts, Rosenblatt and others, building up to our own Multi-Layer Perceptron to classify images.\n",
    "\n",
    "### The basics\n",
    "\n",
    "Let's explore how we can craft functions to do the following:\n",
    "* Make a prediction\n",
    "* Calculate a loss\n",
    "\n",
    "These are the essential first steps in ML. We make a prediction, and we need a metric to show how wrong we are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baeef32-6f8e-4544-88dc-e2592fa7ca3f",
   "metadata": {},
   "source": [
    "### Random Noise\n",
    "\n",
    "Firstly, we need some data to which we will try to fit a line. This could be predicting plant height from leaf size, or house price from square footage, for example. To keep things simple, we will try to fit a line to random 'noise'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fd1d0-43aa-4385-9c89-5e6c2c5f70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data generation (same as before)\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100)\n",
    "y_true = 2 * X**2 + 3 * X + 1\n",
    "noise = np.random.normal(0, 3, size=X.shape)\n",
    "y = y_true + noise\n",
    "\n",
    "# Prepare data as list of (x, y) tuples\n",
    "data = list(zip(X, y))\n",
    "\n",
    "plt.scatter(X, y, label='Noisy Data', alpha=0.6)\n",
    "plt.plot(X, y_true, color='black', linestyle='--', label='True Function')\n",
    "plt.legend()\n",
    "plt.title(\"Generated Noisy Quadratic Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab722e7-d540-4d65-bc12-70963be562f7",
   "metadata": {},
   "source": [
    "### Quadratic Function\n",
    "\n",
    "Quadratic functions are useful to model the trajectory of projectiles, arcs and parabolic shapes. They are often used in optimization problems, or to fit lines to data. \n",
    "\n",
    "The following function tries to predict weight from height, following this formula:\n",
    "\n",
    "$a ⋅ x^2 + b ⋅ x + c$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe7c28-41a2-4b10-a442-6a50842c03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_quadratic(x, a, b, c):\n",
    "    return a * x**2 + b * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214876f-956e-47e2-9a6b-95727b005b9d",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "There are a variety of loss functions used in ML; one of the most common is Mean Squared Error (MSE). MSE is a formal way of saying the 'average of our mistakes'.\n",
    "\n",
    "This function calculates the difference between a prediction and the actual value, then squares that value.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "In the mathematical notation, $n$ is the number of examples in our data, $y_i$ is the actual value of example $i$, $\\hat{y_i}$, pronounced \"y hat\" is the prediction.\n",
    "\n",
    "As with many calculations in ML, it may be easier to understand what's happening in the Python function. This function is more lines of code than what we would write in practice, however, showing the calculation steps in the loop should show exactly what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee74c4a-5c31-4dbc-b7c3-445d0f1540ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(data, a, b, c):\n",
    "    total_error = 0\n",
    "    for x, actual_y in data:\n",
    "        predicted_y = predict_quadratic(x, a, b, c)\n",
    "        error = predicted_y - actual_y\n",
    "        squared_error = error ** 2\n",
    "        total_error += squared_error \n",
    "\n",
    "    mse = total_error / len(data)\n",
    "    return mse\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c6202-2cb2-4ec1-bd34-4193e3bb703d",
   "metadata": {},
   "source": [
    "Now that we have an idea of how wrong our predictions are, we can try updating parameters to improve accuracy. \n",
    "\n",
    "In the following function, we take one parameter (a, b, c) at a time, and try increasing, decreasing and keeping it the same. We use MSE to calculate what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed6149-4397-403b-b1f2-4da8018acaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual parameter update using simple finite differences (crude update)\n",
    "def try_update_param(param, delta, fixed_params, data, param_index):\n",
    "    a, b, c = fixed_params\n",
    "    \n",
    "    # Try positive delta\n",
    "    params_pos = [a, b, c]\n",
    "    params_pos[param_index] = param + delta\n",
    "    loss_pos = mean_squared_error(data, *params_pos)\n",
    "    \n",
    "    # Try negative delta\n",
    "    params_neg = [a, b, c]\n",
    "    params_neg[param_index] = param - delta\n",
    "    loss_neg = mean_squared_error(data, *params_neg)\n",
    "    \n",
    "    # Current loss without change\n",
    "    params_curr = [a, b, c]\n",
    "    params_curr[param_index] = param\n",
    "    loss_curr = mean_squared_error(data, *params_curr)\n",
    "    \n",
    "    # Choose best option\n",
    "    if loss_pos < loss_curr and loss_pos < loss_neg:\n",
    "        return param + delta, loss_pos\n",
    "    elif loss_neg < loss_curr and loss_neg < loss_pos:\n",
    "        return param - delta, loss_neg\n",
    "    else:\n",
    "        return param, loss_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560da717-1d95-441d-b20a-38bb3384d333",
   "metadata": {},
   "source": [
    "This process is cumbersone because we have to evaluate the model multiple times, and there are no gradients (slopes) calculated, so just comparing up, down and same is limited. This scenario is comparable to trying to find your way down a mountain blindfolded, just feeling around. This analogy will soon be extended...\n",
    "\n",
    "Let's visualize our current training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71c570-0d86-4482-a8b6-4076eca5de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Initialize parameters randomly\n",
    "a, b, c = np.random.randn(3)\n",
    "delta = 0.1\n",
    "epochs = 50\n",
    "\n",
    "loss_history = []\n",
    "predictions_history = []\n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    a, loss_a = try_update_param(a, delta, (a, b, c), data, 0)\n",
    "    b, loss_b = try_update_param(b, delta, (a, b, c), data, 1)\n",
    "    c, loss_c = try_update_param(c, delta, (a, b, c), data, 2)\n",
    "    \n",
    "    loss = max(loss_a, loss_b, loss_c)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: a={a:.3f}, b={b:.3f}, c={c:.3f}, Loss={loss:.3f}\")\n",
    "        y_pred = predict_quadratic(X, a, b, c)\n",
    "        predictions_history.append((epoch, y_pred.copy()))\n",
    "\n",
    "end = datetime.now()\n",
    "manual_time = end-start\n",
    "print(f\"Time taken:{manual_time}\")\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss over epochs (crude manual update)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d5039-0ed7-4b5b-9ac6-0bb73a60ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, alpha=0.3, label=\"Noisy Data\")\n",
    "plt.plot(X, y_true, label=\"True Function\", color='black', linestyle='--')\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(predictions_history)))\n",
    "for (epoch, pred), color in zip(predictions_history, colors):\n",
    "    plt.plot(X, pred, color=color, label=f\"Epoch {epoch}\")\n",
    "\n",
    "plt.title(\"Prediction Curve Over Training\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6181c4-dadb-4989-9f3b-3b3fa15d3d14",
   "metadata": {},
   "source": [
    "### Vectorized MSE\n",
    "\n",
    "If you're familiar with Python, you have probably come across vectorized operations. Vectorization refers to the process of performing calculations across entire arrays, matricies or tensors (more on these soon) at once, rather than using loops to iterate through values one by one.\n",
    "\n",
    "#### Numpy\n",
    "\n",
    "Numpy is a popular scientific programming library for Python and we will use several of its methods today. Numpy makes vector operations easy - and will vastly simplify our `mean_squared_error` function. We'll give the updated function a new name to avoid confusion.\n",
    "Here again is MSE in vanilla Python:\n",
    "\n",
    "```\n",
    "def mean_squared_error(data, a, b, c):\n",
    "    total_error = 0\n",
    "    for x, actual_y in data:\n",
    "        predicted_y = predict_quadratic(x, a, b, c)\n",
    "        error = predicted_y - actual_y\n",
    "        squared_error = error ** 2\n",
    "        total_error += squared_error \n",
    "\n",
    "    mse = total_error / len(data)\n",
    "    return mse\n",
    "```\n",
    "\n",
    "In Numpy, we simply use the `np.mean` method across the squared difference between our predictions and actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3d941-7bc3-4f8d-ac2c-ee2b12545d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dad4e7-90a6-48fe-b89f-d17bac8209e6",
   "metadata": {},
   "source": [
    "Let's rewrite the function to update params using our new, vectorized MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf4424-3754-4f26-991d-a1d2a9dc6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_update_param(param, delta, fixed_params, x, y, param_index):\n",
    "    \"\"\"\n",
    "    param: current value of the parameter to update\n",
    "    delta: step size to try (+delta and -delta)\n",
    "    fixed_params: tuple/list of the other two parameters\n",
    "    param_index: index of param in (a,b,c) to know position\n",
    "    \n",
    "    Returns the updated param and the corresponding loss.\n",
    "    \"\"\"\n",
    "    a, b, c = fixed_params\n",
    "    \n",
    "    # Try positive delta\n",
    "    params_pos = [a, b, c]\n",
    "    params_pos[param_index] = param + delta\n",
    "    y_pred_pos = predict_quadratic(*params_pos, x)\n",
    "    loss_pos = mse_loss(y_pred_pos, y)\n",
    "    \n",
    "    # Try negative delta\n",
    "    params_neg = [a, b, c]\n",
    "    params_neg[param_index] = param - delta\n",
    "    y_pred_neg = predict_quadratic(*params_neg, x)\n",
    "    loss_neg = mse_loss(y_pred_neg, y)\n",
    "    \n",
    "    # Current loss without change\n",
    "    params_curr = [a, b, c]\n",
    "    params_curr[param_index] = param\n",
    "    y_pred_curr = predict_quadratic(*params_curr, x)\n",
    "    loss_curr = mse_loss(y_pred_curr, y)\n",
    "    \n",
    "    # Choose best option\n",
    "    if loss_pos < loss_curr and loss_pos < loss_neg:\n",
    "        return param + delta, loss_pos\n",
    "    elif loss_neg < loss_curr and loss_neg < loss_pos:\n",
    "        return param - delta, loss_neg\n",
    "    else:\n",
    "        return param, loss_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2294877-2532-49a6-9f1a-14bc5f7e2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = np.random.randn(3)\n",
    "delta = 0.1\n",
    "epochs = 50\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "start = datetime.now()\n",
    "for epoch in range(epochs):\n",
    "    # Update a\n",
    "    a, loss_a = try_update_param(a, delta, (a, b, c), X, y, 0)\n",
    "    # Update b\n",
    "    b, loss_b = try_update_param(b, delta, (a, b, c), X, y, 1)\n",
    "    # Update c\n",
    "    c, loss_c = try_update_param(c, delta, (a, b, c), X, y, 2)\n",
    "    \n",
    "    # Use the worst loss among the updates (or average, but worst is safe)\n",
    "    loss = max(loss_a, loss_b, loss_c)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: a={a:.3f}, b={b:.3f}, c={c:.3f}, Loss={loss:.3f}\")\n",
    "\n",
    "end = datetime.now()\n",
    "vectorized_time = end - start\n",
    "print(f\"Time taken:{vectorized_time}\")\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss over epochs (crude manual update)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06a1fc-a337-425f-94c8-e2cd062f595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, alpha=0.3, label=\"Noisy Data\")\n",
    "plt.plot(X, y_true, label=\"True Function\", color='black', linestyle='--')\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(predictions_history)))\n",
    "for (epoch, pred), color in zip(predictions_history, colors):\n",
    "    plt.plot(X, pred, color=color, label=f\"Epoch {epoch}\")\n",
    "\n",
    "plt.title(\"Prediction Curve Over Training\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b86b4-cf6a-465b-9f3b-d105027a44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vectorized time {manual_time - vectorized_time} faster than manual time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282905d-d2f0-4b32-883f-dd59fec508cc",
   "metadata": {},
   "source": [
    "Though the vectorized operation is only milliseconds faster, such gains over millions and billions of calculations will be significant!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60603da9-4773-4e45-94fc-6f266a8218e2",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Now that we've explored making predictions and checking how wrong we are, the next feature of neural networks worth exploring is gradient descent, an efficient way to take steps in the right directon to minimize the loss. \n",
    "\n",
    "Here's the promised extension of the mountain analogy: imagine you're at the summit and it's getting misty, cold and dark. You want to find the fastest way down the mountain, so look for the steepest descent available and head down it. Many more steps and decisions to find the right direction will be necessary, since the steepest descent may lead to a plateau, from which you will have to find a new way down. \n",
    "\n",
    "Gradient descent is a key building component of how neural networks learn. Now we know how to calculate how wrong we are, we need to be able to suggest incremental improvements to get better, faster. \n",
    "\n",
    "#### Intuitive algorithm:\n",
    "\n",
    "We calculate which direction provides the best option for minimizing the loss, and take a small 'step' in that direction. Repeat.\n",
    "\n",
    "The parameters, or 'weights' of the model, are updated iteratively using the gradients of the loss function. ML training is in part a process of making millions or more of these tiny adjustments until the model finds the best weights for a task.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Here's the more academic mathematical formulation:\n",
    "\n",
    "For parameters θ = [θ₀, θ₁, ..., θₙ]:\n",
    "\n",
    "**Loss Function:** $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)^2$\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**Gradient:** $\\nabla J(\\theta_j) = \\frac{1}{m} \\sum_{i=1}^{m} \\left(h_\\theta(x^{(i)}) - y^{(i)}\\right) x^{(i)}_j$\n",
    "\n",
    "**Update Rule:** $\\theta_j = \\theta_j - \\alpha \\cdot \\nabla J(\\theta_j)$\n",
    "\n",
    "Where:\n",
    "- m = number of training examples\n",
    "- α = learning rate\n",
    "- hθ(x) = hypothesis function (the model's prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd251d-7931-495b-8bfd-7b79492c7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100)\n",
    "y_true = 2 * X**2 + 3 * X + 1\n",
    "noise = np.random.normal(0, 3, size=X.shape)\n",
    "y = y_true + noise\n",
    "\n",
    "# Initialize parameters\n",
    "a, b, c = np.random.randn(3)\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "loss_history = []\n",
    "predictions_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: predict\n",
    "    y_pred = predict_quadratic(X, a, b, c)\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        predictions_history.append((epoch, y_pred.copy()))\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    # These lines calculate the gradients / derivatives that inform:\n",
    "    # which direction each parameter should move (pos or neg)\n",
    "    # how steep the slope is (how much to change)\n",
    "    N = len(X)\n",
    "    dL_da = (2/N) * np.sum((y_pred - y) * X**2)\n",
    "    dL_db = (2/N) * np.sum((y_pred - y) * X)\n",
    "    dL_dc = (2/N) * np.sum((y_pred - y) * 1)\n",
    "\n",
    "    # Update parameters\n",
    "    # Here, we move each parameter in the \n",
    "    # opposite direction of its gradient\n",
    "    a -= learning_rate * dL_da\n",
    "    b -= learning_rate * dL_db\n",
    "    c -= learning_rate * dL_dc\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3}: a={a:.3f}, b={b:.3f}, c={c:.3f}, Loss={loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2107972-f9dd-467a-af1b-be208068a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title(\"MSE Loss Over Epochs (Gradient Descent)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f9451-ad54-44f0-b041-d5a323682469",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, alpha=0.3, label=\"Noisy Data\")\n",
    "plt.plot(X, y_true, color='black', linestyle='--', label=\"True Function\")\n",
    "\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(predictions_history)))\n",
    "for (epoch, pred), color in zip(predictions_history, colors):\n",
    "    plt.plot(X, pred, color=color, label=f\"Epoch {epoch}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Curve Fitting Progress (Gradient Descent)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c0000-5b95-4f87-9d0f-c6041b5a580e",
   "metadata": {},
   "source": [
    "As we can see from this toy example, gradient descent offers several benefits over manual updates:\n",
    "* smart direction - gradients tell us which way to go efficiently\n",
    "* smooth progress - we take steps proportional to the slope steepness\n",
    "* Scale - just imagine doing manual updates for large models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a58f8-2ada-48df-87d9-d430a2ed7e85",
   "metadata": {},
   "source": [
    "### Model Class\n",
    "\n",
    "Now that the basics of a 'model' are coming together, lets add our functions to a Python class to keep the methods together, make it easy to do training and prediction, and have somewhere to add further capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b69fe0-b276-4914-8faf-b41da29fdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticModel:\n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.a = np.random.randn()\n",
    "        self.b = np.random.randn()\n",
    "        self.c = np.random.randn()\n",
    "        self.predictions_history = []\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.a * x**2 + self.b * x + self.c\n",
    "\n",
    "    def compute_gradients(self, x, y_true):\n",
    "        y_pred = self.predict(x)\n",
    "        error = y_pred - y_true\n",
    "        N = len(x)\n",
    "        dL_da = (2/N) * np.sum(error * x**2)\n",
    "        dL_db = (2/N) * np.sum(error * x)\n",
    "        dL_dc = (2/N) * np.sum(error)\n",
    "        loss = mse_loss(y_pred, y_true)\n",
    "        return loss, dL_da, dL_db, dL_dc\n",
    "\n",
    "    def update_parameters(self, grads, lr):\n",
    "        _, dL_da, dL_db, dL_dc = grads\n",
    "        self.a -= lr * dL_da\n",
    "        self.b -= lr * dL_db\n",
    "        self.c -= lr * dL_dc\n",
    "\n",
    "    def train(self, x, y, epochs=100, lr=0.01, verbose=True):\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss, dL_da, dL_db, dL_dc = self.compute_gradients(x, y)\n",
    "            self.update_parameters((loss, dL_da, dL_db, dL_dc), lr)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                y_pred = self.predict(x)\n",
    "                self.predictions_history.append((epoch, y_pred.copy()))\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:3}: a={self.a:.3f}, b={self.b:.3f}, c={self.c:.3f}, Loss={loss:.3f}\")\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4511eec-ba6a-4911-ba32-79353fa19254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100)\n",
    "y_true = 2 * X**2 + 3 * X + 1 + np.random.normal(0, 3, size=X.shape)\n",
    "noise = np.random.normal(0, 3, size=X.shape)\n",
    "\n",
    "# Train model\n",
    "model = QuadraticModel()\n",
    "loss_history = model.train(X, y, epochs=100, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682c216-e3f6-48b2-955c-81119776cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_over_time(X, y, model):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(18, 6), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (epoch, y_pred) in enumerate(model.predictions_history[:10]):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(X, y, label='Data', alpha=0.3)\n",
    "        ax.plot(X, y_pred, color='red', label='Prediction')\n",
    "        ax.set_title(f\"Epoch {epoch}\")\n",
    "        ax.legend()\n",
    "\n",
    "    fig.suptitle(\"Prediction Progression Over Epochs\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions_over_time(X, y, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eb623-83af-45f0-a9e9-868fdab0c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imageio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf1014-4881-4b08-8be1-51eba549683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "import os\n",
    "\n",
    "def create_training_gif(X, y, model, filename):\n",
    "    temp_dir = \"frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    images = []\n",
    "\n",
    "    for i, (epoch, y_pred) in enumerate(model.predictions_history):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(X, y, alpha=0.3, label='Data')\n",
    "        plt.plot(X, y_pred, color='red', label='Prediction')\n",
    "        plt.title(f\"Epoch {epoch}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save frame\n",
    "        frame_path = os.path.join(temp_dir, f\"frame_{i:03}.png\")\n",
    "        plt.savefig(frame_path)\n",
    "        plt.close()\n",
    "\n",
    "        images.append(imageio.imread(frame_path))\n",
    "\n",
    "    # Save as GIF\n",
    "    imageio.mimsave(filename, images, fps=3)\n",
    "    \n",
    "    # Clean up\n",
    "    for f in os.listdir(temp_dir):\n",
    "        os.remove(os.path.join(temp_dir, f))\n",
    "    os.rmdir(temp_dir)\n",
    "\n",
    "    print(f\"GIF saved to {filename}\")\n",
    "\n",
    "\n",
    "create_training_gif(X, y, model, filename=\"quadratic_training.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04744a4b-1eaa-4f91-a289-4c81c3755e0a",
   "metadata": {},
   "source": [
    "![Animation](quadratic_training.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf4ae9-3ee3-463e-9d9d-e94da819a022",
   "metadata": {},
   "source": [
    "### Backpropagation \n",
    "\n",
    "Now that we know how to calculate gradients and moving in the right direction (descent), backpropagation is how we calculate those gradients in a complex neural network. For those who remember high or secondary school calculus, backpropagation recalls the chain rule. Namely, the chain rule means we can take a complex function made of nested calculations (such as a neural network!) and derive how much each calculation contributes to the final output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d1f51-881f-47c8-8934-fe74be4574d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2.0\n",
    "a, b, c = 1.0, 2.0, 1.0\n",
    "y_true = 15.0\n",
    "\n",
    "# Forward pass\n",
    "z1 = x**2         # z1 = x²\n",
    "z2 = a * z1       # z2 = a * x²\n",
    "z3 = b * x        # z3 = b * x\n",
    "z4 = z2 + z3 + c  # z4 = ax² + bx + c\n",
    "y_pred = z4\n",
    "\n",
    "# Loss\n",
    "loss = (y_pred - y_true)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed643f8-1941-400c-a062-7c244efea3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "dL_dypred = 2 * (y_pred - y_true)\n",
    "\n",
    "# Gradients w.r.t each parameter\n",
    "dypred_da = z1          # ∂y_pred/∂a = x²\n",
    "dypred_db = x           # ∂y_pred/∂b = x\n",
    "dypred_dc = 1           # ∂y_pred/∂c = 1\n",
    "\n",
    "# Chain rule\n",
    "dL_da = dL_dypred * dypred_da\n",
    "dL_db = dL_dypred * dypred_db\n",
    "dL_dc = dL_dypred * dypred_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da655a84-7982-4c56-a718-aeb8efc54191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single data point since we're just trying to learn the pattern\n",
    "# This means that when x = 2, the function's ouptut should be 15\n",
    "# As a reminder, we're fitting a quadratic function: y = ax² + bx + c\n",
    "x = 2.0\n",
    "y_true = 15.0\n",
    "\n",
    "# Parameters, or \"weights\" in our simple network\n",
    "a, b, c = 1.0, 2.0, 1.0\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(\"FORWARD PASS\")\n",
    "print(\"Following the computation forward, step by step:\")\n",
    "\n",
    "# --- Forward pass ---\n",
    "# This is like signals flowing forward through a neural network\n",
    "z1 = x**2         # First operation: square the input\n",
    "print(f\"z1 = x² = {x}² = {z1}\")\n",
    "\n",
    "z2 = a * z1       # Multiply by parameter 'a' \n",
    "print(f\"z2 = a × z1 = {a} × {z1} = {z2}\")\n",
    "\n",
    "z3 = b * x        # Multiply input by parameter 'b'\n",
    "print(f\"z3 = b × x = {b} × {x} = {z3}\")\n",
    "\n",
    "z4 = z2 + z3 + c  # Add everything together (our prediction)\n",
    "print(f\"z4 = z2 + z3 + c = {z2} + {z3} + {c} = {z4}\")\n",
    "\n",
    "y_pred = z4\n",
    "print(f\"Final prediction: {y_pred}\")\n",
    "\n",
    "# Calculate how wrong we were\n",
    "loss = (y_pred - y_true)**2\n",
    "print(f\"Target was {y_true}, we predicted {y_pred}\")\n",
    "print(f\"Loss (squared error): ({y_pred} - {y_true})² = {loss:.4f}\")\n",
    "\n",
    "print(\"\\nBACKWARD PASS\")\n",
    "print(\"Now we trace the error backwards to find which parameters need adjustments:\")\n",
    "\n",
    "# --- Backward pass (chain rule) ---\n",
    "# We work backwards from the loss,working out how much \n",
    "# each parameter contributed to inaccuracies\n",
    "\n",
    "# For parameter 'a': it affects prediction through z2 = a * x²\n",
    "# Chain rule: dL/da = (dL/dy_pred) × (dy_pred/da)\n",
    "# Since y_pred = ax² + bx + c, then dy_pred/da = x²\n",
    "dL_da = dL_dypred * x**2\n",
    "print(f\"dL/da = dL/dy_pred × x² = {dL_dypred:.4f} × {x**2} = {dL_da:.4f}\")\n",
    "print(f\"Parameter 'a' is multiplied by x²={x**2}, so it has {x**2}x the impact on our mistake\")\n",
    "\n",
    "# For parameter 'b': it affects prediction through z3 = b * x  \n",
    "# dy_pred/db = x\n",
    "dL_db = dL_dypred * x\n",
    "print(f\"dL/db = dL/dy_pred × x = {dL_dypred:.4f} × {x} = {dL_db:.4f}\")\n",
    "print(f\"Parameter 'b' is multiplied by x={x}, so it has {x}x the impact\")\n",
    "\n",
    "# For parameter 'c': it's added directly to prediction\n",
    "# dy_pred/dc = 1 (adding c by 1 increases prediction by 1)\n",
    "dL_dc = dL_dypred * 1\n",
    "print(f\"dL/dc = dL/dy_pred × 1 = {dL_dypred:.4f} × 1 = {dL_dc:.4f}\")\n",
    "print(f\"Parameter 'c' is added directly, so it has 1x the impact\")\n",
    "\n",
    "print(f\"\\nRESULTS\")\n",
    "print(f\"Parameter 'a' gets blame score: {dL_da:.4f}\")\n",
    "print(f\"Parameter 'b' gets blame score: {dL_db:.4f}\") \n",
    "print(f\"Parameter 'c' gets blame score: {dL_dc:.4f}\")\n",
    "print(\"Bigger blame = bigger adjustment needed!\")\n",
    "\n",
    "print(f\"\\nGRADIENT DESCENT UPDATE\")\n",
    "print(\"Now we adjust each parameter proportional to their inaccuracy:\")\n",
    "print(f\"Old parameters: a={a:.4f}, b={b:.4f}, c={c:.4f}\")\n",
    "\n",
    "\n",
    "# --- Gradient descent update ---\n",
    "# Move each parameter in the opposite direction of its gradient\n",
    "# (negative because we want to reduce loss, not increase it)\n",
    "a -= learning_rate * dL_da\n",
    "b -= learning_rate * dL_db\n",
    "c -= learning_rate * dL_dc\n",
    "\n",
    "print(f\"a = a - learning_rate × dL/da = {a + learning_rate * dL_da:.4f} - {learning_rate} × {dL_da:.4f} = {a:.4f}\")\n",
    "print(f\"b = b - learning_rate × dL/db = {b + learning_rate * dL_db:.4f} - {learning_rate} × {dL_db:.4f} = {b:.4f}\")\n",
    "print(f\"c = c - learning_rate × dL/dc = {c + learning_rate * dL_dc:.4f} - {learning_rate} × {dL_dc:.4f} = {c:.4f}\")\n",
    "\n",
    "print(f\"\\nUpdated parameters: a={a:.4f}, b={b:.4f}, c={c:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a3475-c713-4130-929b-45eea9792d03",
   "metadata": {},
   "source": [
    "### Summarry \n",
    "\n",
    "In this notebook, we covered the basic building blocks of a neural network including:\n",
    "* Loss function\n",
    "* Gradient descent\n",
    "* Backpropagation\n",
    "\n",
    "In the next notebook, we will look at how we introduce non-linearity into our networks with *activation* functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb9671-ddb2-4f2a-9830-dd456bdcb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html module_1_a.ipynb   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
