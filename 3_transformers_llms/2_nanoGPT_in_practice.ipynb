{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cbae02-918d-40a5-9753-aca8e718aa92",
   "metadata": {},
   "source": [
    "# Understanding Transformers \n",
    "\n",
    "In this notebook, we will use Andrej Karpathy's excellent nanoGPT, to understand how transformers learn and generate output.\n",
    "\n",
    "Let's go through the code step by step to understand how each function fits into the broader transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d58e7-0a79-4d0f-9afd-b7c819585a43",
   "metadata": {},
   "source": [
    "### Layer Normalization \n",
    "\n",
    "Layer Normalization speeds up training time by computing \n",
    "the mean and variance used for normalization from all of the summed inputs to the \n",
    "neurons in a layer on a single case. This implementation optionally supports bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca9d14-9831-47e6-bad5-fef53996209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9690cfc-e27c-4cfe-94b5-49430236e7e8",
   "metadata": {},
   "source": [
    "### Causal Self-Attention\n",
    "\n",
    "The heart of the transformer, this is a multi-head, self-attention mechanism with causal masking.\n",
    "* Causal masking means tokens can only attend to previous positions (so the model does not look ahead).\n",
    "* Multi-head: splits attention into multiple heads that can focus on different aspects\n",
    "* Flash attention is an optimized attention implementation requiring PyTorch 2.0\n",
    "* Efficient batching: computes Queries, Keys and Values for all heads in one matrix operation\n",
    "\n",
    "This attention mechanism allows the model to check previous tokens to understand context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de92cc9-9fdd-45dd-96aa-28a7b61b5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head causal self-attention.\n",
    "    This is the core of the transformer - it allows the model to attend to previous tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # Use Flash Attention if available (PyTorch 2.0+)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1a4f0-ab71-4c77-9454-159e9f8de9fc",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "\n",
    "A standard feed-forward neural network with two layers providing non-linear GELU activations between attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb756acc-3d39-4202-9d74-b16c8e262d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network)\n",
    "    This provides non-linearity and processing power between attention layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd7365-4a9f-4a3f-a7d9-3f688c25f132",
   "metadata": {},
   "source": [
    "### Block\n",
    "\n",
    "Here is our Transformer building block class. Each block comprises:\n",
    "* LayerNorm -> Attention -> Residual connection\n",
    "* LayerNorm -> MLP -> Residual connection\n",
    "The layernorm and residual connections help with value stability during training.\n",
    "A residual connection simply means adding the input of a layer to its output eg\n",
    "`x = x + layer(x)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921bf89-0546-4c62-b03b-b054496c063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block: Attention + MLP with residual connections and layer normalization.\n",
    "    This is the fundamental building block that gets stacked to create the full model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))  # Residual connection around attention\n",
    "        x = x + self.mlp(self.ln_2(x))   # Residual connection around MLP\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170d4cf-a64a-4f18-8b3e-2242afbeace2",
   "metadata": {},
   "source": [
    "### GPTConfig\n",
    "\n",
    "A dataclass that centralizes all hyperparameters, making it easy to experiment with different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fdb16-19ef-48a8-bd73-272c38dfe730",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration class for GPT model hyperparameters\"\"\"\n",
    "    block_size: int = 1024      # Maximum sequence length\n",
    "    vocab_size: int = 50304     # Number of tokens in vocabulary  \n",
    "    n_layer: int = 12           # Number of transformer blocks\n",
    "    n_head: int = 12            # Number of attention heads\n",
    "    n_embd: int = 768           # Embedding dimension\n",
    "    dropout: float = 0.0        # Dropout probability\n",
    "    bias: bool = True           # Whether to use bias in linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2085e-faf5-4e88-95f8-dc17cc7d56f5",
   "metadata": {},
   "source": [
    "### The complete GPT\n",
    "\n",
    "Everything together:\n",
    "* Token and position embeddings\n",
    "* Transformer blocks\n",
    "* Output projection mapping final hidden states back to vocabulary logits\n",
    "\n",
    "#### Notable design choices\n",
    "\n",
    "* Weight tying: token embeddings and output projections share weights - this is a common optimization\n",
    "* Flexible forward pass: model can compute loss during training or just logits during inference\n",
    "\n",
    "#### Modes\n",
    "Here's what happens in training mode:\n",
    "* We process the entire sequences in parallel\n",
    "* Compute cross-entropy loss accross all positions\n",
    "* Use ground truth tokens vs model predictions\n",
    "\n",
    "And in inference mode:\n",
    "* We just compute logits for the last token\n",
    "* `generate()` enables autoregressive sampling\n",
    "\n",
    "#### So how do we actually 'generate'?\n",
    "\n",
    "Using the `generate()` method, we:\n",
    "* Take current sequence\n",
    "* Get logits for next token\n",
    "* Apply temperature scaling and optional k-filtering\n",
    "* Sample from the probability distribution\n",
    "* Append to sequence and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eec611-b8c9-4619-8a4a-293e5ad16aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    The complete GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),    # Token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),    # Position embeddings\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # Transformer blocks\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),       # Final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)  # Output projection\n",
    "        \n",
    "        # Weight tying: share weights between token embeddings and output projection\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"Return the number of parameters in the model.\"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # Training mode: calculate loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference mode: only compute logits for the last token\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if it's too long\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Apply temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # Optionally crop to top-k\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # Convert to probabilities and sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "print(\"‚úÖ Model architecture defined!\")\n",
    "\n",
    "#### This is the end of Karpathy's code at https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b48e2-1be1-4622-9f2c-0a3b9a48af41",
   "metadata": {},
   "source": [
    "Here's the end of Karpathy's code. So the architecture flow looks like this:\n",
    "\n",
    "Input tokens ‚Üí Token + Position embeddings ‚Üí Transformer blocks ‚Üí Layer norm ‚Üí Output projection ‚Üí Logits\n",
    "\n",
    "and each Transformer block is doing:\n",
    "\n",
    "`x = x + attention(layernorm(x)) then x = x + mlp(layernorm(x))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75604d03-7ee0-4647-9f1a-d36cff82f3c7",
   "metadata": {},
   "source": [
    "Let's see the model in action with some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3462a-043f-4284-9f5f-d2f17284e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    A simple character-level tokenizer.\n",
    "    Converts text to integers and back.\n",
    "    \"\"\"\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}  # string to int\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}  # int to string\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.itos[i] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c95bbb-29f0-4bc6-b2d6-319e433a641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_educational_dataset():\n",
    "    \"\"\"\n",
    "    Create a simple dataset that has clear patterns.\n",
    "    We'll use counting numbers so it's easy to see if the model learns.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    # Simple counting pattern\n",
    "    for i in range(50):\n",
    "        text += f\"Number {i} comes before {i+1}. \"\n",
    "    \n",
    "    # Add some simple sequences\n",
    "    for i in range(20):\n",
    "        text += f\"A{i}B{i}C{i}. \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d6cea-99ac-4f6c-8f3f-0bd2addcc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our dataset\n",
    "dataset_text = create_educational_dataset()\n",
    "print(f\"Dataset length: {len(dataset_text)} characters\")\n",
    "print(f\"\\nFirst 200 characters:\")\n",
    "print(repr(dataset_text[:200]))\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer(dataset_text)\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Characters in vocabulary: {repr(''.join(tokenizer.chars))}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_text = \"Number 5\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nTokenizer test:\")\n",
    "print(f\"Original: {repr(test_text)}\")\n",
    "print(f\"Encoded:  {encoded}\")\n",
    "print(f\"Decoded:  {repr(decoded)}\")\n",
    "\n",
    "# Create a small model configuration for educational purposes\n",
    "config = GPTConfig(\n",
    "    block_size=64,              # Small context window\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_layer=6,                  # 6 transformer blocks\n",
    "    n_head=6,                   # 6 attention heads\n",
    "    n_embd=192,                 # Small embedding dimension\n",
    "    dropout=0.1,                # Some dropout for regularization\n",
    "    bias=False                  # No bias for simplicity\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Block size (context length): {config.block_size}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  Number of layers: {config.n_layer}\")\n",
    "print(f\"  Number of attention heads: {config.n_head}\")\n",
    "print(f\"  Embedding dimension: {config.n_embd}\")\n",
    "print(f\"  Dropout: {config.dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6f5c3-08f6-4649-9e7d-5e5b2a3a0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = GPT(config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d07e0-be95-484a-9aa4-0f279efbe7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation(model, tokenizer, prompt, max_tokens=50, temperature=1.0):\n",
    "    \"\"\"Test text generation with the model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt\n",
    "    context = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(context, max_new_tokens=max_tokens, temperature=temperature)\n",
    "        result = tokenizer.decode(generated[0].tolist())\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Testing model before training\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Number \",\n",
    "    \"A1B1\",\n",
    "    \"comes before\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=30, temperature=1.0)\n",
    "    print(f\"Prompt: {repr(prompt)}\")\n",
    "    print(f\"Generated: {repr(result)}\")\n",
    "    print(f\"New text: {repr(result[len(prompt):])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da464291-04dd-4e56-adea-931e1660127b",
   "metadata": {},
   "source": [
    "As expected, the untrained model produces mostly gibbersish.\n",
    "\n",
    "Let's create some training data and try a short training cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c0368-cc3a-421b-8646-8d0ff3d2ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(text, tokenizer, block_size):\n",
    "    \"\"\"Convert text to training data format\"\"\"\n",
    "    # Encode the entire text\n",
    "    data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "    \n",
    "    # Split into train/validation\n",
    "    n = len(data)\n",
    "    train_data = data[:int(n*0.9)]\n",
    "    val_data = data[int(n*0.9):]\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def get_batch(data, block_size, batch_size, device):\n",
    "    \"\"\"Get a batch of training data\"\"\"\n",
    "    # Random starting positions\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input and target sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Prepare the data\n",
    "train_data, val_data = prepare_training_data(dataset_text, tokenizer, config.block_size)\n",
    "\n",
    "print(f\"Training data: {len(train_data)} tokens\")\n",
    "print(f\"Validation data: {len(val_data)} tokens\")\n",
    "\n",
    "# Test batch creation\n",
    "batch_size = 4\n",
    "x, y = get_batch(train_data, config.block_size, batch_size, device)\n",
    "\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"Input (x): {x.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "\n",
    "print(f\"\\nExample training pair:\")\n",
    "print(f\"Input:  {repr(tokenizer.decode(x[0].tolist()))}\")\n",
    "print(f\"Target: {repr(tokenizer.decode(y[0].tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ec2b0-ed0f-493d-a26a-9fb844bb9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, config, num_steps=500):\n",
    "    \"\"\"Train the model and track progress\"\"\"\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate = 3e-4\n",
    "    batch_size = 8\n",
    "    eval_interval = 50\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    print(\"Starting Training!\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        x, y = get_batch(train_data, config.block_size, batch_size, device)\n",
    "        \n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        if step % eval_interval == 0 or step == num_steps - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Validate\n",
    "                val_x, val_y = get_batch(val_data, config.block_size, batch_size, device)\n",
    "                val_logits, val_loss = model(val_x, val_y)\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "            steps.append(step)\n",
    "            \n",
    "            print(f\"Step {step:4d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, steps\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, steps = train_model(model, train_data, val_data, config, num_steps=300)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3500bb9-b2cb-4b00-8366-de78331d3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, train_losses, label='Training Loss', marker='o', markersize=4)\n",
    "plt.plot(steps, val_losses, label='Validation Loss', marker='s', markersize=4)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aa0b4-a1da-4697-b785-91f97048df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing model after training\")\n",
    "\n",
    "# Test with the same prompts as before\n",
    "test_prompts = [\n",
    "    \"Number \",\n",
    "    \"A1B1\",\n",
    "    \"comes before\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=30, temperature=0.8)\n",
    "    print(f\"Prompt: {repr(prompt)}\")\n",
    "    print(f\"Generated: {repr(result)}\")\n",
    "    print(f\"New text: {repr(result[len(prompt):])}\")\n",
    "    print()\n",
    "\n",
    "print(\"Compare this to the pre-training gibberish!\")\n",
    "print(\"\\nLet's also try some longer generations:\")\n",
    "\n",
    "# Longer generations\n",
    "longer_prompts = [\n",
    "    \"Number 5 comes\",\n",
    "    \"A10B10\"\n",
    "]\n",
    "\n",
    "for prompt in longer_prompts:\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=60, temperature=0.7)\n",
    "    print(f\"Prompt: {repr(prompt)}\")\n",
    "    print(f\"Generated: {repr(result)}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481caab5-f67d-4b05-bca7-0482d1f7174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model analysis\")\n",
    "\n",
    "# Test different temperatures\n",
    "print(\"Testing different temperatures:\")\n",
    "prompt = \"Number 3\"\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5]\n",
    "for temp in temperatures:\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=25, temperature=temp)\n",
    "    print(f\"Temperature {temp}: {repr(result[len(prompt):])}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Test model's understanding of patterns\n",
    "print(\"Testing pattern completion:\")\n",
    "test_cases = [\n",
    "    \"Number 15 comes before\",\n",
    "    \"A5B5\",\n",
    "    \"before 7\"\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    result = test_generation(model, tokenizer, case, max_tokens=20, temperature=0.5)\n",
    "    print(f\"Input: {repr(case)}\")\n",
    "    print(f\"Completion: {repr(result[len(case):])}\")\n",
    "    print()\n",
    "\n",
    "# Show model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (fp32)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92f544-a4ad-4dc9-a30e-8f5aecb1c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What did the model learn?\")\n",
    "\n",
    "# Test if it learned the counting pattern\n",
    "print(\"1. Counting pattern recognition:\")\n",
    "for i in [7, 12, 25]:\n",
    "    prompt = f\"Number {i} comes before\"\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=10, temperature=0.1)\n",
    "    predicted = result[len(prompt):].strip()\n",
    "    print(f\"  {i} comes before... Model says: {repr(predicted)}\")\n",
    "\n",
    "print(\"\\n2. Letter-number pattern recognition:\")\n",
    "for i in [3, 7, 15]:\n",
    "    prompt = f\"A{i}B{i}\"\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=10, temperature=0.1)\n",
    "    predicted = result[len(prompt):].strip()\n",
    "    print(f\"  A{i}B{i}... Model says: {repr(predicted)}\")\n",
    "\n",
    "print(\"\\n3. Novel pattern generation:\")\n",
    "# Try something not in training data\n",
    "novel_prompts = [\n",
    "    \"Number 100\",\n",
    "    \"A50B50\",\n",
    "]\n",
    "\n",
    "for prompt in novel_prompts:\n",
    "    result = test_generation(model, tokenizer, prompt, max_tokens=20, temperature=0.5)\n",
    "    print(f\"  {repr(prompt)} ‚Üí {repr(result[len(prompt):])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263d71d-60f8-4d8b-9a84-b905c1467d95",
   "metadata": {},
   "source": [
    "Our small example shows promising improvement! Here we can try a larger dataset, using [tinyshakespeare](https://huggingface.co/datasets/karpathy/tiny_shakespeare) (more thanks to Andrej!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1eeb5-de61-47a5-9489-ecaa844ae413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Extension Examples for NanoGPT Educational Demo\n",
    "\n",
    "# Shakespeare / poetry dataset\n",
    "def load_shakespeare_data():\n",
    "    \"\"\"Load a small Shakespeare dataset\"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    # Download tiny Shakespeare dataset (about 1MB)\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text = response.read().decode('utf-8')\n",
    "        \n",
    "        # Take just a subset for faster training\n",
    "        text = text[:50000]  # First 50k characters\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        # Fallback to hardcoded Shakespeare sample\n",
    "        return \"\"\"\n",
    "        ROMEO: But soft, what light through yonder window breaks?\n",
    "        It is the east, and Juliet is the sun.\n",
    "        Arise, fair sun, and kill the envious moon,\n",
    "        Who is already sick and pale with grief,\n",
    "        That thou, her maid, art far more fair than she.\n",
    "        \"\"\" * 100  # Repeat to get more training data\n",
    "\n",
    "# Simple stories dataset\n",
    "def create_simple_stories():\n",
    "    \"\"\"Create a dataset of very simple stories\"\"\"\n",
    "    \n",
    "    stories = [\n",
    "        \"Once upon a time, there was a little cat. The cat was very happy. The cat liked to play with yarn. The end.\",\n",
    "        \"There was a big dog. The dog was brown. The dog loved to run in the park. The dog made many friends.\",\n",
    "        \"A small bird lived in a tree. The bird could sing beautiful songs. All the animals loved to listen.\",\n",
    "        \"The sun was shining bright. The flowers were blooming. It was a perfect day for a picnic.\",\n",
    "        \"A young boy found a magic stone. The stone could grant wishes. He wished for happiness for everyone.\",\n",
    "    ]\n",
    "    \n",
    "    # Create more variation\n",
    "    names = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Emma\"]\n",
    "    animals = [\"cat\", \"dog\", \"bird\", \"rabbit\", \"fish\"]\n",
    "    actions = [\"played\", \"ran\", \"sang\", \"danced\", \"jumped\"]\n",
    "    \n",
    "    extended_stories = stories.copy()\n",
    "    \n",
    "    for name in names:\n",
    "        for animal in animals:\n",
    "            for action in actions:\n",
    "                story = f\"{name} had a pet {animal}. The {animal} {action} every day. {name} was very happy. The end.\"\n",
    "                extended_stories.append(story)\n",
    "    \n",
    "    return \" \".join(extended_stories)\n",
    "\n",
    "# Code dataset\n",
    "def create_simple_code_dataset():\n",
    "    \"\"\"Create a simple Python code dataset\"\"\"\n",
    "    \n",
    "    code_examples = []\n",
    "    \n",
    "    # Simple functions\n",
    "    for i in range(1, 20):\n",
    "        code_examples.append(f\"\"\"\n",
    "            def add_numbers(a, b):\n",
    "                return a + b\n",
    "            \n",
    "            result = add_numbers({i}, {i+1})\n",
    "            print(result)\n",
    "            \"\"\")\n",
    "    \n",
    "    # Simple loops\n",
    "    for i in range(1, 10):\n",
    "        code_examples.append(f\"\"\"\n",
    "            for i in range({i}):\n",
    "                print(f\"Number: {{i}}\")\n",
    "            \"\"\")\n",
    "    \n",
    "    # Simple conditionals\n",
    "    for i in range(1, 15):\n",
    "        code_examples.append(f\"\"\"\n",
    "            x = {i}\n",
    "            if x > 5:\n",
    "                print(\"Big number\")\n",
    "            else:\n",
    "                print(\"Small number\")\n",
    "            \"\"\")\n",
    "    \n",
    "    return \"\\n\".join(code_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f27b4-5826-420c-97a3-c6532622d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified tokenizer for real text\n",
    "class ImprovedTokenizer:\n",
    "    \"\"\"\n",
    "    Slightly improved tokenizer that handles real text better\n",
    "    \"\"\"\n",
    "    def __init__(self, text):\n",
    "        # Get unique characters and add special tokens\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        \n",
    "        # Add special tokens if needed\n",
    "        special_tokens = ['<UNK>']\n",
    "        for token in special_tokens:\n",
    "            if token not in self.chars:\n",
    "                self.chars.append(token)\n",
    "        \n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        # Track unknown character for robustness\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.unk_id = self.stoi.get(self.unk_token, 0)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Handle unknown characters gracefully\n",
    "        return [self.stoi.get(ch, self.unk_id) for ch in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.itos.get(i, self.unk_token) for i in tokens])\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        import re\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dcbdc-7916-42eb-a536-96ab5e2ab463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated model config for real text\n",
    "def get_text_model_config(vocab_size):\n",
    "    \"\"\"Get a good config for real text datasets\"\"\"\n",
    "    return GPTConfig(\n",
    "        block_size=128,         # Longer context for real text\n",
    "        vocab_size=vocab_size,\n",
    "        n_layer=8,             # A bit deeper\n",
    "        n_head=8,              # More heads\n",
    "        n_embd=256,            # Larger embedding\n",
    "        dropout=0.1,\n",
    "        bias=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced589b-3873-433e-9046-95eab39ef9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better training for real text\n",
    "def train_on_real_text(dataset_name=\"shakespeare\"):\n",
    "    \"\"\"Complete example of training on real text\"\"\"\n",
    "    \n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "    \n",
    "    if dataset_name == \"shakespeare\":\n",
    "        text = load_shakespeare_data()\n",
    "    elif dataset_name == \"stories\":\n",
    "        text = create_simple_stories()\n",
    "    elif dataset_name == \"code\":\n",
    "        text = create_simple_code_dataset()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    print(f\"Dataset size: {len(text)} characters\")\n",
    "    print(f\"Sample: {repr(text[:200])}\")\n",
    "    \n",
    "    # Create improved tokenizer\n",
    "    tokenizer = ImprovedTokenizer(text)\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Create model\n",
    "    config = get_text_model_config(tokenizer.vocab_size)\n",
    "    model = GPT(config)\n",
    "    \n",
    "    # Prepare data\n",
    "    data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "    train_data = data[:int(len(data) * 0.9)]\n",
    "    val_data = data[int(len(data) * 0.9):]\n",
    "    \n",
    "    print(f\"Training tokens: {len(train_data):,}\")\n",
    "    print(f\"Validation tokens: {len(val_data):,}\")\n",
    "    \n",
    "    # Train with more steps for real text\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training loop (simplified)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for step in range(1000):  # More steps for real text\n",
    "        # Get batch\n",
    "        batch_size = 16\n",
    "        ix = torch.randint(len(train_data) - config.block_size, (batch_size,))\n",
    "        x = torch.stack([train_data[i:i+config.block_size] for i in ix]).to(device)\n",
    "        y = torch.stack([train_data[i+1:i+config.block_size+1] for i in ix]).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d293ebd-5274-41f0-bfdd-05e14d184f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on real text\n",
    "def test_real_text_generation(model, tokenizer, dataset_type=\"shakespeare\"):\n",
    "    \"\"\"Test generation on real text datasets\"\"\"\n",
    "    \n",
    "    if dataset_type == \"shakespeare\":\n",
    "        prompts = [\n",
    "            \"ROMEO:\",\n",
    "            \"To be or not to be\",\n",
    "            \"Fair\"\n",
    "        ]\n",
    "    elif dataset_type == \"stories\":\n",
    "        prompts = [\n",
    "            \"Once upon a time\",\n",
    "            \"The cat\",\n",
    "            \"Alice\"\n",
    "        ]\n",
    "    elif dataset_type == \"code\":\n",
    "        prompts = [\n",
    "            \"def \",\n",
    "            \"for i in range\",\n",
    "            \"if x >\"\n",
    "        ]\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    print(f\"Testing {dataset_type} generation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        context = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(context, max_new_tokens=50, temperature=0.8)\n",
    "            result = tokenizer.decode(generated[0].tolist())\n",
    "        \n",
    "        print(f\"Prompt: {repr(prompt)}\")\n",
    "        print(f\"Generated: {repr(result)}\")\n",
    "        print(f\"New text: {repr(result[len(prompt):])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a03282-08ae-4cf2-8fd3-e0fb484eb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train on Shakespeare\n",
    "    model, tokenizer = train_on_real_text(\"shakespeare\")\n",
    "    test_real_text_generation(model, tokenizer, \"shakespeare\")\n",
    "\n",
    "    # model, tokenizer = train_on_real_text(\"code\")\n",
    "    # test_real_text_generation(model, tokenizer, \"code\")\n",
    "\n",
    "    # model, tokenizer = train_on_real_text(\"stories\")\n",
    "    # test_real_text_generation(model, tokenizer, \"stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7c592-3cdb-43e6-8b8a-92aa37244cc0",
   "metadata": {},
   "source": [
    "### Wrap up\n",
    "\n",
    "Our transformer architecture features:\n",
    "* Self-attention, allowing modeling long-range dependencies\n",
    "* Layer normalization and residual connections enabling deep networks\n",
    "* Causal masking ensuring autoregressive generation\n",
    "\n",
    "The training process:\n",
    "* Language modeling = predicting next token\n",
    "* Loss decreases as model learns patterns\n",
    "* Small models can learn simple patterns effectively\n",
    "\n",
    "Generation:\n",
    "* Autoregressive: generate one token at a time\n",
    "* Temperature controls randomness/creativity\n",
    "* Model can generalize beyond training examples\n",
    "\n",
    "Scale:\n",
    "* This is a tiny model of ~2,673,792 parameters, worth comparing to:\n",
    "** GPT-2: ~1.5B parameters\n",
    "** GPT-3: ~175B parameters\n",
    "* Same architecture, just bigger!\n",
    "\n",
    "Further explorations:\n",
    "* Try different datasets (have a look online for small text datasets)\n",
    "* Experiment with model size (layers, heads, embedding dim)\n",
    "* Add more sophisticated tokenization (BPE)\n",
    "* Implement attention visualization\n",
    "* Try fine-tuning on specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e53a45-17d2-4aea-a2fc-6df41fbffcc1",
   "metadata": {},
   "source": [
    "# Visualizing attention\n",
    "\n",
    "Let's make a simple visualization to see what each token is paying attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9cde7-7e6c-47c4-b7ff-750748a94ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    Shows the key insight: how transformers look at previous tokens.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Encode text and get tokens for display\n",
    "    tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    token_names = [tokenizer.itos[i] for i in tokenizer.encode(text)]\n",
    "    \n",
    "    # Hook to capture attention weights from first layer, first head\n",
    "    attention_weights = None\n",
    "    \n",
    "    def get_attention(module, input, output):\n",
    "        nonlocal attention_weights\n",
    "        x = input[0]\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate attention (simplified from the actual module)\n",
    "        qkv = module.c_attn(x)\n",
    "        q, k, v = qkv.split(module.n_embd, dim=2)\n",
    "        q = q.view(B, T, module.n_head, C // module.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, module.n_head, C // module.n_head).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) / np.sqrt(k.size(-1))\n",
    "        if not module.flash:\n",
    "            att = att.masked_fill(module.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(att, dim=-1)[0, 0].detach().cpu().numpy()\n",
    "    \n",
    "    # Capture attention from first layer\n",
    "    hook = model.transformer.h[0].attn.register_forward_hook(get_attention)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model(tokens)\n",
    "    hook.remove()\n",
    "    \n",
    "    # Simple visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_weights, cmap='Blues')\n",
    "    plt.xticks(range(len(token_names)), [f\"'{t}'\" for t in token_names], rotation=45)\n",
    "    plt.yticks(range(len(token_names)), [f\"'{t}'\" for t in token_names])\n",
    "    plt.xlabel('What we look AT')\n",
    "    plt.ylabel('Who is looking')\n",
    "    plt.title(f'Attention Pattern: \"{text}\"')\n",
    "    plt.colorbar(label='Attention')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Simple explanation\n",
    "    print(f\"What's happening in '{text}':\")\n",
    "    for i, token in enumerate(token_names):\n",
    "        # Find what this token pays most attention to\n",
    "        best_idx = np.argmax(attention_weights[i])\n",
    "        best_weight = attention_weights[i, best_idx]\n",
    "        best_token = token_names[best_idx]\n",
    "        \n",
    "        if best_weight > 0.2:  # Only show significant attention\n",
    "            print(f\"  '{token}' ‚Üí focuses most on '{best_token}' ({best_weight:.2f})\")\n",
    "\n",
    "# Test it with a simple example\n",
    "print(\"üîç ATTENTION VISUALIZATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show attention for one clear example\n",
    "show_attention(model, tokenizer, \"The rain in Spain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
