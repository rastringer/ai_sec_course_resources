{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0874c93-b5da-427c-841c-7053df2ce7c1",
   "metadata": {},
   "source": [
    "### Defences for differential privacy and federated learning\n",
    "\n",
    "This notebook implements *true differentially private stochastic gradient descent* (DP-SGD) in a federated setting and evaluates privacy via membership inference AUC and utility via test accuracy.\n",
    "\n",
    "True DP-SGD was formalized by [Abadi et al.](https://arxiv.org/pdf/2102.03013) as a way to train neural nets with DP guarantees by combining three steps on each mini-batch of data:\n",
    "* Pre-sample gradient computation\n",
    "* Clipping gradients so L2 norm is less than the clip norm. This ensures no single sample can dominate the update\n",
    "* Noise addition: adding a Gaussian noise to the sum of clipped gradients, for which we will see the mathematical notation below: σ * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba75d61-ff8d-4b70-b42c-4cddbedf8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d324212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP-SGD utilities \n",
    "\n",
    "def _per_sample_grads(model, loss_per_sample):\n",
    "    \"\"\"Compute per-sample gradients by looping each example (fine for small N).\"\"\"\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    B = loss_per_sample.shape[0]\n",
    "    grad_samples = [torch.zeros((B, *p.shape), device=loss_per_sample.device, dtype=p.dtype)\n",
    "                    for p in params]\n",
    "    for i in range(B):\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss_per_sample[i].backward(retain_graph=True)\n",
    "        for j, p in enumerate(params):\n",
    "            grad_samples[j][i] = p.grad.detach().clone()\n",
    "    return grad_samples\n",
    "\n",
    "def _clip_and_aggregate(grad_samples, C, sigma):\n",
    "    \"\"\"Clip per-sample gradients by L2 norm C and add Gaussian noise (sigma*C).\"\"\"\n",
    "    B = grad_samples[0].shape[0]\n",
    "    with torch.no_grad():\n",
    "        norms_sq = None\n",
    "        for g in grad_samples:\n",
    "            g_flat = g.view(B, -1)\n",
    "            norms_sq = (g_flat**2).sum(dim=1) if norms_sq is None else norms_sq + (g_flat**2).sum(dim=1)\n",
    "        norms = norms_sq.sqrt() + 1e-12\n",
    "        factors = (C / norms).clamp(max=1.0)  # [B]\n",
    "    agg = []\n",
    "    for g in grad_samples:\n",
    "        view_shape = (B,) + (1,) * (g.dim() - 1)\n",
    "        clipped = g * factors.view(view_shape)\n",
    "        summed = clipped.sum(dim=0)\n",
    "        noise = torch.normal(0.0, sigma * C, size=summed.shape, device=summed.device, dtype=summed.dtype)\n",
    "        agg.append((summed + noise) / B)  # average\n",
    "    return agg\n",
    "\n",
    "def dp_sgd_step(model, optimizer, X, y, C=1.0, sigma=1.0):\n",
    "    logits = model(X)\n",
    "    loss_vec = F.cross_entropy(logits, y, reduction='none')\n",
    "    grad_samples = _per_sample_grads(model, loss_vec)\n",
    "    agg_grads = _clip_and_aggregate(grad_samples, C=C, sigma=sigma)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for p, g in zip([p for p in model.parameters() if p.requires_grad], agg_grads):\n",
    "        p.grad = g\n",
    "    optimizer.step()\n",
    "    return loss_vec.mean().item()\n",
    "\n",
    "def approximate_epsilon(sampling_rate, steps, sigma, delta=1e-5):\n",
    "    \"\"\"Approximate epsilon for DP-SGD (Abadi-style bound; good for demo).\"\"\"\n",
    "    import math\n",
    "    q = sampling_rate\n",
    "    c2 = 2 * math.log(1.25 / max(delta, 1e-12))\n",
    "    eps = q * math.sqrt(steps * c2) / sigma + steps * (q**2) / (sigma**2)\n",
    "    return float(eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synethetic data creation \n",
    "\n",
    "def create_medical_dataset(n_patients=300):\n",
    "    np.random.seed(42)\n",
    "    ages = np.random.normal(55, 15, n_patients).clip(20, 90)\n",
    "    cholesterol = 150 + 2 * ages + np.random.normal(0, 30, n_patients)\n",
    "    cholesterol = cholesterol.clip(120, 350)\n",
    "    systolic_bp = 90 + 0.8 * ages + 0.1 * cholesterol + np.random.normal(0, 15, n_patients)\n",
    "    systolic_bp = systolic_bp.clip(90, 200)\n",
    "    heart_rate = 85 - 0.2 * ages + np.random.normal(0, 10, n_patients)\n",
    "    heart_rate = heart_rate.clip(50, 120)\n",
    "    exercise_hours = np.maximum(0, 8 - 0.05 * ages + np.random.normal(0, 3, n_patients))\n",
    "    bmi = 22 + 0.1 * ages - 0.3 * exercise_hours + np.random.normal(0, 4, n_patients)\n",
    "    bmi = bmi.clip(16, 45)\n",
    "    X = np.column_stack([ages, cholesterol, systolic_bp, heart_rate, exercise_hours, bmi])\n",
    "    risk_score = (\n",
    "        0.03*ages + 0.01*cholesterol + 0.02*systolic_bp - 0.05*exercise_hours + 0.1*bmi\n",
    "        + np.random.normal(0, 2, n_patients)\n",
    "    )\n",
    "    y = (risk_score > np.percentile(risk_score, 70)).astype(int)\n",
    "    feature_names = ['Age', 'Cholesterol', 'Systolic_BP', 'Heart_Rate', 'Exercise_Hours', 'BMI']\n",
    "    return X, y, feature_names\n",
    "\n",
    "def create_hospital_data():\n",
    "    X, y, feature_names = create_medical_dataset()\n",
    "    hospital_a_indices = np.where((X[:, 0] < 50) | (X[:, 4] > 5))[0][:80]\n",
    "    hospital_b_indices = np.where((X[:, 0] >= 40) & (X[:, 0] <= 65))[0][:90]\n",
    "    hospital_c_indices = np.where((X[:, 0] > 55) | (X[:, 2] > 140))[0][:80]\n",
    "    used_indices = set()\n",
    "    hospitals_data = []\n",
    "    hospital_names = [\"Edinburgh Hospital\", \"Clydebank Infirmary\", \"Orkney Hospital\"]\n",
    "    for i, indices in enumerate([hospital_a_indices, hospital_b_indices, hospital_c_indices]):\n",
    "        available = [idx for idx in indices if idx not in used_indices][:70 + i*10]\n",
    "        used_indices.update(available)\n",
    "        hospitals_data.append((X[available], y[available], hospital_names[i]))\n",
    "    return hospitals_data, feature_names\n",
    "\n",
    "# Build datasets and scaler\n",
    "hospitals_data, feature_names = create_hospital_data()\n",
    "scaler = StandardScaler()\n",
    "all_data = np.vstack([X for (X, y, name) in hospitals_data])\n",
    "scaler.fit(all_data)\n",
    "\n",
    "print(\"Hospital splits:\")\n",
    "for Xh, yh, nm in hospitals_data:\n",
    "    print(f\"  {nm}: {len(Xh)} patients, disease rate {yh.mean()*100:.1f}%\")\n",
    "\n",
    "# Build a holdout test set EARLY (used by multiple sections)\n",
    "def build_holdout(all_data, scaler):\n",
    "    excluded = set().union(\n",
    "        set(np.where((all_data[:, 0] < 50) | (all_data[:, 4] > 5))[0][:80]),\n",
    "        set(np.where((all_data[:, 0] >= 40) & (all_data[:, 0] <= 65))[0][:90]),\n",
    "        set(np.where((all_data[:, 0] > 55) | (all_data[:, 2] > 140))[0][:80]),\n",
    "    )\n",
    "    test_indices = list(set(range(len(all_data))) - excluded)[:50]\n",
    "    X = scaler.transform(all_data[test_indices])\n",
    "    risk = (\n",
    "        0.03 * all_data[test_indices, 0] +\n",
    "        0.01 * all_data[test_indices, 1] +\n",
    "        0.02 * all_data[test_indices, 2] -\n",
    "        0.05 * all_data[test_indices, 4] +\n",
    "        0.10 * all_data[test_indices, 5]\n",
    "    )\n",
    "    y = (risk > np.percentile(risk, 70)).astype(int)\n",
    "    return X, y\n",
    "\n",
    "test_X, test_y = build_holdout(all_data, scaler)\n",
    "print(f\"Holdout built: X={test_X.shape}, positives={test_y.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "class HeartDiseasePredictor(nn.Module):\n",
    "    def __init__(self, n_features=6, hidden=12):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.fc1 = nn.Linear(n_features, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, 2)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a67381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP-SGD Client and FL Server \n",
    "class PrivateHospitalClient:\n",
    "    def __init__(self, patient_data_x, patient_data_y, hospital_name,\n",
    "                 clip_norm=1.0, noise_multiplier=1.2, delta=1e-5,\n",
    "                 batch_size=16, local_epochs=6, lr=0.02):\n",
    "        self.hospital_name = hospital_name\n",
    "        self.X = torch.FloatTensor(patient_data_x)\n",
    "        self.y = torch.LongTensor(patient_data_y)\n",
    "        self.model = None\n",
    "        self.C = clip_norm\n",
    "        self.sigma = noise_multiplier\n",
    "        self.delta = delta\n",
    "        self.batch_size = batch_size\n",
    "        self.local_epochs = local_epochs\n",
    "        self.lr = lr\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def set_model(self, global_model):\n",
    "        hidden = getattr(global_model, 'hidden', global_model.fc1.out_features)\n",
    "        self.model = HeartDiseasePredictor(n_features=self.X.shape[1], hidden=hidden)\n",
    "        self.model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "\n",
    "    def private_train_on_patients(self):\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=1e-4)\n",
    "        initial = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
    "        N = len(self.X)\n",
    "        idx = torch.arange(N)\n",
    "        losses = []\n",
    "        for _ in range(self.local_epochs):\n",
    "            perm = idx[torch.randperm(N)]\n",
    "            for s in range(0, N, self.batch_size):\n",
    "                b = perm[s:s+self.batch_size]\n",
    "                loss_val = dp_sgd_step(self.model, optimizer, self.X[b], self.y[b], C=self.C, sigma=self.sigma)\n",
    "                self.steps_done += 1\n",
    "                losses.append(loss_val)\n",
    "        q = min(1.0, self.batch_size / N)\n",
    "        eps = approximate_epsilon(q, self.steps_done, self.sigma, delta=self.delta)\n",
    "        new_state = self.model.state_dict()\n",
    "        delta_state = {k: (new_state[k] - initial[k]) for k in new_state.keys()}\n",
    "        log = {\n",
    "            \"mean_train_loss\": float(np.mean(losses)),\n",
    "            \"epsilon_estimate\": float(eps),\n",
    "            \"steps\": int(self.steps_done),\n",
    "            \"sampling_rate\": float(q),\n",
    "            \"sigma\": float(self.sigma),\n",
    "            \"clip_norm\": float(self.C)\n",
    "        }\n",
    "        return delta_state, log\n",
    "\n",
    "class PrivateFLServer:\n",
    "    def __init__(self, model_template):\n",
    "        self.global_model = model_template\n",
    "        self.private_hospitals = []\n",
    "        self.round_logs = []\n",
    "\n",
    "    def add_hospital(self, hospital):\n",
    "        self.private_hospitals.append(hospital)\n",
    "        hospital.set_model(self.global_model)\n",
    "\n",
    "    def _apply_delta(self, delta_state):\n",
    "        with torch.no_grad():\n",
    "            for name, p in self.global_model.named_parameters():\n",
    "                p.add_(delta_state[name])\n",
    "\n",
    "    def private_collaboration_round(self):\n",
    "        deltas, logs = [], []\n",
    "        for client in self.private_hospitals:\n",
    "            d, lg = client.private_train_on_patients()\n",
    "            deltas.append(d)\n",
    "            logs.append((client.hospital_name, lg))\n",
    "        avg_delta = {}\n",
    "        for name, _ in self.global_model.named_parameters():\n",
    "            avg_delta[name] = torch.stack([d[name] for d in deltas]).mean(dim=0)\n",
    "        self._apply_delta(avg_delta)\n",
    "        for c in self.private_hospitals:\n",
    "            c.set_model(self.global_model)\n",
    "        self.round_logs.append(logs)\n",
    "        return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different presets\n",
    "# Preset 1: Balanced privacy (recommended)\n",
    "NOISE_MULT = 2.5\n",
    "BATCH_FRAC = 0.15   # target q ~ 0.15\n",
    "LOCAL_EPOCHS = 4\n",
    "ROUNDS = 2\n",
    "\n",
    "# # Preset 2: Strong privacy\n",
    "# NOISE_MULT = 3.0\n",
    "# BATCH_FRAC = 0.10\n",
    "# LOCAL_EPOCHS = 3\n",
    "# ROUNDS = 2\n",
    "\n",
    "# # Preset 3: High-utility (ε ~4–6)\n",
    "# NOISE_MULT = 2.0\n",
    "# BATCH_FRAC = 0.20\n",
    "# LOCAL_EPOCHS = 5\n",
    "# ROUNDS = 3\n",
    "\n",
    "# (Re)build DP clients with dynamic batch size to keep q small \n",
    "private_hospitals = []\n",
    "for Xh, yh, name in hospitals_data:\n",
    "    Xstd = scaler.transform(Xh)\n",
    "    N = len(Xstd)\n",
    "    # dynamic batch so q ~= BATCH_FRAC (and never > 0.25 for small N)\n",
    "    bs = int(np.ceil(BATCH_FRAC * max(1, N)))\n",
    "    bs = max(4, min(bs, 32))          # clamp to a sensible range\n",
    "    bs = min(bs, max(4, int(0.25 * N)))  # ensure q <= 0.25 even for tiny N\n",
    "\n",
    "    client = PrivateHospitalClient(\n",
    "        Xstd, yh, name,\n",
    "        clip_norm=1.0,\n",
    "        noise_multiplier=NOISE_MULT,\n",
    "        delta=1e-5,\n",
    "        batch_size=bs,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        lr=0.02\n",
    "    )\n",
    "    private_hospitals.append(client)\n",
    "    q_est = min(1.0, bs / max(1, N))\n",
    "    print(f\"{name}: N={N}, batch={bs} (q≈{q_est:.2f}), sigma={NOISE_MULT}, epochs={LOCAL_EPOCHS}\")\n",
    "\n",
    "# Fresh global model (slightly wider to help utility under DP noise)\n",
    "private_model = HeartDiseasePredictor(n_features=len(feature_names), hidden=16)\n",
    "private_fl_server = PrivateFLServer(private_model)\n",
    "for h in private_hospitals:\n",
    "    private_fl_server.add_hospital(h)\n",
    "\n",
    "print(f\"\\\\nRunning {ROUNDS} private FL rounds...\")\n",
    "for r in range(ROUNDS):\n",
    "    round_log = private_fl_server.private_collaboration_round()\n",
    "    print(f\"Round {r+1}:\")\n",
    "    for nm, lg in round_log:\n",
    "        print(f\"  {nm}: loss={lg['mean_train_loss']:.3f}, ε≈{lg['epsilon_estimate']:.2f} \"\n",
    "              f\"(steps={lg['steps']}, q={lg['sampling_rate']:.2f}, σ={lg['sigma']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membership inference (privacy) and accuracy (utility) \n",
    "def membership_auc(model, member_X, member_y, nonmember_X, nonmember_y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        m_loss = F.cross_entropy(model(torch.FloatTensor(member_X)), torch.LongTensor(member_y), reduction='none').numpy()\n",
    "        nm_loss = F.cross_entropy(model(torch.FloatTensor(nonmember_X)), torch.LongTensor(nonmember_y), reduction='none').numpy()\n",
    "    y_true = np.concatenate([np.ones_like(m_loss), np.zeros_like(nm_loss)])\n",
    "    y_score = np.concatenate([-m_loss, -nm_loss])\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# DP model AUCs\n",
    "dp_auc_by_hosp = []\n",
    "print(\"\\nMembership inference AUC for DP-FL model (lower is better, 0.5 ≈ random):\")\n",
    "for (Xh, yh, name) in hospitals_data:\n",
    "    Xh_std = scaler.transform(Xh)\n",
    "    auc = membership_auc(private_fl_server.global_model, Xh_std, yh, test_X, test_y)\n",
    "    dp_auc_by_hosp.append((name, auc))\n",
    "    print(f\"  {name}: AUC = {auc:.3f}\")\n",
    "\n",
    "# Non-DP pooled baseline for contrast\n",
    "X_pool = scaler.transform(np.vstack([X for (X, y, _) in hospitals_data]))\n",
    "y_pool = np.concatenate([y for (X, y, _) in hospitals_data])\n",
    "baseline_non_dp = HeartDiseasePredictor(n_features=len(feature_names), hidden=12)\n",
    "opt = optim.Adam(baseline_non_dp.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "for epoch in range(15):\n",
    "    xb = torch.FloatTensor(X_pool)\n",
    "    yb = torch.LongTensor(y_pool)\n",
    "    opt.zero_grad()\n",
    "    loss = F.cross_entropy(baseline_non_dp(xb), yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(\"\\nMembership inference AUC for non-DP baseline (expect higher AUC -> worse privacy):\")\n",
    "ndp_auc_by_hosp = []\n",
    "for (Xh, yh, name) in hospitals_data:\n",
    "    Xh_std = scaler.transform(Xh)\n",
    "    auc = membership_auc(baseline_non_dp, Xh_std, yh, test_X, test_y)\n",
    "    ndp_auc_by_hosp.append((name, auc))\n",
    "    print(f\"  {name}: AUC = {auc:.3f}\")\n",
    "\n",
    "# Utility (accuracy on holdout)\n",
    "def evaluate_accuracy(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.FloatTensor(X))\n",
    "        pred = logits.argmax(dim=1).numpy()\n",
    "    return (pred == y).mean()\n",
    "\n",
    "baseline_accuracy = evaluate_accuracy(baseline_non_dp, test_X, test_y)\n",
    "private_accuracy = evaluate_accuracy(private_fl_server.global_model, test_X, test_y)\n",
    "print(f\"\\nUtility (accuracy on holdout): Non-DP={baseline_accuracy:.3f} | DP-FL={private_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Privacy & Utility \n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "# AUC bars\n",
    "ax = axes[0]\n",
    "names = [n for n,_ in dp_auc_by_hosp]\n",
    "dp_aucs = [a for _,a in dp_auc_by_hosp]\n",
    "ndp_aucs = [a for _,a in ndp_auc_by_hosp]\n",
    "x = np.arange(len(names)); w = 0.35\n",
    "ax.bar(x - w/2, ndp_aucs, w, label='Non-DP', alpha=0.85)\n",
    "ax.bar(x + w/2, dp_aucs, w, label='DP-FL', alpha=0.85)\n",
    "ax.set_xticks(x); ax.set_xticklabels([n.replace(' ', '\\n') for n in names])\n",
    "ax.set_ylabel('Membership AUC (lower=better privacy)')\n",
    "ax.set_title('Privacy Leakage (Membership Inference)')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy bars\n",
    "ax2 = axes[1]\n",
    "ax2.bar(['Non-DP baseline','DP-FL'], [baseline_accuracy, private_accuracy], alpha=0.85)\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Utility (Test Accuracy)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3efb0-eb87-4300-b3ca-1b4d167ff742",
   "metadata": {},
   "source": [
    "### Privacy budget overview\n",
    "\n",
    "Here, ε denotes the cumulative privacy loss, depending on:\n",
    "* σ (noise multiplier)\n",
    "* sampling rate q (batch_size / N)\n",
    "* number of steps/rounds\n",
    "* δ (failure probability)\n",
    "\n",
    "Smaller ε means a stronger privacy guarantee.\n",
    "\n",
    "Typical guidelines:\n",
    "\n",
    "* ε ≤ 1–3 → strong privacy\n",
    "* ε ~ 5–10 → moderate\n",
    "* ε > 10 → weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba58045",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_budget = 5.0  # illustrative target\n",
    "last_logs = dict(private_fl_server.round_logs[-1])  # {name: log}\n",
    "for name in [h.hospital_name for h in private_hospitals]:\n",
    "    lg = last_logs[name]\n",
    "    eps = lg[\"epsilon_estimate\"]\n",
    "    remaining = max(0.0, target_budget - eps)\n",
    "    print(f\" {name}: ε≈{eps:.2f} after {lg['steps']} steps (σ={lg['sigma']}, q={lg['sampling_rate']:.2f})\")\n",
    "    print(f\"   Remaining vs ε={target_budget:.1f} target: {remaining:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f62ba",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We implemented **True DP-SGD**, which includes per-sample clipping, Gaussian noise at **each step**, and composition via a simple accountant\n",
    "* **Privacy**: Membership inference AUC should approach **0.5** under stronger noise; the  **Non‑DP** model’s AUC will be higher (worse privacy).\n",
    "* **Utility**: Compare DP-FL accuracy to a Non‑DP baseline; tune `sigma` (better privacy) and training rounds/hidden size (better utility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05580a-a840-477f-9ead-467cf4e5193f",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "##### Tuning experiment\n",
    "    * Vary noise multiplier (σ) across {1.5, 2.5, 3.5}.\n",
    "    * For each run, record test accuracy and ε.\n",
    "    * Plot accuracy vs ε to visualize the privacy–utility tradeoff.\n",
    "\n",
    "##### Batch size & sampling rate\n",
    "\n",
    "   * Fix σ=2.5, but change batch fractions (q) by setting batch_size = 0.1N, 0.25N, 0.5N.\n",
    "   * Measure ε and accuracy for each.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "* How should an organisation decide what ε is acceptible?\n",
    "* If tighter privacy hurts prediction accuracy, how do we set thresholds to denote when the model ceases to be clinically useful? Who should decide this threshold?\n",
    "* In federated learning, some hospitals may be small (Rural) and hit their privacy budget much faster. Should they be allowed to use weaker privacy so they can contribute longer, or should all hospitals adhere to the same ε? What are the implications for equity between large and small organizations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
