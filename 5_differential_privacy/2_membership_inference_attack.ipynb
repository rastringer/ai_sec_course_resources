{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b6e33d-aab6-47cc-86a0-2467af594a4e",
   "metadata": {},
   "source": [
    "# Membership Inference Attacks\n",
    "\n",
    "Given a trained model and a datapoint, we can use membership inference attacks to determine if the datapoint was used to in the model's training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25d574-0ab6-4aac-9787-5cfbe8e98779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd27f0f-2fd0-404d-98f8-91885bef69ad",
   "metadata": {},
   "source": [
    "Let's create some more synthetic health data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c93e6-b663-4a46-9f0e-0040fb5a5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_health_dataset(n_samples=2000):\n",
    "    \"\"\"Create a realistic health dataset for our attack\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Demographics\n",
    "    age = np.random.normal(45, 15, n_samples)\n",
    "    age = np.clip(age, 18, 90)\n",
    "    \n",
    "    bmi = np.random.normal(26, 4, n_samples)\n",
    "    bmi = np.clip(bmi, 15, 45)\n",
    "    \n",
    "    # Risk factors (correlated with outcome)\n",
    "    smoking = np.random.binomial(1, 0.3, n_samples)\n",
    "    family_history = np.random.binomial(1, 0.4, n_samples)\n",
    "    exercise_hours = np.random.exponential(2, n_samples)\n",
    "    exercise_hours = np.clip(exercise_hours, 0, 10)\n",
    "    \n",
    "    # Create disease outcome (realistic correlations)\n",
    "    risk_score = (\n",
    "        0.05 * age + \n",
    "        0.1 * bmi + \n",
    "        2.0 * smoking + \n",
    "        1.5 * family_history - \n",
    "        0.3 * exercise_hours + \n",
    "        np.random.normal(0, 2, n_samples)\n",
    "    )\n",
    "    \n",
    "    # Convert to probability and sample\n",
    "    prob_disease = 1 / (1 + np.exp(-risk_score + 5))  # Logistic function\n",
    "    has_disease = np.random.binomial(1, prob_disease, n_samples)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'age': age,\n",
    "        'bmi': bmi,\n",
    "        'smoking': smoking,\n",
    "        'family_history': family_history,\n",
    "        'exercise_hours': exercise_hours,\n",
    "        'has_disease': has_disease\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca7c59-bce9-4506-bcc1-839a4a57d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_health_dataset(2000)\n",
    "\n",
    "print(f\"   â€¢ {len(data)} patient records\")\n",
    "print(f\"   â€¢ {data['has_disease'].sum()} positive cases ({data['has_disease'].mean()*100:.1f}%)\")\n",
    "print(f\"   â€¢ Features: {list(data.columns[:-1])}\")\n",
    "print()\n",
    "print(\"Sample data:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004f266-004d-4e45-bbc6-6f0c60bf1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data.drop('has_disease', axis=1)\n",
    "y = data['has_disease']\n",
    "\n",
    "# Split into train/test (victim model will only see training data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"   â€¢ Training set: {len(X_train)} samples (victim model sees these)\")\n",
    "print(f\"   â€¢ Test set: {len(X_test)} samples (victim model has NEVER seen these)\")\n",
    "\n",
    "# Train the victim model\n",
    "print(f\"\\nTraining victim model (Random Forest)...\")\n",
    "victim_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "victim_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc = accuracy_score(y_train, victim_model.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, victim_model.predict(X_test))\n",
    "\n",
    "print(f\"   Training accuracy: {train_acc:.3f}\")\n",
    "print(f\"   Test accuracy: {test_acc:.3f}\")\n",
    "print(f\"   Model looks good! But does it leak membership info?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec1306-4de2-4ee5-b7aa-3a6554d6622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for both sets\n",
    "train_probs = victim_model.predict_proba(X_train)\n",
    "test_probs = victim_model.predict_proba(X_test)\n",
    "\n",
    "# Extract confidence scores (max probability)\n",
    "train_confidence = np.max(train_probs, axis=1)\n",
    "test_confidence = np.max(test_probs, axis=1)\n",
    "\n",
    "print(f\"Confidence statistics:\")\n",
    "print(f\"   Training data - Mean: {train_confidence.mean():.3f}, Std: {train_confidence.std():.3f}\")\n",
    "print(f\"   Test data     - Mean: {test_confidence.mean():.3f}, Std: {test_confidence.std():.3f}\")\n",
    "print(f\"   Difference: {train_confidence.mean() - test_confidence.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f80d5-7fb0-429a-8826-9cc64bfeaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_confidence, bins=30, alpha=0.7, label='Training Data', color='red', density=True)\n",
    "plt.hist(test_confidence, bins=30, alpha=0.7, label='Test Data', color='blue', density=True)\n",
    "plt.xlabel('Model Confidence')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Confidence Distribution: Training vs Test')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC curve for membership inference\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create labels: 1 for training data (member), 0 for test data (non-member)\n",
    "membership_labels = np.concatenate([np.ones(len(train_confidence)), np.zeros(len(test_confidence))])\n",
    "confidence_scores = np.concatenate([train_confidence, test_confidence])\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(membership_labels, confidence_scores)\n",
    "auc_score = roc_auc_score(membership_labels, confidence_scores)\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'Membership Inference (AUC: {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: Membership Inference Attack')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae5117-fc84-42c1-afa1-f22cd8976ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nATTACK RESULTS:\")\n",
    "print(f\"   â€¢ AUC Score: {auc_score:.3f} (0.5 = random, 1.0 = perfect attack)\")\n",
    "if auc_score > 0.6:\n",
    "    print(f\"   â€¢ SUCCESSFUL ATTACK! Model leaks membership information\")\n",
    "elif auc_score > 0.55:\n",
    "    print(f\"   â€¢ WEAK ATTACK: Some membership leakage detected\")\n",
    "else:\n",
    "    print(f\"   â€¢ Attack failed - model seems robust\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff095f-0bbf-4193-8300-17490e6b96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shadow_data(original_data, n_shadow_models=5):\n",
    "    \"\"\"Create shadow datasets by sampling from the same distribution\"\"\"\n",
    "    shadow_datasets = []\n",
    "    \n",
    "    for i in range(n_shadow_models):\n",
    "        # Sample with replacement to create new datasets\n",
    "        shadow_data = original_data.sample(n=len(original_data)//2, replace=True, random_state=i)\n",
    "        shadow_datasets.append(shadow_data)\n",
    "    \n",
    "    return shadow_datasets\n",
    "\n",
    "print(f\"\\nCreating shadow models...\")\n",
    "shadow_datasets = create_shadow_data(data, n_shadow_models=5)\n",
    "\n",
    "# Train shadow models and collect attack training data\n",
    "shadow_attack_features = []\n",
    "shadow_attack_labels = []\n",
    "\n",
    "for i, shadow_data in enumerate(shadow_datasets):\n",
    "    print(f\"   Training shadow model {i+1}/5...\")\n",
    "    \n",
    "    # Split shadow data\n",
    "    X_shadow = shadow_data.drop('has_disease', axis=1)\n",
    "    y_shadow = shadow_data['has_disease']\n",
    "    \n",
    "    X_shadow_train, X_shadow_test, y_shadow_train, y_shadow_test = train_test_split(\n",
    "        X_shadow, y_shadow, test_size=0.5, random_state=i, stratify=y_shadow\n",
    "    )\n",
    "    \n",
    "    # Train shadow model\n",
    "    shadow_model = RandomForestClassifier(n_estimators=50, random_state=i)\n",
    "    shadow_model.fit(X_shadow_train, y_shadow_train)\n",
    "    \n",
    "    # Get predictions for attack training data\n",
    "    train_probs_shadow = shadow_model.predict_proba(X_shadow_train)\n",
    "    test_probs_shadow = shadow_model.predict_proba(X_shadow_test)\n",
    "    \n",
    "    # Create features for attack model (prediction probabilities)\n",
    "    shadow_attack_features.extend(train_probs_shadow)  # These are members\n",
    "    shadow_attack_features.extend(test_probs_shadow)   # These are non-members\n",
    "    \n",
    "    # Create labels (1 = member, 0 = non-member)\n",
    "    shadow_attack_labels.extend([1] * len(train_probs_shadow))\n",
    "    shadow_attack_labels.extend([0] * len(test_probs_shadow))\n",
    "\n",
    "shadow_attack_features = np.array(shadow_attack_features)\n",
    "shadow_attack_labels = np.array(shadow_attack_labels)\n",
    "\n",
    "print(f\"   Generated {len(shadow_attack_features)} attack training examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b042a-5395-4294-8c0f-6f8667ad53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = LogisticRegression(random_state=42)\n",
    "attack_model.fit(shadow_attack_features, shadow_attack_labels)\n",
    "\n",
    "attack_train_acc = accuracy_score(shadow_attack_labels, attack_model.predict(shadow_attack_features))\n",
    "print(f\"   Attack model accuracy on shadow data: {attack_train_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f406bfb-812f-4ad6-ad13-203a3059bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get victim model predictions for attack\n",
    "victim_train_probs = victim_model.predict_proba(X_train)\n",
    "victim_test_probs = victim_model.predict_proba(X_test)\n",
    "\n",
    "# Prepare attack data\n",
    "attack_features = np.vstack([victim_train_probs, victim_test_probs])\n",
    "attack_true_labels = np.concatenate([np.ones(len(victim_train_probs)), np.zeros(len(victim_test_probs))])\n",
    "\n",
    "# Execute attack\n",
    "attack_predictions = attack_model.predict_proba(attack_features)[:, 1]  # Probability of being a member\n",
    "attack_binary = attack_model.predict(attack_features)\n",
    "\n",
    "# Evaluate attack success\n",
    "attack_accuracy = accuracy_score(attack_true_labels, attack_binary)\n",
    "attack_auc = roc_auc_score(attack_true_labels, attack_predictions)\n",
    "\n",
    "print(f\"SHADOW MODEL ATTACK RESULTS:\")\n",
    "print(f\"   â€¢ Attack Accuracy: {attack_accuracy:.3f}\")\n",
    "print(f\"   â€¢ Attack AUC: {attack_auc:.3f}\")\n",
    "print(f\"   â€¢ Improvement over simple method: {attack_auc - auc_score:.3f}\")\n",
    "\n",
    "if attack_auc > 0.7:\n",
    "    print(f\"   â€¢ HIGHLY SUCCESSFUL ATTACK!\")\n",
    "elif attack_auc > 0.6:\n",
    "    print(f\"   â€¢ SUCCESSFUL ATTACK\")\n",
    "else:\n",
    "    print(f\"   â€¢ Attack had limited success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab17e3-4e03-4b50-b085-8ba5adc33877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attack results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Attack predictions by membership\n",
    "plt.subplot(1, 3, 1)\n",
    "member_scores = attack_predictions[attack_true_labels == 1]\n",
    "non_member_scores = attack_predictions[attack_true_labels == 0]\n",
    "\n",
    "plt.hist(member_scores, bins=25, alpha=0.7, label='Actual Members', color='red', density=True)\n",
    "plt.hist(non_member_scores, bins=25, alpha=0.7, label='Non-Members', color='blue', density=True)\n",
    "plt.xlabel('Attack Model Confidence')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Shadow Model Attack Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC curve comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "fpr_simple, tpr_simple, _ = roc_curve(membership_labels, confidence_scores)\n",
    "fpr_shadow, tpr_shadow, _ = roc_curve(attack_true_labels, attack_predictions)\n",
    "\n",
    "plt.plot(fpr_simple, tpr_simple, linewidth=2, label=f'Simple Attack (AUC: {auc_score:.3f})')\n",
    "plt.plot(fpr_shadow, tpr_shadow, linewidth=2, label=f'Shadow Attack (AUC: {attack_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Attack Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall curve\n",
    "plt.subplot(1, 3, 3)\n",
    "precision, recall, _ = precision_recall_curve(attack_true_labels, attack_predictions)\n",
    "plt.plot(recall, precision, linewidth=2, color='purple')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf8961-e73b-4db0-a4ee-33ef27cc2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_defense(X_train, X_test, y_train, y_test, model_params, defense_name):\n",
    "    \"\"\"Evaluate a defense against membership inference\"\"\"\n",
    "    \n",
    "    # Train defended model\n",
    "    defended_model = RandomForestClassifier(**model_params)\n",
    "    defended_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test model performance\n",
    "    train_acc = accuracy_score(y_train, defended_model.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, defended_model.predict(X_test))\n",
    "    \n",
    "    # Test membership inference vulnerability\n",
    "    train_probs = defended_model.predict_proba(X_train)\n",
    "    test_probs = defended_model.predict_proba(X_test)\n",
    "    \n",
    "    train_conf = np.max(train_probs, axis=1)\n",
    "    test_conf = np.max(test_probs, axis=1)\n",
    "    \n",
    "    # Simple attack evaluation\n",
    "    membership_labels = np.concatenate([np.ones(len(train_conf)), np.zeros(len(test_conf))])\n",
    "    confidence_scores = np.concatenate([train_conf, test_conf])\n",
    "    attack_auc = roc_auc_score(membership_labels, confidence_scores)\n",
    "    \n",
    "    return {\n",
    "        'defense': defense_name,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'attack_auc': attack_auc,\n",
    "        'utility_loss': test_acc - test_acc,  # Will compare to baseline\n",
    "        'privacy_gain': 0  # Will compute relative to baseline\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbad829-957e-4201-82c4-0bbdd7c0f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline (no defense)\n",
    "baseline = evaluate_defense(X_train, X_test, y_train, y_test, \n",
    "                          {'n_estimators': 100, 'random_state': 42}, \n",
    "                          'Baseline (No Defense)')\n",
    "\n",
    "# Defense 1: Regularization (reduce overfitting)\n",
    "regularized = evaluate_defense(X_train, X_test, y_train, y_test,\n",
    "                             {'n_estimators': 50, 'max_depth': 5, 'min_samples_leaf': 10, 'random_state': 42},\n",
    "                             'Regularization')\n",
    "\n",
    "# Defense 2: Early stopping (fewer trees)\n",
    "early_stop = evaluate_defense(X_train, X_test, y_train, y_test,\n",
    "                            {'n_estimators': 20, 'random_state': 42},\n",
    "                            'Early Stopping')\n",
    "\n",
    "# Defense 3: Dropout (bootstrap sampling)\n",
    "dropout = evaluate_defense(X_train, X_test, y_train, y_test,\n",
    "                         {'n_estimators': 100, 'max_samples': 0.7, 'random_state': 42},\n",
    "                         'Bootstrap Sampling')\n",
    "\n",
    "results = [baseline, regularized, early_stop, dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cd607-a816-4a08-96b7-72d4938d0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative metrics\n",
    "baseline_auc = baseline['attack_auc']\n",
    "baseline_acc = baseline['test_acc']\n",
    "\n",
    "for result in results:\n",
    "    result['privacy_gain'] = baseline_auc - result['attack_auc']\n",
    "    result['utility_loss'] = baseline_acc - result['test_acc']\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nðŸ“Š DEFENSE EVALUATION RESULTS:\")\n",
    "print(f\"{'Defense':<20} {'Test Acc':<10} {'Attack AUC':<12} {'Privacy Gain':<13} {'Utility Loss'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result['defense']:<20} {result['test_acc']:<10.3f} {result['attack_auc']:<12.3f} \"\n",
    "          f\"{result['privacy_gain']:<13.3f} {result['utility_loss']:<10.3f}\")\n",
    "\n",
    "print(f\"\\nKEY INSIGHTS:\")\n",
    "print(f\"   â€¢ Lower Attack AUC = Better Privacy (closer to 0.5)\")\n",
    "print(f\"   â€¢ Higher Privacy Gain = More effective defense\")\n",
    "print(f\"   â€¢ Lower Utility Loss = Better model performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e48549-e539-4960-9eac-87294f2c8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tradeoffs\n",
    "plt.figure(figsize=(10, 6))\n",
    "defense_names = [r['defense'] for r in results]\n",
    "privacy_gains = [r['privacy_gain'] for r in results]\n",
    "utility_losses = [r['utility_loss'] for r in results]\n",
    "\n",
    "plt.scatter(utility_losses, privacy_gains, s=100, alpha=0.7)\n",
    "for i, name in enumerate(defense_names):\n",
    "    plt.annotate(name, (utility_losses[i], privacy_gains[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.xlabel('Utility Loss (Lower is Better)')\n",
    "plt.ylabel('Privacy Gain (Higher is Better)')\n",
    "plt.title('Privacy-Utility Tradeoff: Different Defenses')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='No Privacy Gain')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='No Utility Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede5859-6f30-42ea-96c0-975384c81c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dp_noise_to_model(model, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Simulate adding DP noise to model parameters\n",
    "    This is a simplified version for educational purposes\n",
    "    \"\"\"\n",
    "    # In real DP-SGD, noise is added to gradients during training\n",
    "    # Here we'll add noise to final predictions as a proxy\n",
    "    \n",
    "    def dp_predict_proba(X):\n",
    "        # Get original predictions\n",
    "        original_probs = model.predict_proba(X)\n",
    "        \n",
    "        # Add noise proportional to privacy budget\n",
    "        noise_scale = 0.1 / epsilon  # Simplified noise calculation\n",
    "        noise = np.random.laplace(0, noise_scale, original_probs.shape)\n",
    "        \n",
    "        # Add noise and renormalize\n",
    "        noisy_probs = original_probs + noise\n",
    "        noisy_probs = np.clip(noisy_probs, 0.001, 0.999)  # Clip to valid probabilities\n",
    "        \n",
    "        # Renormalize to ensure probabilities sum to 1\n",
    "        noisy_probs = noisy_probs / noisy_probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return noisy_probs\n",
    "    \n",
    "    # Monkey patch the model\n",
    "    model.dp_predict_proba = dp_predict_proba\n",
    "    return model\n",
    "\n",
    "# Test different epsilon values\n",
    "epsilons = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "dp_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88dac-8967-4f40-8ed8-5028556dc3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in epsilons:\n",
    "    # Create DP model\n",
    "    dp_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    dp_model.fit(X_train, y_train)\n",
    "    dp_model = add_dp_noise_to_model(dp_model, epsilon)\n",
    "    \n",
    "    # Evaluate utility\n",
    "    test_probs_dp = dp_model.dp_predict_proba(X_test)\n",
    "    test_pred_dp = np.argmax(test_probs_dp, axis=1)\n",
    "    test_acc_dp = accuracy_score(y_test, test_pred_dp)\n",
    "    \n",
    "    # Evaluate privacy (membership inference resistance)\n",
    "    train_probs_dp = dp_model.dp_predict_proba(X_train)\n",
    "    train_conf_dp = np.max(train_probs_dp, axis=1)\n",
    "    test_conf_dp = np.max(test_probs_dp, axis=1)\n",
    "    \n",
    "    membership_labels = np.concatenate([np.ones(len(train_conf_dp)), np.zeros(len(test_conf_dp))])\n",
    "    confidence_scores = np.concatenate([train_conf_dp, test_conf_dp])\n",
    "    attack_auc_dp = roc_auc_score(membership_labels, confidence_scores)\n",
    "    \n",
    "    dp_results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'test_acc': test_acc_dp,\n",
    "        'attack_auc': attack_auc_dp,\n",
    "        'privacy_gain': baseline_auc - attack_auc_dp,\n",
    "        'utility_loss': baseline_acc - test_acc_dp\n",
    "    })\n",
    "    \n",
    "    print(f\"   Îµ={epsilon:4.1f}: Acc={test_acc_dp:.3f}, Attack AUC={attack_auc_dp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061eaddf-15ed-4fa2-a3f9-2b172bb5d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DP tradeoff\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "epsilons_plot = [r['epsilon'] for r in dp_results]\n",
    "utility_losses = [r['utility_loss'] for r in dp_results]\n",
    "privacy_gains = [r['privacy_gain'] for r in dp_results]\n",
    "\n",
    "plt.plot(epsilons_plot, utility_losses, 'r-o', linewidth=2, label='Utility Loss')\n",
    "plt.plot(epsilons_plot, privacy_gains, 'b-o', linewidth=2, label='Privacy Gain')\n",
    "plt.xlabel('Epsilon (Privacy Parameter)')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Differential Privacy Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "attack_aucs = [r['attack_auc'] for r in dp_results]\n",
    "plt.plot(epsilons_plot, attack_aucs, 'g-o', linewidth=2)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random Guessing')\n",
    "plt.xlabel('Epsilon (Privacy Parameter)')\n",
    "plt.ylabel('Attack AUC')\n",
    "plt.title('Attack Success vs Privacy Level')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f91ff-ec57-4818-8c90-52a758e94c8b",
   "metadata": {},
   "source": [
    "### Exercises \n",
    "\n",
    "1. Attack Different Model Types\n",
    "   * Try membership inference on Logistic Regression, SVM, or Neural Networks\n",
    "   * Question: Which model types are most vulnerable?\n",
    "<br></br>\n",
    "2. Vary Dataset Properties\n",
    "   * Create datasets with different sizes (100, 1000, 10000 samples)\n",
    "   * Test attack success vs dataset size\n",
    "   * Question: Are smaller datasets more vulnerable?\n",
    "<br></br>\n",
    "3. Design Your Own Defense\n",
    "   * Implement a custom defense (e.g., prediction smoothing, ensemble voting)\n",
    "   * Compare its privacy-utility tradeoff to existing defenses\n",
    "   * Question: Can you beat regularization?\n",
    "<br></br>\n",
    "4. Multi-class Membership Inference\n",
    "   * Extend the attack to datasets with more than 2 classes\n",
    "   * Does attack success change with number of classes?\n",
    "   * Question: How does class imbalance affect membership inference?\n",
    "<br></br>\n",
    "5. Stretch exercise: Realistic Attack Scenario\n",
    "   * Assume you're an attacker with limited knowledge\n",
    "   * You don't know the exact training data distribution\n",
    "   * Can you still mount a successful membership inference attack?\n",
    "   * Hint: Try using publicly available similar datasets for shadow models\n",
    "\n",
    "### Discussion\n",
    "\n",
    "1. In what scenarios would membership inference attacks be most dangerous?\n",
    "2. How might federated learning change the attack landscape?\n",
    "3. What ethical considerations arise when publishing models?\n",
    "4. How should organizations balance model utility vs privacy risks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec905b72-cce0-4427-a22f-a8d3797cdee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
