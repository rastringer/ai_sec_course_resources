{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68a09c6-fa09-410e-8317-5afd73f68571",
   "metadata": {},
   "source": [
    "# Federated Learning Vulnerabilities\n",
    "\n",
    "Federated learning is a process whereby different organizations, or devices, or groups, train models locally on their 'federated' data, and submit the gradients to a global model. This process in theory preserves the privacy of users or data samples. \n",
    "\n",
    "Imagine a population health and economic model to give insights into behaviour, spending, financial status and health. Obviously these are all areas where identifiable data is extremely sensitive, so federated learning would be key. \n",
    "\n",
    "Here is a diagram to show the flow:\n",
    "\n",
    "<img src=\"images/federated_learning.png\" width=\"600\">\n",
    "\n",
    "In the imaginery scenario for this notebook, we will focus on a health model. We will use synthetic medical data again from three hospitals that have decided to collaborate on a heart disease prediction model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a1263-a7d4-442d-899d-1289f7f390bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de2416-3ced-41f2-a1fd-98d80a8f84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_medical_dataset(n_patients=300):\n",
    "    \"\"\"Creates a synthetic medical dataset\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate correlated medical features\n",
    "    ages = np.random.normal(55, 15, n_patients).clip(20, 90)\n",
    "    \n",
    "    # Cholesterol tends to increase with age, with some noise\n",
    "    cholesterol = 150 + 2 * ages + np.random.normal(0, 30, n_patients)\n",
    "    cholesterol = cholesterol.clip(120, 350)\n",
    "    \n",
    "    # Blood pressure correlated with age and cholesterol\n",
    "    systolic_bp = 90 + 0.8 * ages + 0.1 * cholesterol + np.random.normal(0, 15, n_patients)\n",
    "    systolic_bp = systolic_bp.clip(90, 200)\n",
    "    \n",
    "    # Heart rate - younger people tend to have higher resting rates\n",
    "    heart_rate = 85 - 0.2 * ages + np.random.normal(0, 10, n_patients)\n",
    "    heart_rate = heart_rate.clip(50, 120)\n",
    "    \n",
    "    # Exercise hours per week - tends to decrease with age\n",
    "    exercise_hours = np.maximum(0, 8 - 0.05 * ages + np.random.normal(0, 3, n_patients))\n",
    "    \n",
    "    # BMI - some correlation with age and exercise\n",
    "    bmi = 22 + 0.1 * ages - 0.3 * exercise_hours + np.random.normal(0, 4, n_patients)\n",
    "    bmi = bmi.clip(16, 45)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = np.column_stack([ages, cholesterol, systolic_bp, heart_rate, exercise_hours, bmi])\n",
    "    \n",
    "    # Create heart disease labels based on realistic risk factors\n",
    "    risk_score = (\n",
    "        0.03 * ages + \n",
    "        0.01 * cholesterol + \n",
    "        0.02 * systolic_bp + \n",
    "        -0.05 * exercise_hours + \n",
    "        0.1 * bmi + \n",
    "        np.random.normal(0, 2, n_patients)\n",
    "    )\n",
    "    \n",
    "    # Convert to binary classification (heart disease or not)\n",
    "    y = (risk_score > np.percentile(risk_score, 70)).astype(int)  # 30% have heart disease\n",
    "    \n",
    "    feature_names = ['Age', 'Cholesterol', 'Systolic_BP', 'Heart_Rate', 'Exercise_Hours', 'BMI']\n",
    "    \n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47903345-0c27-4a7f-9199-d92a28efc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hospital_data():\n",
    "    \"\"\"Create data for three different hospitals with realistic differences\"\"\"\n",
    "    X, y, feature_names = create_medical_dataset()\n",
    "    \n",
    "    # Hospital A: Urban hospital, younger population, more health-conscious\n",
    "    hospital_a_indices = np.where((X[:, 0] < 50) | (X[:, 4] > 5))[0][:80]  # Younger or more active\n",
    "    \n",
    "    # Hospital B: Suburban hospital, middle-aged population\n",
    "    hospital_b_indices = np.where((X[:, 0] >= 40) & (X[:, 0] <= 65))[0][:90]\n",
    "    \n",
    "    # Hospital C: Rural hospital, older population, less access to preventive care\n",
    "    hospital_c_indices = np.where((X[:, 0] > 55) | (X[:, 2] > 140))[0][:80]  # Older or higher BP\n",
    "    \n",
    "    # Ensure no overlap\n",
    "    used_indices = set()\n",
    "    hospitals_data = []\n",
    "    hospital_names = [\"Edinburgh Hospital\", \"Clydebank Hospital\", \"Orkney Hospital\"]\n",
    "    \n",
    "    for i, indices in enumerate([hospital_a_indices, hospital_b_indices, hospital_c_indices]):\n",
    "        available_indices = [idx for idx in indices if idx not in used_indices][:70 + i*10]\n",
    "        used_indices.update(available_indices)\n",
    "        \n",
    "        hospital_X = X[available_indices]\n",
    "        hospital_y = y[available_indices]\n",
    "        \n",
    "        hospitals_data.append((hospital_X, hospital_y, hospital_names[i]))\n",
    "    \n",
    "    return hospitals_data, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e11b0-633c-4875-83c9-8f9d62f48258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hospital datasets\n",
    "hospitals_data, feature_names = create_hospital_data()\n",
    "\n",
    "print(f\"\\nCreated data for {len(hospitals_data)} hospitals:\")\n",
    "for i, (hosp_X, hosp_y, name) in enumerate(hospitals_data):\n",
    "    heart_disease_rate = np.mean(hosp_y) * 100\n",
    "    avg_age = np.mean(hosp_X[:, 0])\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  • {len(hosp_X)} patients\")\n",
    "    print(f\"  • Heart disease rate: {heart_disease_rate:.1f}%\")\n",
    "    print(f\"  • Average age: {avg_age:.1f} years\")\n",
    "    print(f\"  • Avg cholesterol: {np.mean(hosp_X[:, 1]):.0f} mg/dL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81be3cd-ed7b-4802-9f8d-518a0569e03f",
   "metadata": {},
   "source": [
    "### Federated learning set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ea56d-e077-4b44-9658-a9496be312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HospitalClient:\n",
    "    \"\"\"Represents a hospital participating in federated learning\"\"\"\n",
    "    \n",
    "    def __init__(self, patient_data_x, patient_data_y, hospital_name):\n",
    "        self.hospital_name = hospital_name\n",
    "        self.patient_data_x = torch.FloatTensor(patient_data_x)\n",
    "        self.patient_data_y = torch.LongTensor(patient_data_y)\n",
    "        self.model = None\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def train_on_patients(self, epochs=1, lr=0.01):\n",
    "        \"\"\"Train model on hospital's patients and return gradients\"\"\"\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        initial_params = [p.clone().detach() for p in self.model.parameters()]\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = self.model(self.patient_data_x)\n",
    "            loss = criterion(predictions, self.patient_data_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate gradients\n",
    "        gradients = []\n",
    "        for initial, current in zip(initial_params, self.model.parameters()):\n",
    "            gradients.append(current.detach() - initial)\n",
    "        \n",
    "        return gradients, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757a782-f3b0-4be4-809c-d4ff97e8b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalFLServer:\n",
    "    \"\"\"Federated learning server for medical collaboration\"\"\"\n",
    "    \n",
    "    def __init__(self, model_template):\n",
    "        self.global_model = model_template\n",
    "        self.hospitals = []\n",
    "    \n",
    "    def add_hospital(self, hospital):\n",
    "        self.hospitals.append(hospital)\n",
    "        hospital.set_model(self.global_model)\n",
    "    \n",
    "    def collaboration_round(self):\n",
    "        \"\"\"One round of inter-hospital collaboration\"\"\"\n",
    "        all_gradients = []\n",
    "        \n",
    "        print(\"\\nHospitals training locally and sharing gradients...\")\n",
    "        for hospital in self.hospitals:\n",
    "            gradients, loss = hospital.train_on_patients()\n",
    "            all_gradients.append(gradients)\n",
    "            print(f\"{hospital.hospital_name}: Training loss = {loss:.4f}\")\n",
    "        \n",
    "        # Average gradients (FedAvg)\n",
    "        avg_gradients = []\n",
    "        for layer_idx in range(len(all_gradients[0])):\n",
    "            layer_grads = torch.stack([grads[layer_idx] for grads in all_gradients])\n",
    "            avg_gradients.append(torch.mean(layer_grads, dim=0))\n",
    "        \n",
    "        # Update global model\n",
    "        for param, avg_grad in zip(self.global_model.parameters(), avg_gradients):\n",
    "            param.data += avg_grad\n",
    "        \n",
    "        # Send updated model back to hospitals\n",
    "        for hospital in self.hospitals:\n",
    "            hospital.set_model(self.global_model)\n",
    "        \n",
    "        return all_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b82954-f2dd-4a27-addc-d5305dbf186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical prediction model\n",
    "class HeartDiseasePredictor(nn.Module):\n",
    "    def __init__(self, n_features=6):\n",
    "        super(HeartDiseasePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 2)  # No heart disease, Heart disease\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043629e9-39e5-41c1-b5e9-3c300023202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize medical data\n",
    "scaler = StandardScaler()\n",
    "all_data = np.vstack([data[0] for data in hospitals_data])\n",
    "scaler.fit(all_data)\n",
    "\n",
    "# Set up federated learning\n",
    "model = HeartDiseasePredictor(n_features=len(feature_names))\n",
    "fl_server = MedicalFLServer(model)\n",
    "\n",
    "hospitals = []\n",
    "for hosp_X, hosp_y, name in hospitals_data:\n",
    "    standardized_X = scaler.transform(hosp_X)\n",
    "    hospital = HospitalClient(standardized_X, hosp_y, name)\n",
    "    hospitals.append(hospital)\n",
    "    fl_server.add_hospital(hospital)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfa6f2-f31e-4830-92ad-0b42f8617e2b",
   "metadata": {},
   "source": [
    "### What can we learn from the gradients?\n",
    "\n",
    "In gradient analysis attacks, attackers learn purely from gradient statistics. Gradients are shared as part of federated learning, and a global model used for all contributed data will be accessible to at least some members of participating institutions. Each institution will use the model on their own training data, sending the gradients back to the FL server to update the global model.\n",
    "\n",
    "#### What do gradients actually reveal?\n",
    "\n",
    "Everything from batch size to training difficulty - large gradients can indicate diverse or complicated data. They also offer clues about label distribution, or how diverse and varied the data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b78d2-0e9c-47c7-90d7-0f3e60a421e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hospital_gradients(gradients, hospital_data, hospital_name):\n",
    "    \"\"\"Analyze what an attacker can learn from hospital gradients\"\"\"\n",
    "    hosp_X, hosp_y, _ = hospital_data\n",
    "    \n",
    "    print(f\"\\nAnalyzing {hospital_name}'s gradients...\")\n",
    "    \n",
    "    # Basic information that leaks\n",
    "    estimated_patients = len(hosp_y)\n",
    "    heart_disease_cases = sum(hosp_y)\n",
    "    \n",
    "    print(f\"Information revealed:\")\n",
    "    print(f\"  • Number of patients in training batch: {estimated_patients}\")\n",
    "    print(f\"  • Patients with heart disease: {heart_disease_cases}\")\n",
    "    print(f\"  • Heart disease rate: {heart_disease_cases/estimated_patients*100:.1f}%\")\n",
    "    \n",
    "    # Gradient statistics reveal training characteristics\n",
    "    total_grad_magnitude = sum(g.abs().sum().item() for g in gradients)\n",
    "    print(f\"  • Total gradient magnitude: {total_grad_magnitude:.2f}\")\n",
    "    print(f\"    (High values suggest difficult/diverse cases)\")\n",
    "    \n",
    "    # Advanced inference: can detect demographic skews\n",
    "    print(f\"\\nPotential demographic inferences:\")\n",
    "    if total_grad_magnitude > 50:\n",
    "        print(f\"High gradient magnitude suggests diverse patient population\")\n",
    "    else:\n",
    "        print(f\"Low gradient magnitude suggests homogeneous patient population\")\n",
    "    \n",
    "    return {\n",
    "        'patient_count': estimated_patients,\n",
    "        'disease_cases': heart_disease_cases,\n",
    "        'disease_rate': heart_disease_cases/estimated_patients,\n",
    "        'gradient_magnitude': total_grad_magnitude\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057ffbe-c585-42c6-9b42-c3d3d17d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run federated learning and analyze what leaks\n",
    "print(\"Starting federated learning collaboration...\")\n",
    "hospital_gradients = fl_server.collaboration_round()\n",
    "\n",
    "# Analyze each hospital's gradients (what an attacker could learn)\n",
    "leaked_info = []\n",
    "for i, (gradients, hospital_data) in enumerate(zip(hospital_gradients, hospitals_data)):\n",
    "    info = analyze_hospital_gradients(gradients, hospital_data, hospital_data[2])\n",
    "    leaked_info.append(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e19e2b-6295-4ae0-8ba2-db96bddc5ddd",
   "metadata": {},
   "source": [
    "### Reconstructing patient data\n",
    "\n",
    "Can we reconstruct actual patient information from shared gradients via a model inversion attack?\n",
    "An attacker with access to the gradients may not also need access to the model, since model architectures are often similar for related tasks. Experimenting with their own models, attackers can run their own copy of the model. \n",
    "\n",
    "#### How the model inversion attack works\n",
    "\n",
    "In our scenario, hospital patient data is used to train a model and create real gradients, which are shared in our FL server.\n",
    "\n",
    "The model inversion attack follows this flow:\n",
    "\n",
    "Same Global Model → Train on Dummy Patients → Dummy Gradients → Compare to Real Gradients\n",
    "\n",
    "We use an iterative process to continually adjust dummy patient data to minimize the difference between the dummy and real gradients until we have essentially cloned the original patient data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98815cc3-a5ed-445d-ae43-6618be0b5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataReconstructor:\n",
    "    \"\"\"Attempts to reconstruct patient data from gradients\"\"\"\n",
    "    \n",
    "    def __init__(self, target_model, n_features, feature_names):\n",
    "        self.target_model = target_model\n",
    "        self.n_features = n_features\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def reconstruct_patients(self, gradients, n_patients, iterations=100):\n",
    "        \"\"\"Try to reconstruct patient data from gradients\"\"\"\n",
    "        \n",
    "        print(f\"\\nAttempting to reconstruct {n_patients} patients' data...\")\n",
    "        \n",
    "        # Initialize dummy patient data\n",
    "        dummy_patients = torch.randn(n_patients, self.n_features, requires_grad=True)\n",
    "        dummy_labels = torch.randint(0, 2, (n_patients,))  # Random initial labels\n",
    "        \n",
    "        optimizer = optim.Adam([dummy_patients], lr=0.1)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        reconstruction_losses = []\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass, run dummy data through the model\n",
    "            dummy_predictions = self.target_model(dummy_patients)\n",
    "            dummy_loss = criterion(dummy_predictions, dummy_labels)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dummy_gradients = torch.autograd.grad(dummy_loss, self.target_model.parameters(), \n",
    "                                                 create_graph=True)\n",
    "            \n",
    "            # Compare dummy gradients to real gradients\n",
    "            gradient_diff = 0\n",
    "            for dummy_grad, real_grad in zip(dummy_gradients, gradients):\n",
    "                gradient_diff += ((dummy_grad - real_grad) ** 2).sum()\n",
    "\n",
    "            # Adjust dummy data to minimize gradient difference\n",
    "            gradient_diff.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            reconstruction_losses.append(gradient_diff.item())\n",
    "            \n",
    "            if iteration % 25 == 0:\n",
    "                print(f\"  Iteration {iteration}: Reconstruction loss = {gradient_diff.item():.4f}\")\n",
    "        \n",
    "        # Convert back to interpretable medical values\n",
    "        reconstructed_data = dummy_patients.detach().numpy()\n",
    "        \n",
    "        # Inverse transform to get original medical units\n",
    "        reconstructed_medical = scaler.inverse_transform(reconstructed_data)\n",
    "        \n",
    "        return reconstructed_medical, reconstruction_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c1ddf-0a4d-4124-be96-81a237f717bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack the first hospital (Edinburgh)\n",
    "target_hospital_idx = 0\n",
    "target_hospital_data = hospitals_data[target_hospital_idx]\n",
    "target_hospital_name = target_hospital_data[2]\n",
    "\n",
    "print(f\"\\nTargeting {target_hospital_name}\")\n",
    "print(\"(Attacker has access to gradients from federated learning)\")\n",
    "\n",
    "reconstructor = PatientDataReconstructor(fl_server.global_model, len(feature_names), feature_names)\n",
    "\n",
    "# Reconstruct first 3 patients\n",
    "n_patients_to_attack = 3\n",
    "reconstructed_patients, losses = reconstructor.reconstruct_patients(\n",
    "    hospital_gradients[target_hospital_idx], \n",
    "    n_patients_to_attack, \n",
    "    iterations=75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c5f04-0d6a-45a6-b1dc-d911979bb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original patient data for comparison\n",
    "original_patients = hospitals_data[target_hospital_idx][0][:n_patients_to_attack]\n",
    "\n",
    "print(f\"Comparing reconstructed vs actual patient data from {target_hospital_name}:\\n\")\n",
    "\n",
    "for i in range(n_patients_to_attack):\n",
    "    print(f\"PATIENT {i+1}:\")\n",
    "    print(\"     Feature          Original    Reconstructed    Error\")\n",
    "    print(\"     \" + \"-\" * 50)\n",
    "    \n",
    "    for j, feature_name in enumerate(feature_names):\n",
    "        original_val = original_patients[i, j]\n",
    "        reconstructed_val = reconstructed_patients[i, j]\n",
    "        error = abs(original_val - reconstructed_val)\n",
    "        \n",
    "        print(f\"     {feature_name:<15} {original_val:8.1f}    {reconstructed_val:8.1f}    {error:6.1f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Calculate overall reconstruction accuracy\n",
    "total_error = np.mean(np.abs(original_patients - reconstructed_patients))\n",
    "print(f\"Average reconstruction error: {total_error:.2f}\")\n",
    "print(f\"   (Lower = more accurate reconstruction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631118f7-0992-48ce-948f-63fcbd562110",
   "metadata": {},
   "source": [
    "### Property inference attack\n",
    "What can we learn about each hospital's patient population?\n",
    "\n",
    "This is a statistical attack for which we just need access to the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990c07e-be23-495b-8779-78db7ddc3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_hospital_from_gradients(gradients, hospital_name):\n",
    "    \"\"\"Create a profile of the hospital based on gradient analysis\"\"\"\n",
    "    \n",
    "    print(f\"\\n{hospital_name} Profile:\")\n",
    "    \n",
    "    # Analyze gradient patterns\n",
    "    layer1_grads = gradients[0].flatten()\n",
    "    layer2_grads = gradients[1].flatten() if len(gradients) > 1 else None\n",
    "    \n",
    "    # Statistical analysis\n",
    "    grad_variance = layer1_grads.var().item()\n",
    "    grad_mean_abs = layer1_grads.abs().mean().item()\n",
    "    \n",
    "    print(f\"Gradient Statistics:\")\n",
    "    print(f\"• Variance: {grad_variance:.4f}\")\n",
    "    print(f\"• Mean absolute: {grad_mean_abs:.4f}\")\n",
    "    \n",
    "    # Make inferences about patient population\n",
    "    if grad_variance > 0.01:\n",
    "        print(f\"Inference: Likely has DIVERSE patient population\")\n",
    "        print(f\"(High gradient variance suggests varied patient characteristics)\")\n",
    "    else:\n",
    "        print(f\"Inference: Likely has HOMOGENEOUS patient population\")\n",
    "        print(f\"(Low gradient variance suggests similar patient characteristics)\")\n",
    "    \n",
    "    if grad_mean_abs > 0.1:\n",
    "        print(f\"High-risk patient population (strong model updates needed)\")\n",
    "    else:\n",
    "        print(f\"Lower-risk patient population (mild model updates)\")\n",
    "\n",
    "# Profile each hospital\n",
    "for i, (gradients, (_, _, hospital_name)) in enumerate(zip(hospital_gradients, hospitals_data)):\n",
    "    profile_hospital_from_gradients(gradients, hospital_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b33b2-874f-488c-b27f-4eb780ce9221",
   "metadata": {},
   "source": [
    "### Visualize privacy breaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e626bdc-c678-4ce3-b355-21bd8c6d1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Hospital patient demographics (what we can infer)\n",
    "ax1 = axes[0, 0]\n",
    "hospitals_names = [data[2].replace(' ', '\\n') for data in hospitals_data]\n",
    "disease_rates = [info['disease_rate'] * 100 for info in leaked_info]\n",
    "patient_counts = [info['patient_count'] for info in leaked_info]\n",
    "\n",
    "bars = ax1.bar(hospitals_names, disease_rates, alpha=0.7, color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax1.set_ylabel('Heart Disease Rate (%)')\n",
    "ax1.set_title('LEAKED: Hospital Disease Rates\\n(Inferred from Gradients)', color='red')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add patient count labels on bars\n",
    "for bar, count in zip(bars, patient_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1, \n",
    "             f'{int(count)} patients', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Reconstruction accuracy over time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(losses, color='red', linewidth=2)\n",
    "ax2.set_xlabel('Attack Iterations')\n",
    "ax2.set_ylabel('Reconstruction Loss')\n",
    "ax2.set_title('Patient Data Reconstruction Progress', color='red')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Original vs Reconstructed Patient Data\n",
    "ax3 = axes[1, 0]\n",
    "features_to_plot = [0, 1]  # Age and Cholesterol\n",
    "ax3.scatter(original_patients[:, features_to_plot[0]], original_patients[:, features_to_plot[1]], \n",
    "           s=100, alpha=0.7, label='Real Patients', color='green', marker='o')\n",
    "ax3.scatter(reconstructed_patients[:, features_to_plot[0]], reconstructed_patients[:, features_to_plot[1]], \n",
    "           s=100, alpha=0.7, label='Reconstructed', color='red', marker='x')\n",
    "\n",
    "ax3.set_xlabel(feature_names[features_to_plot[0]])\n",
    "ax3.set_ylabel(feature_names[features_to_plot[1]])\n",
    "ax3.set_title('Reconstructed vs Real Patient Data', color='red')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Privacy Risk Assessment\n",
    "ax4 = axes[1, 1]\n",
    "risk_categories = ['Patient Count\\nLeakage', 'Disease Rate\\nLeakage', 'Data\\nReconstruction', 'Demographics\\nProfiling']\n",
    "risk_levels = [9, 8, 6, 7]  # Risk scores out of 10\n",
    "colors = ['#ff4757', '#ff6348', '#ff7675', '#fd79a8']\n",
    "\n",
    "bars = ax4.bar(risk_categories, risk_levels, color=colors, alpha=0.8)\n",
    "ax4.set_ylabel('Privacy Risk Level (1-10)')\n",
    "ax4.set_title('Privacy Risk Assessment', color='red', fontweight='bold')\n",
    "ax4.set_ylim(0, 10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add risk level labels\n",
    "for bar, risk in zip(bars, risk_levels):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1, \n",
    "             f'Risk: {risk}/10', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b30e61-1e75-427d-8c2a-a5141aba6987",
   "metadata": {},
   "source": [
    "### Data breaches and hospital statistics discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7576fee-f125-451a-b16d-afeda5dcc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (info, (_, _, name)) in enumerate(zip(leaked_info, hospitals_data)):\n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"     • {info['patient_count']} patients exposed\")\n",
    "    print(f\"     • {info['disease_rate']*100:.1f}% heart disease rate revealed\")\n",
    "    print(f\"     • Population characteristics inferred\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b3f46-2050-4ceb-bff5-ae9e54370012",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We successfully recontructed individual patient medical data and exposed sensitive health information such as age and  cholesterol. We have also opened the door to demographic profiling, showing diseasse prevalence patterns. \n",
    "\n",
    "This information could be used by insurance companies to identify high-risk hospitals, areas or even patients. The reconstructed data could amount to a compliance violation such as GDPR in the UK or others. Patients may lose trust and there could be legal liability for the hospitals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46be1e-4896-463b-8bca-2eb55b556b5f",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "##### Experimenting with parameters:\n",
    "   * Increase reconstruction iterations (try 200+)\n",
    "   * Attack more patients simultaneously\n",
    "   * Try different hospitals as targets\n",
    "\n",
    "##### Attack enhancement:\n",
    "   * Can you infer which hospital has the oldest patients?\n",
    "   * Try to determine which hospital has the most severe cases\n",
    "   * Experiment with different gradient analysis techniques\n",
    "\n",
    "### Discussion\n",
    "   * Should hospitals be allowed to collaborate this way?\n",
    "   * What are the trade-offs between medical progress and privacy?\n",
    "   * How would you explain these risks to hospital administrators?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
